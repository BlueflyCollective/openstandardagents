apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-10T15:12:37Z"
    generateName: drupal-frontend-7ccd54b7ff-
    labels:
      app: frontend
      pod-template-hash: 7ccd54b7ff
    name: drupal-frontend-7ccd54b7ff-454gc
    namespace: agent-chat
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: drupal-frontend-7ccd54b7ff
      uid: 90387c1b-362d-4d74-9184-4fc0947ccb9e
    resourceVersion: "7643563"
    uid: 78a275e2-45c6-41dd-bf63-cfb32602e596
  spec:
    containers:
    - env:
      - name: DRUPAL_DATABASE_HOST
        value: postgres.agent-chat.local.bluefly.io
      - name: DRUPAL_DATABASE_PORT
        value: "5432"
      - name: DRUPAL_DATABASE_NAME
        value: llm_platform
      - name: DRUPAL_DATABASE_USERNAME
        value: llm_user
      - name: DRUPAL_DATABASE_PASSWORD
        value: llm_password
      image: drupal:10-apache
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 80
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: drupal
      ports:
      - containerPort: 80
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 80
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/www/html/sites/default/files
        name: drupal-files
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-p98zj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: drupal-files
    - name: kube-api-access-p98zj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T03:46:33Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:54Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:54Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T03:46:33Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://1c6831d2c9d88f9c0ad9421cb18627d805aaca994de25774c566aaf842999163
      image: drupal:10-apache
      imageID: docker-pullable://drupal@sha256:2b49d9dc2d4ef57ab858fc1d3665bb3892ac4f4fe2a22995c3de66844243331f
      lastState:
        terminated:
          containerID: docker://292f0199ccabb44a23498d4a45d8970720eab707bedfae53e8bfd7c634669698
          exitCode: 0
          finishedAt: "2025-11-18T01:36:43Z"
          reason: Completed
          startedAt: "2025-11-18T01:34:12Z"
      name: drupal
      ready: true
      restartCount: 96
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:21Z"
      volumeMounts:
      - mountPath: /var/www/html/sites/default/files
        name: drupal-files
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-p98zj
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.112
    podIPs:
    - ip: 192.168.194.112
    - ip: fd07:b51a:cc66:a::1e04
    qosClass: Burstable
    startTime: "2025-11-11T03:46:33Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-10-31T07:14:10-04:00"
    creationTimestamp: "2025-11-11T03:29:03Z"
    generateName: gateway-c7c6db957-
    labels:
      app: gateway
      pod-template-hash: c7c6db957
    name: gateway-c7c6db957-ntgcw
    namespace: agent-studio
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gateway-c7c6db957
      uid: 05183faf-db65-43cf-8cea-7dd5146dd814
    resourceVersion: "7642953"
    uid: 82e12fc4-5f38-4eba-82b2-d89d340df0c0
  spec:
    containers:
    - env:
      - name: PHOENIX_COLLECTOR_ENDPOINT
        value: http://phoenix.observability.svc.cluster.local:4318/v1/traces
      - name: NODE_ENV
        value: development
      image: nginx:alpine
      imagePullPolicy: IfNotPresent
      name: gateway
      ports:
      - containerPort: 80
        protocol: TCP
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ttcbh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-ttcbh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T03:46:33Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T03:46:33Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://cf32e151f7c7cee7f56da75d1a45bf3f51915d49dacbecbc66c2b1c4af5f9635
      image: nginx:alpine
      imageID: docker-pullable://nginx@sha256:b3c656d55d7ad751196f21b7fd2e8d4da9cb430e32f646adcf92441b72f82b14
      lastState:
        terminated:
          containerID: docker://97b76e6bb8de2ca763fef2ce414a690d8e4688f13fd50332e9c2217674102c41
          exitCode: 0
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Completed
          startedAt: "2025-11-18T01:34:12Z"
      name: gateway
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:21Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ttcbh
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.110
    podIPs:
    - ip: 192.168.194.110
    - ip: fd07:b51a:cc66:a::1e02
    qosClass: Burstable
    startTime: "2025-11-11T03:46:33Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-10T15:12:29Z"
    generateName: aiflow-social-agent-5fbbb8c656-
    labels:
      app: aiflow-social-agent
      pod-template-hash: 5fbbb8c656
    name: aiflow-social-agent-5fbbb8c656-qhnnd
    namespace: agents-staging
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: aiflow-social-agent-5fbbb8c656
      uid: 4636e973-84c9-460f-850f-80f30009a342
    resourceVersion: "7642927"
    uid: 520935fc-4036-44aa-b398-5a9e8323d37a
  spec:
    containers:
    - command:
      - sh
      - -c
      - while true; do echo 'Mock AIFlow agent running'; sleep 30; done
      env:
      - name: AGENT_ID
        value: social-agent-aiflow
      - name: PHOENIX_COLLECTOR_ENDPOINT
        value: http://phoenix.observability.svc.cluster.local:4318/v1/traces
      - name: NODE_ENV
        value: development
      image: python:3.11-slim
      imagePullPolicy: IfNotPresent
      name: aiflow-agent
      ports:
      - containerPort: 8000
        name: http
        protocol: TCP
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8lcfh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-8lcfh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T03:46:33Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T03:46:33Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://1b26c03b2210a724aa2676b2c6c671a78b06e3788e28ed5446930ae29b7877ad
      image: python:3.11-slim
      imageID: docker-pullable://python@sha256:fa9b525a0be0c5ae5e6f2209f4be6fdc5a15a36fed0222144d98ac0d08f876d4
      lastState:
        terminated:
          containerID: docker://957ee3b007a4332f0f9f25314639e24ae3b147dcff9ea4d6038e17c92f1cc800
          exitCode: 137
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Error
          startedAt: "2025-11-18T01:34:12Z"
      name: aiflow-agent
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:21Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8lcfh
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.113
    podIPs:
    - ip: 192.168.194.113
    - ip: fd07:b51a:cc66:a::1e05
    qosClass: Burstable
    startTime: "2025-11-11T03:46:33Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-11T02:58:17Z"
    generateName: apidog-runner-587c6b45b7-
    labels:
      app: apidog-runner
      pod-template-hash: 587c6b45b7
    name: apidog-runner-587c6b45b7-jcxt7
    namespace: apidog
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: apidog-runner-587c6b45b7
      uid: 9cd9a45b-fb80-4310-836c-2efcc3a08110
    resourceVersion: "7643089"
    uid: 3e88c705-114c-4504-b295-fd35d70f0eb2
  spec:
    containers:
    - env:
      - name: TZ
        value: America/New_York
      - name: SERVER_APP_BASE_URL
        value: https://api.apidog.com
      - name: TEAM_ID
        valueFrom:
          secretKeyRef:
            key: TEAM_ID
            name: apidog-runner-config
      - name: RUNNER_ID
        valueFrom:
          secretKeyRef:
            key: RUNNER_ID
            name: apidog-runner-config
      - name: ACCESS_TOKEN
        valueFrom:
          secretKeyRef:
            key: ACCESS_TOKEN
            name: apidog-runner-config
      image: apidog/self-hosted-general-runner:latest
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 4524
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: runner
      ports:
      - containerPort: 4524
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 4524
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 256Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zdbv8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-zdbv8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T03:46:33Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:28Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:28Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T03:46:33Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://899d7edc3da31ebf5e869e0889468fd047450f0dccfb4e1733f887896a14d92e
      image: apidog/self-hosted-general-runner:latest
      imageID: docker-pullable://apidog/self-hosted-general-runner@sha256:4afd96ec81f271ccca452e6e26e4afcb301116cfaa1a4993bb1b6022122a5bd5
      lastState:
        terminated:
          containerID: docker://b62b2f6f90d9169504fce73bed3aacbc92c01ae214de7d8c93bc7922a1fc76e5
          exitCode: 0
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Completed
          startedAt: "2025-11-18T01:34:14Z"
      name: runner
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:22Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zdbv8
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.96
    podIPs:
    - ip: 192.168.194.96
    - ip: fd07:b51a:cc66:a::1df4
    qosClass: Burstable
    startTime: "2025-11-11T03:46:33Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-13T23:00:05Z"
    generateName: argo-server-54467f6857-
    labels:
      app: argo-server
      pod-template-hash: 54467f6857
    name: argo-server-54467f6857-9bw5j
    namespace: argo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: argo-server-54467f6857
      uid: 47e4b6e2-285e-4c0c-ada9-4f0f52748e97
    resourceVersion: "7643410"
    uid: 1091115d-c3dc-4a4d-b41c-8be154e35fcd
  spec:
    containers:
    - args:
      - server
      image: quay.io/argoproj/argocli:v3.5.2
      imagePullPolicy: IfNotPresent
      name: argo-server
      ports:
      - containerPort: 2746
        name: web
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 2746
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 20
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mt8h8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
    serviceAccount: argo-server
    serviceAccountName: argo-server
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-mt8h8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T08:01:20Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:45Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:45Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T08:01:20Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://2b5dec0b4ac0da7acfc7eb81991b5e4d3642e714036700379b93525f699e8796
      image: quay.io/argoproj/argocli:v3.5.2
      imageID: docker-pullable://quay.io/argoproj/argocli@sha256:026b30a353022d8909adb03816e9593ce95b99afe4581e401ec39606121a22b3
      lastState:
        terminated:
          containerID: docker://ea83fcc43848a16b2fa9bfc290927404173f068ff54909fb45307781df50e953
          exitCode: 2
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Error
          startedAt: "2025-11-18T01:34:11Z"
      name: argo-server
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:20Z"
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mt8h8
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.105
    podIPs:
    - ip: 192.168.194.105
    - ip: fd07:b51a:cc66:a::1dfd
    qosClass: Burstable
    startTime: "2025-11-14T08:01:20Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-13T23:00:05Z"
    generateName: workflow-controller-855c5bfc48-
    labels:
      app: workflow-controller
      pod-template-hash: 855c5bfc48
    name: workflow-controller-855c5bfc48-vs5mw
    namespace: argo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: workflow-controller-855c5bfc48
      uid: 5dce8914-f3d1-4f09-9f49-231b5854f768
    resourceVersion: "7642992"
    uid: 9d2ae5d7-2da6-46ab-badf-071ff47c0490
  spec:
    containers:
    - command:
      - workflow-controller
      env:
      - name: LEADER_ELECTION_IDENTITY
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: quay.io/argoproj/workflow-controller:v3.5.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 6060
          scheme: HTTP
        initialDelaySeconds: 90
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: workflow-controller
      ports:
      - containerPort: 9090
        name: metrics
        protocol: TCP
      - containerPort: 6060
        protocol: TCP
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d8tmg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000
    priorityClassName: workflow-controller
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
    serviceAccount: argo
    serviceAccountName: argo
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-d8tmg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-13T23:00:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-13T23:00:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://32490268bb2fc4c9e56f27ddbb4128c1f26cb9b8bef58387b72da1c920876a21
      image: quay.io/argoproj/workflow-controller:v3.5.2
      imageID: docker-pullable://quay.io/argoproj/workflow-controller@sha256:1c55865e168fcda8769fa1dfef0442cadb829e1f3b51aa305601b2cc34fe6c73
      lastState:
        terminated:
          containerID: docker://1667095b7bca09479ea600b513dd0483803ae18d8044194933d8f4b157dbec5e
          exitCode: 2
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Error
          startedAt: "2025-11-18T01:34:11Z"
      name: workflow-controller
      ready: true
      restartCount: 13
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:21Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d8tmg
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.114
    podIPs:
    - ip: 192.168.194.114
    - ip: fd07:b51a:cc66:a::1e06
    qosClass: Burstable
    startTime: "2025-11-13T23:00:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-14T17:11:32Z"
    generateName: agent-orchestrator-7847c9cb8c-
    labels:
      app: agent-orchestrator
      pod-template-hash: 7847c9cb8c
    name: agent-orchestrator-7847c9cb8c-wpx5c
    namespace: drupal-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: agent-orchestrator-7847c9cb8c
      uid: d09db007-17c8-491b-9307-8fa1705210ff
    resourceVersion: "7642934"
    uid: 7964df83-d686-47d1-8e2d-aafdcb9cb867
  spec:
    containers:
    - args:
      - -c
      - php -S 0.0.0.0:8080 -t /app
      command:
      - /bin/bash
      env:
      - name: DRUPAL_ROOT
        value: /app
      image: drupal:11-apache
      imagePullPolicy: IfNotPresent
      name: orchestrator
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 250m
          memory: 256Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /app
        name: app-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n9pvz
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /Users/flux423/Sites/Cannabis/indoorplantkingdom.com
        type: Directory
      name: app-volume
    - name: kube-api-access-n9pvz
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T17:11:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T17:11:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://5579c0d9def8c715871d2be28d1c67b5a7ad60706558cbe5b79d12e34855cd37
      image: drupal:11-apache
      imageID: docker-pullable://drupal@sha256:649413a25a1cf8fd58594d4911f57403ec17de438461ddaa07f7547f1e25b33b
      lastState:
        terminated:
          containerID: docker://313dd8e4165987a4455e46c76d3b8d28a0148cf5e8db52cdc0a60eb23fa5b5f5
          exitCode: 137
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Error
          startedAt: "2025-11-18T01:34:13Z"
      name: orchestrator
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:20Z"
      volumeMounts:
      - mountPath: /app
        name: app-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n9pvz
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.100
    podIPs:
    - ip: 192.168.194.100
    - ip: fd07:b51a:cc66:a::1df8
    qosClass: Burstable
    startTime: "2025-11-14T17:11:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-11T03:29:03Z"
    generateName: ingress-nginx-controller-5b8bf5478-
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.13.2
      helm.sh/chart: ingress-nginx-4.13.2
      pod-template-hash: 5b8bf5478
    name: ingress-nginx-controller-5b8bf5478-pkvgw
    namespace: ingress-nginx
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: ingress-nginx-controller-5b8bf5478
      uid: b4d730a9-604f-48d0-a1e2-1daf5fbefe21
    resourceVersion: "7643140"
    uid: 7ebb8c97-f7f6-4586-ae26-13f01eb8db3f
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - /nginx-ingress-controller
      - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
      - --election-id=ingress-nginx-leader
      - --controller-class=k8s.io/ingress-nginx
      - --ingress-class=nginx
      - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
      - --validating-webhook=:8443
      - --validating-webhook-certificate=/usr/local/certificates/cert
      - --validating-webhook-key=/usr/local/certificates/key
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: LD_PRELOAD
        value: /usr/local/lib/libmimalloc.so
      image: registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /wait-shutdown
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: controller
      ports:
      - containerPort: 80
        name: http
        protocol: TCP
      - containerPort: 443
        name: https
        protocol: TCP
      - containerPort: 8443
        name: webhook
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 90Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsGroup: 82
        runAsNonRoot: true
        runAsUser: 101
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /usr/local/certificates/
        name: webhook-cert
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rsh69
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: ingress-nginx
    serviceAccountName: ingress-nginx
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: webhook-cert
      secret:
        defaultMode: 420
        secretName: ingress-nginx-admission
    - name: kube-api-access-rsh69
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T17:13:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T17:13:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://511c2f9744a81e6c48ac68cf72876eb0c7474e1d919dc91377af4de04299b499
      image: sha256:5ed383cb88c3470aedf7c0117d988df816e23e0b9099534baad143d608998026
      imageID: docker-pullable://registry.k8s.io/ingress-nginx/controller@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef
      lastState:
        terminated:
          containerID: docker://9e0a258c9edb8707f3b611551fbb4e4b50ebac1a25038acf34c5b3b157b7b2c8
          exitCode: 137
          finishedAt: "2025-11-18T01:36:44Z"
          reason: Error
          startedAt: "2025-11-18T01:34:12Z"
      name: controller
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:20Z"
      volumeMounts:
      - mountPath: /usr/local/certificates/
        name: webhook-cert
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rsh69
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.99
    podIPs:
    - ip: 192.168.194.99
    - ip: fd07:b51a:cc66:a::1df7
    qosClass: Burstable
    startTime: "2025-11-11T17:13:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-11T03:29:03Z"
    generateName: keda-admission-webhooks-6656fbbb89-
    labels:
      app: keda-admission-webhooks
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: keda
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: keda-admission-webhooks
      app.kubernetes.io/part-of: keda-operator
      app.kubernetes.io/version: 2.18.0
      helm.sh/chart: keda-2.18.0
      name: keda-admission-webhooks
      pod-template-hash: 6656fbbb89
    name: keda-admission-webhooks-6656fbbb89-sqv2x
    namespace: keda
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: keda-admission-webhooks-6656fbbb89
      uid: 8986819c-ec54-4ec4-bdcf-fc0d00076c7f
    resourceVersion: "7643401"
    uid: f4a65a35-8cd7-4713-b92b-6087ac0cfc37
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --zap-log-level=info
      - --zap-encoder=console
      - --zap-time-encoding=rfc3339
      - --cert-dir=/certs
      - --health-probe-bind-address=:8081
      - --metrics-bind-address=:8080
      command:
      - /keda-admission-webhooks
      env:
      - name: WATCH_NAMESPACE
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: ghcr.io/kedacore/keda-admission-webhooks:2.18.0
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 25
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: keda-admission-webhooks
      ports:
      - containerPort: 9443
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 3
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "1"
          memory: 1000Mi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /certs
        name: certificates
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-45nf7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
    serviceAccount: keda-webhook
    serviceAccountName: keda-webhook
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: certificates
      secret:
        defaultMode: 420
        secretName: kedaorg-certs
    - name: kube-api-access-45nf7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T17:13:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T17:13:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://a1caa3bdfdd34c21c4f636f4b242f03edf60bfe85e029aa90ce4655f74939d3f
      image: ghcr.io/kedacore/keda-admission-webhooks:2.18.0
      imageID: docker-pullable://ghcr.io/kedacore/keda-admission-webhooks@sha256:95f923ff25e397519ae2d1ce98efd565d4c431dbfff087e065605c3d4eb1b1fa
      lastState:
        terminated:
          containerID: docker://81b0f30c7569026946a4ae345128c83f97d0b340b2e5b81f0e84bf35f5a4c9ae
          exitCode: 0
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Completed
          startedAt: "2025-11-18T01:34:14Z"
      name: keda-admission-webhooks
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:22Z"
      volumeMounts:
      - mountPath: /certs
        name: certificates
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-45nf7
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.94
    podIPs:
    - ip: 192.168.194.94
    - ip: fd07:b51a:cc66:a::1df2
    qosClass: Burstable
    startTime: "2025-11-11T17:13:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-11T03:29:03Z"
    generateName: keda-operator-6687649c57-
    labels:
      app: keda-operator
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: keda
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: keda-operator
      app.kubernetes.io/part-of: keda-operator
      app.kubernetes.io/version: 2.18.0
      helm.sh/chart: keda-2.18.0
      name: keda-operator
      pod-template-hash: 6687649c57
    name: keda-operator-6687649c57-jvp28
    namespace: keda
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: keda-operator-6687649c57
      uid: c1ae0db0-cf9d-4272-b509-b2f52b7af8ef
    resourceVersion: "7643339"
    uid: 13ea2b1e-83cf-4915-942d-170c6323b221
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --leader-elect
      - --disable-compression=true
      - --zap-log-level=info
      - --zap-encoder=console
      - --zap-time-encoding=rfc3339
      - --enable-webhook-patching=true
      - --cert-dir=/certs
      - --enable-cert-rotation=true
      - --cert-secret-name=kedaorg-certs
      - --operator-service-name=keda-operator
      - --metrics-server-service-name=keda-operator-metrics-apiserver
      - --webhooks-service-name=keda-admission-webhooks
      - --k8s-cluster-name=kubernetes-default
      - --k8s-cluster-domain=cluster.local
      - --enable-prometheus-metrics=false
      command:
      - /keda
      env:
      - name: WATCH_NAMESPACE
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: OPERATOR_NAME
        value: keda-operator
      - name: KEDA_HTTP_DEFAULT_TIMEOUT
        value: "3000"
      - name: KEDA_HTTP_MIN_TLS_VERSION
        value: TLS12
      image: ghcr.io/kedacore/keda:2.18.0
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 25
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: keda-operator
      ports:
      - containerPort: 9666
        name: metricsservice
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 3
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "1"
          memory: 1000Mi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /certs
        name: certificates
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j8z4k
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
    serviceAccount: keda-operator
    serviceAccountName: keda-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: certificates
      secret:
        defaultMode: 420
        optional: true
        secretName: kedaorg-certs
    - name: kube-api-access-j8z4k
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T17:13:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:42Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:42Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T17:13:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://483cdaf4e614a7f73efbd9bfa955c8df767af4427f15cce7cf7192255bc240a8
      image: ghcr.io/kedacore/keda:2.18.0
      imageID: docker-pullable://ghcr.io/kedacore/keda@sha256:fe551cf11fbf720af72ad37da041dd7bc73f7eed81ca55cc0d2507d52e1d24a2
      lastState:
        terminated:
          containerID: docker://d64d63f4873becaa26cc7575685961d0012dfe62dd2ee39d426052be0835aa87
          exitCode: 0
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Completed
          startedAt: "2025-11-18T01:34:14Z"
      name: keda-operator
      ready: true
      restartCount: 12
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:22Z"
      volumeMounts:
      - mountPath: /certs
        name: certificates
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j8z4k
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.98
    podIPs:
    - ip: 192.168.194.98
    - ip: fd07:b51a:cc66:a::1df6
    qosClass: Burstable
    startTime: "2025-11-11T17:13:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-11T02:58:43Z"
    generateName: keda-operator-metrics-apiserver-74bcc6f665-
    labels:
      app: keda-operator-metrics-apiserver
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: keda
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: keda-operator-metrics-apiserver
      app.kubernetes.io/part-of: keda-operator
      app.kubernetes.io/version: 2.18.0
      helm.sh/chart: keda-2.18.0
      pod-template-hash: 74bcc6f665
    name: keda-operator-metrics-apiserver-74bcc6f665-c6bpt
    namespace: keda
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: keda-operator-metrics-apiserver-74bcc6f665
      uid: bb11dbb5-7f85-4a1d-a846-0cb86905c955
    resourceVersion: "7643076"
    uid: 863292c7-3eff-4ff5-aa09-3d008d379642
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --port=8080
      - --secure-port=6443
      - --logtostderr=true
      - --stderrthreshold=ERROR
      - --disable-compression=true
      - --metrics-service-address=keda-operator.keda.svc.cluster.local:9666
      - --client-ca-file=/certs/ca.crt
      - --tls-cert-file=/certs/tls.crt
      - --tls-private-key-file=/certs/tls.key
      - --cert-dir=/certs
      - --v=0
      command:
      - /keda-adapter
      env:
      - name: WATCH_NAMESPACE
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: KEDA_HTTP_DEFAULT_TIMEOUT
        value: "3000"
      - name: KEDA_HTTP_MIN_TLS_VERSION
        value: TLS12
      image: ghcr.io/kedacore/keda-metrics-apiserver:2.18.0
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 6443
          scheme: HTTPS
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: keda-operator-metrics-apiserver
      ports:
      - containerPort: 6443
        name: https
        protocol: TCP
      - containerPort: 8080
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 6443
          scheme: HTTPS
        initialDelaySeconds: 5
        periodSeconds: 3
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "1"
          memory: 1000Mi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /certs
        name: certificates
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pzs5m
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
    serviceAccount: keda-metrics-server
    serviceAccountName: keda-metrics-server
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: certificates
      secret:
        defaultMode: 420
        secretName: kedaorg-certs
    - name: kube-api-access-pzs5m
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T17:13:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:27Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:27Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T17:13:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://94f8fe99f48e0e0615053b568b3df0c90b22796f0650c7ae31df4b612edd2e63
      image: ghcr.io/kedacore/keda-metrics-apiserver:2.18.0
      imageID: docker-pullable://ghcr.io/kedacore/keda-metrics-apiserver@sha256:c6f0eb5551d60fd0434ee75218ddf2fe4cb41a6b82fc2b50fa5a3f15f9a662b4
      lastState:
        terminated:
          containerID: docker://02e69e11a9f5cb85c398c4e4324d9c26e3f7c6568ffe0a721cb06cdc242fd519
          exitCode: 0
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Completed
          startedAt: "2025-11-18T01:34:15Z"
      name: keda-operator-metrics-apiserver
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:22Z"
      volumeMounts:
      - mountPath: /certs
        name: certificates
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pzs5m
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.106
    podIPs:
    - ip: 192.168.194.106
    - ip: fd07:b51a:cc66:a::1dfe
    qosClass: Burstable
    startTime: "2025-11-11T17:13:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      deployment-wave: "3"
      generated-by: agent-swarm
      issues-closed: "79"
    creationTimestamp: "2025-11-17T08:00:41Z"
    generateName: khook-674bc6d8c9-
    labels:
      app: khook
      managed-by: agent-buildkit
      ossa-version: v0.2.3
      platform: kagent
      pod-template-hash: 674bc6d8c9
    name: khook-674bc6d8c9-f97r7
    namespace: khook-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: khook-674bc6d8c9
      uid: d9927898-7742-4784-bae4-80a4d05c8568
    resourceVersion: "7512359"
    uid: adb1e0a1-d390-4826-b87d-3b3570e240d7
  spec:
    containers:
    - env:
      - name: KUBERNETES_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: WEBHOOK_PORT
        value: "8443"
      - name: KAGENT_ENABLED
        value: "true"
      - name: NATS_URL
        value: nats://nats.kagent-system.svc.cluster.local:4222
      image: ghcr.io/kagent/khook:latest
      imagePullPolicy: Always
      name: khook
      ports:
      - containerPort: 8443
        name: webhook
        protocol: TCP
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/webhook/certs
        name: webhook-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-44v9f
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: khook
    serviceAccountName: khook
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: webhook-certs
      secret:
        defaultMode: 420
        secretName: khook-webhook-certs
    - name: kube-api-access-44v9f
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T08:05:28Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T08:05:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T08:05:28Z"
      message: 'containers with unready status: [khook]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T08:05:28Z"
      message: 'containers with unready status: [khook]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T08:05:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - image: ghcr.io/kagent/khook:latest
      imageID: ""
      lastState: {}
      name: khook
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: ContainerCreating
      volumeMounts:
      - mountPath: /etc/webhook/certs
        name: webhook-certs
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-44v9f
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Pending
    qosClass: Burstable
    startTime: "2025-11-17T08:05:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-12-22T01:30:24Z"
    generateName: coredns-66cc6945cb-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 66cc6945cb
    name: coredns-66cc6945cb-h5668
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-66cc6945cb
      uid: 5fa6d5db-e4b7-48d1-a432-a852c39f2db0
    resourceVersion: "7642944"
    uid: ba2e4aa4-3105-4141-b3ab-35df59b6a0ce
  spec:
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: rancher/mirrored-coredns-coredns:1.10.1
      imagePullPolicy: IfNotPresent
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      resources:
        limits:
          memory: 340Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w97rn
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: orbstack
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          k8s-app: kube-dns
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        - key: NodeHosts
          path: NodeHosts
        name: coredns
      name: config-volume
    - configMap:
        defaultMode: 420
        name: coredns-custom
        optional: true
      name: custom-config-volume
    - name: kube-api-access-w97rn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-12-22T01:30:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-12-22T01:30:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://2c2b13684fe2cdb4725759c2ff758e9341b6ec93e67612c0b0a2f823a4df7665
      image: rancher/mirrored-coredns-coredns:1.10.1
      imageID: docker-pullable://rancher/mirrored-coredns-coredns@sha256:a11fafae1f8037cbbd66c5afa40ba2423936b72b4fd50a7034a7e8b955163594
      lastState:
        terminated:
          containerID: docker://6f3b950f373413261c3a7f52a3c13279d2f63c828f2c3f9dd4a71ec201998f40
          exitCode: 0
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Completed
          startedAt: "2025-11-18T01:34:13Z"
      name: coredns
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:21Z"
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w97rn
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.109
    podIPs:
    - ip: 192.168.194.109
    - ip: fd07:b51a:cc66:a::1e01
    qosClass: Burstable
    startTime: "2024-12-22T01:30:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-10T04:43:54Z"
    generateName: local-path-provisioner-777cb94f89-
    labels:
      app: local-path-provisioner
      pod-template-hash: 777cb94f89
    name: local-path-provisioner-777cb94f89-rvdmp
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: local-path-provisioner-777cb94f89
      uid: 3ea7ba0a-7d65-4ea9-a33f-4819c9bef530
    resourceVersion: "7642962"
    uid: 50bf064d-4938-40ae-ad93-d9e7ff0cf297
  spec:
    containers:
    - command:
      - local-path-provisioner
      - start
      - --config
      - /etc/config/config.json
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: rancher/local-path-provisioner:v0.0.31
      imagePullPolicy: IfNotPresent
      name: local-path-provisioner
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config/
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wn2h7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: local-path-provisioner-service-account
    serviceAccountName: local-path-provisioner-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: local-path-config
      name: config-volume
    - name: kube-api-access-wn2h7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-10T04:43:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-10T04:43:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://b42e24f7c353b773d437a5e2ac624d7785994104c6a6229c1ec0b6bc5812ea37
      image: rancher/local-path-provisioner:v0.0.31
      imageID: docker-pullable://rancher/local-path-provisioner@sha256:80496fdeb307541007621959aa13aed41d31db9cd2dc4167c19833e0bfa3878c
      lastState:
        terminated:
          containerID: docker://b7a95dc024631d32a5554a3a68dc7b186b1a942371d6a2a171175f282a9f55a9
          exitCode: 0
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Completed
          startedAt: "2025-11-18T01:34:11Z"
      name: local-path-provisioner
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:21Z"
      volumeMounts:
      - mountPath: /etc/config/
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wn2h7
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.116
    podIPs:
    - ip: 192.168.194.116
    - ip: fd07:b51a:cc66:a::1e08
    qosClass: Burstable
    startTime: "2025-11-10T04:43:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-09-07T02:48:49Z"
    generateName: metrics-server-867d48dc9c-
    labels:
      k8s-app: metrics-server
      pod-template-hash: 867d48dc9c
    name: metrics-server-867d48dc9c-kjnvk
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: metrics-server-867d48dc9c
      uid: ff1b092e-aac8-4849-acea-29fa44749738
    resourceVersion: "7643365"
    uid: 08af9876-9fd9-422a-8e59-f77945b151d0
  spec:
    containers:
    - args:
      - --cert-dir=/tmp
      - --secure-port=10250
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --kubelet-use-node-status-port
      - --metric-resolution=15s
      image: registry.k8s.io/metrics-server/metrics-server:v0.8.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: https
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: metrics-server
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: https
          scheme: HTTPS
        initialDelaySeconds: 20
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j296b
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: metrics-server
    serviceAccountName: metrics-server
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp-dir
    - name: kube-api-access-j296b
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:20Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-09-07T02:48:49Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:43Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:43Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-09-07T02:48:49Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://bec26c2d4dcab1faecb426f8eae23cce6bb17c4b8c82d2560bcae5804ca38caa
      image: registry.k8s.io/metrics-server/metrics-server:v0.8.0
      imageID: docker-pullable://registry.k8s.io/metrics-server/metrics-server@sha256:89258156d0e9af60403eafd44da9676fd66f600c7934d468ccc17e42b199aee2
      lastState:
        terminated:
          containerID: docker://1aa5942ae5574c2d21cad87f98454c50c00af47f49a85d8802dcc96943da4e1b
          exitCode: 0
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Completed
          startedAt: "2025-11-18T01:34:11Z"
      name: metrics-server
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:20Z"
      volumeMounts:
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j296b
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.93
    podIPs:
    - ip: 192.168.194.93
    - ip: fd07:b51a:cc66:a::1df1
    qosClass: Burstable
    startTime: "2025-09-07T02:48:49Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-10-04T15:48:21Z"
    generateName: svclb-frontend-42f5eca3-
    labels:
      app: svclb-frontend-42f5eca3
      controller-revision-hash: 5c7b65c79b
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: frontend
      svccontroller.k3s.cattle.io/svcnamespace: agent-chat
    name: svclb-frontend-42f5eca3-nqtnp
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-frontend-42f5eca3
      uid: 651aa697-5081-436e-931d-f97b17e53ca5
    resourceVersion: "7642830"
    uid: d585920f-ea0b-4a2a-8249-4cef9fc796a7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - orbstack
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "3000"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "3000"
      - name: DEST_IPS
        value: 192.168.194.164
      image: rancher/klipper-lb:v0.4.13
      imagePullPolicy: IfNotPresent
      name: lb-tcp-3000
      ports:
      - containerPort: 3000
        hostPort: 3000
        name: lb-tcp-3000
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:20Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-10-28T13:28:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:20Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:20Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-10-28T13:28:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://8e9f7976191f4e7736ed8fd3067d119d29f57e4145d749b71e6deb1f873fc38c
      image: rancher/klipper-lb:v0.4.13
      imageID: docker-pullable://rancher/klipper-lb@sha256:7eb86d5b908ec6ddd9796253d8cc2f43df99420fc8b8a18452a94dc56f86aca0
      lastState:
        terminated:
          containerID: docker://28c60e6ecc469fa6c664970ed59a9bff7e9960a95ced48456358cd6b1eecd824
          exitCode: 1
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Error
          startedAt: "2025-11-18T01:34:12Z"
      name: lb-tcp-3000
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:20Z"
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.91
    podIPs:
    - ip: 192.168.194.91
    - ip: fd07:b51a:cc66:a::1def
    qosClass: BestEffort
    startTime: "2025-10-28T13:28:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-11T21:07:43Z"
    generateName: svclb-grafana-service-bc805f8c-
    labels:
      app: svclb-grafana-service-bc805f8c
      controller-revision-hash: 6574d4745d
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: grafana-service
      svccontroller.k3s.cattle.io/svcnamespace: ossa-agents
    name: svclb-grafana-service-bc805f8c-hldzv
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-grafana-service-bc805f8c
      uid: fabaf0c6-32d7-4764-86ec-cb0f9bcd80c8
    resourceVersion: "7482370"
    uid: e791fb46-1813-457b-8e84-8a6d21c17c3b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - orbstack
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "3000"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "3000"
      - name: DEST_IPS
        value: 192.168.194.253
      image: rancher/klipper-lb:v0.4.13
      imagePullPolicy: IfNotPresent
      name: lb-tcp-3000
      ports:
      - containerPort: 3000
        hostPort: 3000
        name: lb-tcp-3000
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T21:07:43Z"
      message: '0/1 nodes are available: 1 node(s) didn''t have free ports for the
        requested pod ports. preemption: 0/1 nodes are available: 1 node(s) didn''t
        have free ports for the requested pod ports.'
      reason: Unschedulable
      status: "False"
      type: PodScheduled
    phase: Pending
    qosClass: BestEffort
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-09-07T03:15:30Z"
    generateName: svclb-ingress-nginx-controller-2227b54c-
    labels:
      app: svclb-ingress-nginx-controller-2227b54c
      controller-revision-hash: 5f9b6bcb69
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: ingress-nginx-controller
      svccontroller.k3s.cattle.io/svcnamespace: ingress-nginx
    name: svclb-ingress-nginx-controller-2227b54c-tn75l
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-ingress-nginx-controller-2227b54c
      uid: 8e1ee24a-9380-47e7-9b76-3360fc82ebfb
    resourceVersion: "7642922"
    uid: b73906a3-ef45-4e6b-a6f0-9b190154428e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - orbstack
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "80"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "80"
      - name: DEST_IPS
        value: 192.168.194.249
      image: rancher/klipper-lb:v0.4.13
      imagePullPolicy: IfNotPresent
      name: lb-tcp-80
      ports:
      - containerPort: 80
        hostPort: 80
        name: lb-tcp-80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    - env:
      - name: SRC_PORT
        value: "443"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "443"
      - name: DEST_IPS
        value: 192.168.194.249
      image: rancher/klipper-lb:v0.4.13
      imagePullPolicy: IfNotPresent
      name: lb-tcp-443
      ports:
      - containerPort: 443
        hostPort: 443
        name: lb-tcp-443
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-09-07T03:15:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-09-07T03:15:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://6bf5c61938465dc63a1994c675cefb6631ed58c2cb25b04a2aa6c11a913e9e05
      image: rancher/klipper-lb:v0.4.13
      imageID: docker-pullable://rancher/klipper-lb@sha256:7eb86d5b908ec6ddd9796253d8cc2f43df99420fc8b8a18452a94dc56f86aca0
      lastState:
        terminated:
          containerID: docker://5a2349c4e18ef88ecaa86759a74952e335b1c44b214bb3bf733f8b20c0bc3326
          exitCode: 1
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Error
          startedAt: "2025-11-18T01:34:13Z"
      name: lb-tcp-443
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:21Z"
    - containerID: docker://69c76c573444a3403db8a0a688c908ed95702112d47d47624fce004f50b48425
      image: rancher/klipper-lb:v0.4.13
      imageID: docker-pullable://rancher/klipper-lb@sha256:7eb86d5b908ec6ddd9796253d8cc2f43df99420fc8b8a18452a94dc56f86aca0
      lastState:
        terminated:
          containerID: docker://707b2866a393197018eed85d763188b6d7f104e48359fd2883795f36fff91d46
          exitCode: 1
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Error
          startedAt: "2025-11-18T01:34:11Z"
      name: lb-tcp-80
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:20Z"
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.92
    podIPs:
    - ip: 192.168.194.92
    - ip: fd07:b51a:cc66:a::1df0
    qosClass: BestEffort
    startTime: "2025-09-07T03:15:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-11T21:07:43Z"
    generateName: svclb-langfuse-3cff550a-
    labels:
      app: svclb-langfuse-3cff550a
      controller-revision-hash: f5b495dd8
      pod-template-generation: "2"
      svccontroller.k3s.cattle.io/svcname: langfuse
      svccontroller.k3s.cattle.io/svcnamespace: ossa-agents
    name: svclb-langfuse-3cff550a-pnfxs
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-langfuse-3cff550a
      uid: 1f0f3d2a-6fe1-4dbe-a30f-6dea22bbec01
    resourceVersion: "7482372"
    uid: 8d758d22-bffa-4c5f-a269-200120ef24ec
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - orbstack
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "80"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "80"
      - name: DEST_IPS
        value: 192.168.194.206
      image: rancher/klipper-lb:v0.4.13
      imagePullPolicy: IfNotPresent
      name: lb-tcp-80
      ports:
      - containerPort: 80
        hostPort: 80
        name: lb-tcp-80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T21:07:43Z"
      message: '0/1 nodes are available: 1 node(s) didn''t have free ports for the
        requested pod ports. preemption: 0/1 nodes are available: 1 node(s) didn''t
        have free ports for the requested pod ports.'
      reason: Unschedulable
      status: "False"
      type: PodScheduled
    phase: Pending
    qosClass: BestEffort
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-10-04T15:48:21Z"
    generateName: svclb-librechat-251848e0-
    labels:
      app: svclb-librechat-251848e0
      controller-revision-hash: 6b954b7c6c
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: librechat
      svccontroller.k3s.cattle.io/svcnamespace: agent-chat
    name: svclb-librechat-251848e0-s782f
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-librechat-251848e0
      uid: bd295bdd-4096-4d42-a946-964f520339c9
    resourceVersion: "7642883"
    uid: f99e0d4b-d755-47b0-8724-0959852431db
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - orbstack
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "3080"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "3080"
      - name: DEST_IPS
        value: 192.168.194.217
      image: rancher/klipper-lb:v0.4.13
      imagePullPolicy: IfNotPresent
      name: lb-tcp-3080
      ports:
      - containerPort: 3080
        hostPort: 3080
        name: lb-tcp-3080
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:21Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-10-04T15:48:21Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:21Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:21Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-10-04T15:48:21Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://d897dc2bc368a7e3f08313496b16ab33bb045877977297ebb8ab403d2104d139
      image: rancher/klipper-lb:v0.4.13
      imageID: docker-pullable://rancher/klipper-lb@sha256:7eb86d5b908ec6ddd9796253d8cc2f43df99420fc8b8a18452a94dc56f86aca0
      lastState:
        terminated:
          containerID: docker://5d0688d006b88575030f012d66f1287ecd94c2eeedfb78e2bdbc712552f07641
          exitCode: 1
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Error
          startedAt: "2025-11-18T01:34:12Z"
      name: lb-tcp-3080
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:19Z"
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.87
    podIPs:
    - ip: 192.168.194.87
    - ip: fd07:b51a:cc66:a::1deb
    qosClass: BestEffort
    startTime: "2025-10-04T15:48:21Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-10-10T04:25:36Z"
    generateName: svclb-litellm-66a5aca1-
    labels:
      app: svclb-litellm-66a5aca1
      controller-revision-hash: 55df6dfb9c
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: litellm
      svccontroller.k3s.cattle.io/svcnamespace: ossa-agents
    name: svclb-litellm-66a5aca1-vb6c7
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-litellm-66a5aca1
      uid: 5df05f0c-cce5-49e4-9dc2-74759520e9ec
    resourceVersion: "7642998"
    uid: e640cd3b-0d92-42dd-89d8-6a52b597c874
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - orbstack
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "4000"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "4000"
      - name: DEST_IPS
        value: 192.168.194.237
      image: rancher/klipper-lb:v0.4.13
      imagePullPolicy: IfNotPresent
      name: lb-tcp-4000
      ports:
      - containerPort: 4000
        hostPort: 4000
        name: lb-tcp-4000
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    - env:
      - name: SRC_PORT
        value: "4001"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "4001"
      - name: DEST_IPS
        value: 192.168.194.237
      image: rancher/klipper-lb:v0.4.13
      imagePullPolicy: IfNotPresent
      name: lb-tcp-4001
      ports:
      - containerPort: 4001
        hostPort: 4001
        name: lb-tcp-4001
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-10-14T03:10:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-10-14T03:10:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://d95c22fc77703d7d1c0ecc4c2677bb1817dc9b440a50d21a7f9eb7f95f0d64b5
      image: rancher/klipper-lb:v0.4.13
      imageID: docker-pullable://rancher/klipper-lb@sha256:7eb86d5b908ec6ddd9796253d8cc2f43df99420fc8b8a18452a94dc56f86aca0
      lastState:
        terminated:
          containerID: docker://1092fee448dacfed21761185241fee2e25d4b9780ef23131b92b2e8b7de33a54
          exitCode: 1
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Error
          startedAt: "2025-11-18T01:34:13Z"
      name: lb-tcp-4000
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:20Z"
    - containerID: docker://4ddc25a326aa95382d57673744b7c2e05fc72e815ceb01c0e46bd272737b1e12
      image: rancher/klipper-lb:v0.4.13
      imageID: docker-pullable://rancher/klipper-lb@sha256:7eb86d5b908ec6ddd9796253d8cc2f43df99420fc8b8a18452a94dc56f86aca0
      lastState:
        terminated:
          containerID: docker://a370f9c22f294636a86d36a1f253dc0f5cf220ca4f7bae2e70f0f610eefda318
          exitCode: 1
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Error
          startedAt: "2025-11-18T01:34:14Z"
      name: lb-tcp-4001
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:21Z"
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.90
    podIPs:
    - ip: 192.168.194.90
    - ip: fd07:b51a:cc66:a::1dee
    qosClass: BestEffort
    startTime: "2025-10-14T03:10:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-10-15T14:26:35Z"
    generateName: svclb-mlflow-01455b95-
    labels:
      app: svclb-mlflow-01455b95
      controller-revision-hash: 589f8c478f
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: mlflow
      svccontroller.k3s.cattle.io/svcnamespace: ai-ml
    name: svclb-mlflow-01455b95-z7rc8
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-mlflow-01455b95
      uid: 6f0a1430-6027-47f9-b9d3-0c9804b45059
    resourceVersion: "7642825"
    uid: 4601ae6a-667c-42df-9eae-00607270fe32
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - orbstack
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "5000"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "5000"
      - name: DEST_IPS
        value: 192.168.194.228
      image: rancher/klipper-lb:v0.4.13
      imagePullPolicy: IfNotPresent
      name: lb-tcp-5000
      ports:
      - containerPort: 5000
        hostPort: 5000
        name: lb-tcp-5000
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:20Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-10-15T14:26:35Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:20Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:20Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-10-15T14:26:35Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://f7e14a1224fe06233c259ca4ee7713fc2c4ea05c095e6bc1c77d4f19413b37df
      image: rancher/klipper-lb:v0.4.13
      imageID: docker-pullable://rancher/klipper-lb@sha256:7eb86d5b908ec6ddd9796253d8cc2f43df99420fc8b8a18452a94dc56f86aca0
      lastState:
        terminated:
          containerID: docker://4128335379af007b694b6155676ccad5b3bc9a9612fe9ebfa9672bdb32f88d22
          exitCode: 1
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Error
          startedAt: "2025-11-18T01:34:12Z"
      name: lb-tcp-5000
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:20Z"
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.88
    podIPs:
    - ip: 192.168.194.88
    - ip: fd07:b51a:cc66:a::1dec
    qosClass: BestEffort
    startTime: "2025-10-15T14:26:35Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-10-09T20:34:12Z"
    generateName: svclb-phoenix-378c73ce-
    labels:
      app: svclb-phoenix-378c73ce
      controller-revision-hash: 55f497df49
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: phoenix
      svccontroller.k3s.cattle.io/svcnamespace: phoenix
    name: svclb-phoenix-378c73ce-q8bc8
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-phoenix-378c73ce
      uid: f1839462-abed-4f3b-9d0c-472883b776ea
    resourceVersion: "7642995"
    uid: 9d4a7379-fc27-4ca5-8f1f-315b3f7a28be
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - orbstack
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "6006"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "6006"
      - name: DEST_IPS
        value: 192.168.194.179
      image: rancher/klipper-lb:v0.4.13
      imagePullPolicy: IfNotPresent
      name: lb-tcp-6006
      ports:
      - containerPort: 6006
        hostPort: 6006
        name: lb-tcp-6006
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    - env:
      - name: SRC_PORT
        value: "4317"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "4317"
      - name: DEST_IPS
        value: 192.168.194.179
      image: rancher/klipper-lb:v0.4.13
      imagePullPolicy: IfNotPresent
      name: lb-tcp-4317
      ports:
      - containerPort: 4317
        hostPort: 4317
        name: lb-tcp-4317
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    - env:
      - name: SRC_PORT
        value: "4318"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "4318"
      - name: DEST_IPS
        value: 192.168.194.179
      image: rancher/klipper-lb:v0.4.13
      imagePullPolicy: IfNotPresent
      name: lb-tcp-4318
      ports:
      - containerPort: 4318
        hostPort: 4318
        name: lb-tcp-4318
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-10-09T20:34:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-10-09T20:34:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://3c6e78150a6c58457e1d5c9a524b5ba9bcb46e54b41104ab7edb555a2c460401
      image: rancher/klipper-lb:v0.4.13
      imageID: docker-pullable://rancher/klipper-lb@sha256:7eb86d5b908ec6ddd9796253d8cc2f43df99420fc8b8a18452a94dc56f86aca0
      lastState:
        terminated:
          containerID: docker://c2d2f0e936b5f9fee9be38f18a706b19510a2936372619b3fad665fdf7198320
          exitCode: 1
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Error
          startedAt: "2025-11-18T01:34:16Z"
      name: lb-tcp-4317
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:21Z"
    - containerID: docker://57282b7e4b4f0a49543e9e054eac981793a14653a73e54869824791b71e55a0b
      image: rancher/klipper-lb:v0.4.13
      imageID: docker-pullable://rancher/klipper-lb@sha256:7eb86d5b908ec6ddd9796253d8cc2f43df99420fc8b8a18452a94dc56f86aca0
      lastState:
        terminated:
          containerID: docker://2d1e1f081d39aa0b4806589d53827fe8cc00b402d74d4631a53565da05efb740
          exitCode: 1
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Error
          startedAt: "2025-11-18T01:34:17Z"
      name: lb-tcp-4318
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:21Z"
    - containerID: docker://ce7148676dd5958fad28f13c94cd04d91d945b1bf51a9464347fb692f8278eb4
      image: rancher/klipper-lb:v0.4.13
      imageID: docker-pullable://rancher/klipper-lb@sha256:7eb86d5b908ec6ddd9796253d8cc2f43df99420fc8b8a18452a94dc56f86aca0
      lastState:
        terminated:
          containerID: docker://abeeb457c8e4fa13899d029e3a0e8ef2e93c7af1c86cddf09f841d49081349e7
          exitCode: 1
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Error
          startedAt: "2025-11-18T01:34:13Z"
      name: lb-tcp-6006
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:19Z"
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.86
    podIPs:
    - ip: 192.168.194.86
    - ip: fd07:b51a:cc66:a::1dea
    qosClass: BestEffort
    startTime: "2025-10-09T20:34:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-04T02:05:05Z"
    generateName: svclb-prometheus-service-4117498d-
    labels:
      app: svclb-prometheus-service-4117498d
      controller-revision-hash: 6644b8cf5d
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: prometheus-service
      svccontroller.k3s.cattle.io/svcnamespace: ossa-agents
    name: svclb-prometheus-service-4117498d-zk5rp
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-prometheus-service-4117498d
      uid: 5d8d5bd2-9d04-4351-b1ad-bdbf4aa3fb18
    resourceVersion: "7642989"
    uid: b301d335-9170-4f63-9640-551dd9c80cf3
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - orbstack
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "9090"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "9090"
      - name: DEST_IPS
        value: 192.168.194.146
      image: rancher/klipper-lb:v0.4.13
      imagePullPolicy: IfNotPresent
      name: lb-tcp-9090
      ports:
      - containerPort: 9090
        hostPort: 9090
        name: lb-tcp-9090
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-04T02:05:05Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-04T02:05:05Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://2279e05519d334ae99e01b8c9fb0e3c2badb2c92b7bd21ea39815432441991e1
      image: rancher/klipper-lb:v0.4.13
      imageID: docker-pullable://rancher/klipper-lb@sha256:7eb86d5b908ec6ddd9796253d8cc2f43df99420fc8b8a18452a94dc56f86aca0
      lastState:
        terminated:
          containerID: docker://1a052698eace2ddee816622b5984fae71ad4c5e44b72781b05041c4ac066ac06
          exitCode: 1
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Error
          startedAt: "2025-11-18T01:34:12Z"
      name: lb-tcp-9090
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:19Z"
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.85
    podIPs:
    - ip: 192.168.194.85
    - ip: fd07:b51a:cc66:a::1de9
    qosClass: BestEffort
    startTime: "2025-11-04T02:05:05Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-09-09T21:18:20Z"
    generateName: svclb-qdrant-service-e2ff5ec3-
    labels:
      app: svclb-qdrant-service-e2ff5ec3
      controller-revision-hash: 5b5c55f8f8
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: qdrant-service
      svccontroller.k3s.cattle.io/svcnamespace: ossa-agents
    name: svclb-qdrant-service-e2ff5ec3-msf9j
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-qdrant-service-e2ff5ec3
      uid: 00b568f3-00ba-4a9d-b578-afcde76a380c
    resourceVersion: "7642950"
    uid: 8d6f9f65-e246-4e80-99d3-5e3047394db9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - orbstack
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "6333"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "6333"
      - name: DEST_IPS
        value: 192.168.194.213
      image: rancher/klipper-lb:v0.4.13
      imagePullPolicy: IfNotPresent
      name: lb-tcp-6333
      ports:
      - containerPort: 6333
        hostPort: 6333
        name: lb-tcp-6333
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    - env:
      - name: SRC_PORT
        value: "6334"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "6334"
      - name: DEST_IPS
        value: 192.168.194.213
      image: rancher/klipper-lb:v0.4.13
      imagePullPolicy: IfNotPresent
      name: lb-tcp-6334
      ports:
      - containerPort: 6334
        hostPort: 6334
        name: lb-tcp-6334
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-09-09T21:18:20Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-09-09T21:18:20Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://052dd4184227e056643b2abfe95805b314b18c6ee0fa8827cc35fefeb22b8ce0
      image: rancher/klipper-lb:v0.4.13
      imageID: docker-pullable://rancher/klipper-lb@sha256:7eb86d5b908ec6ddd9796253d8cc2f43df99420fc8b8a18452a94dc56f86aca0
      lastState:
        terminated:
          containerID: docker://258dffa643e24898ad613e3525a5c55fef0b3d5800b03de60d238f7535f8c0b7
          exitCode: 1
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Error
          startedAt: "2025-11-18T01:34:11Z"
      name: lb-tcp-6333
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:20Z"
    - containerID: docker://e4f2a382b1f1f6b8b1dcd29c05c79d2c54efabbf1a4d3910be59f99cb92b7e5f
      image: rancher/klipper-lb:v0.4.13
      imageID: docker-pullable://rancher/klipper-lb@sha256:7eb86d5b908ec6ddd9796253d8cc2f43df99420fc8b8a18452a94dc56f86aca0
      lastState:
        terminated:
          containerID: docker://36e7648ebdd321edeb7b565d4513ec1159ab07f99621b3f917de53fc7ebb0e30
          exitCode: 1
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Error
          startedAt: "2025-11-18T01:34:13Z"
      name: lb-tcp-6334
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:21Z"
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.89
    podIPs:
    - ip: 192.168.194.89
    - ip: fd07:b51a:cc66:a::1ded
    qosClass: BestEffort
    startTime: "2025-09-09T21:18:20Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-11T21:07:43Z"
    generateName: svclb-vault-ui-ddd0f82a-
    labels:
      app: svclb-vault-ui-ddd0f82a
      controller-revision-hash: 74857cbb6d
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: vault-ui
      svccontroller.k3s.cattle.io/svcnamespace: vault
    name: svclb-vault-ui-ddd0f82a-fhjln
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-vault-ui-ddd0f82a
      uid: 86c325f2-0818-439e-aa88-e297113ca88c
    resourceVersion: "7482374"
    uid: 37709c86-9202-43fd-8f69-bdd55f431144
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - orbstack
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "443"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "443"
      - name: DEST_IPS
        value: 192.168.194.245
      image: rancher/klipper-lb:v0.4.13
      imagePullPolicy: IfNotPresent
      name: lb-tcp-443
      ports:
      - containerPort: 443
        hostPort: 443
        name: lb-tcp-443
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T21:07:43Z"
      message: '0/1 nodes are available: 1 node(s) didn''t have free ports for the
        requested pod ports. preemption: 0/1 nodes are available: 1 node(s) didn''t
        have free ports for the requested pod ports.'
      reason: Unschedulable
      status: "False"
      type: PodScheduled
    phase: Pending
    qosClass: BestEffort
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-17T19:58:34Z"
    generateName: litellm-proxy-7f46445f6c-
    labels:
      app: litellm
      pod-template-hash: 7f46445f6c
    name: litellm-proxy-7f46445f6c-l95tj
    namespace: llm-inference
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: litellm-proxy-7f46445f6c
      uid: b88e92da-3b07-4687-8426-dc6dc2bc0d5c
    resourceVersion: "7647849"
    uid: 071985b1-a9c4-42ef-806e-bc39fa5001b8
  spec:
    containers:
    - args:
      - --config
      - /app/config.yaml
      - --port
      - "4000"
      - --num_workers
      - "4"
      command:
      - litellm
      image: ghcr.io/berriai/litellm:main-latest
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 4000
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: litellm
      ports:
      - containerPort: 4000
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 4000
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 512Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /app
        name: config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nffjt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: litellm-config
      name: config
    - name: kube-api-access-nffjt
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T23:59:09Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T23:59:09Z"
      message: 'containers with unready status: [litellm]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T23:59:09Z"
      message: 'containers with unready status: [litellm]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T23:59:09Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://26b080a3f0208422c131709d19231577492256c6e69ea69f3dd90356c83759b6
      image: ghcr.io/berriai/litellm:main-latest
      imageID: docker-pullable://ghcr.io/berriai/litellm@sha256:9d3c9ccf539baf17ef6c78375362751d39f4e19c739b05984ddc4f877ab7cc62
      lastState:
        terminated:
          containerID: docker://26b080a3f0208422c131709d19231577492256c6e69ea69f3dd90356c83759b6
          exitCode: 0
          finishedAt: "2025-11-18T02:22:35Z"
          reason: Completed
          startedAt: "2025-11-18T02:21:35Z"
      name: litellm
      ready: false
      restartCount: 42
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=litellm pod=litellm-proxy-7f46445f6c-l95tj_llm-inference(071985b1-a9c4-42ef-806e-bc39fa5001b8)
          reason: CrashLoopBackOff
      volumeMounts:
      - mountPath: /app
        name: config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nffjt
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.108
    podIPs:
    - ip: 192.168.194.108
    - ip: fd07:b51a:cc66:a::1e00
    qosClass: Burstable
    startTime: "2025-11-17T23:59:09Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-17T19:58:34Z"
    finalizers:
    - batch.kubernetes.io/job-tracking
    generateName: ollama-pull-models-
    labels:
      batch.kubernetes.io/controller-uid: 08baaa97-304e-44fb-ad60-829e97e50d2b
      batch.kubernetes.io/job-name: ollama-pull-models
      controller-uid: 08baaa97-304e-44fb-ad60-829e97e50d2b
      job-name: ollama-pull-models
    name: ollama-pull-models-8lfmz
    namespace: llm-inference
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: ollama-pull-models
      uid: 08baaa97-304e-44fb-ad60-829e97e50d2b
    resourceVersion: "7642979"
    uid: ae0877bb-a124-416e-b2d2-c356f1a854f1
  spec:
    containers:
    - command:
      - /bin/sh
      - -c
      - |
        # Wait for Ollama to be ready
        until curl -f http://ollama:11434; do
          echo "Waiting for Ollama..."
          sleep 5
        done

        # Pull free models for different tasks
        curl -X POST http://ollama:11434/api/pull -d '{"name": "qwen2.5-coder:7b"}'
        curl -X POST http://ollama:11434/api/pull -d '{"name": "llama3.2:3b"}'
        curl -X POST http://ollama:11434/api/pull -d '{"name": "deepseek-coder:6.7b"}'

        echo "Models pulled successfully!"
      image: curlimages/curl:latest
      imagePullPolicy: Always
      name: pull-models
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wcqhz
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-wcqhz
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T19:58:34Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T19:58:34Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://561f485440c3f58dbdcefbeb76bbf38d1096878f3a4c5803045f3ad3db339b1b
      image: curlimages/curl:latest
      imageID: docker-pullable://curlimages/curl@sha256:935d9100e9ba842cdb060de42472c7ca90cfe9a7c96e4dacb55e79e560b3ff40
      lastState:
        terminated:
          containerID: docker://ec5b281f16a1f6454e23928c998d1177f19ace3865a1af96137461dd93e428ec
          exitCode: 137
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Error
          startedAt: "2025-11-18T01:34:14Z"
      name: pull-models
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:22Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wcqhz
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.104
    podIPs:
    - ip: 192.168.194.104
    - ip: fd07:b51a:cc66:a::1dfc
    qosClass: BestEffort
    startTime: "2025-11-17T19:58:34Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580
      checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
      checksum/scripts: a9a88a19d473fec18995a3083aa78788c24d98ad662f8eb697f9426b962bbd61
      checksum/secret: f3b30bdb3e109ce14d4714de475ecb9f805ec15fa55fdf70e527ef53c0ddd1e6
    creationTimestamp: "2025-11-11T03:28:57Z"
    generateName: redis-master-
    labels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: redis
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 8.2.2
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: redis-master-cc984cc8c
      helm.sh/chart: redis-23.1.3
      statefulset.kubernetes.io/pod-name: redis-master-0
    name: redis-master-0
    namespace: llm-platform
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: redis-master
      uid: 2703949c-5ceb-4c09-8470-ad67dcf601d8
    resourceVersion: "7643372"
    uid: e9d84e6e-06f3-4110-b021-54b0a11a8793
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: master
                app.kubernetes.io/instance: redis
                app.kubernetes.io/name: redis
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - args:
      - -ec
      - /opt/bitnami/scripts/start-scripts/start-master.sh
      command:
      - /bin/bash
      env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: REDIS_REPLICATION_MODE
        value: master
      - name: ALLOW_EMPTY_PASSWORD
        value: "no"
      - name: REDIS_PASSWORD_FILE
        value: /opt/bitnami/redis/secrets/redis-password
      - name: REDIS_TLS_ENABLED
        value: "no"
      - name: REDIS_PORT
        value: "6379"
      image: registry-1.docker.io/bitnami/redis:latest
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/bash
          - -ec
          - /health/ping_liveness_local.sh 5
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 6
      name: redis
      ports:
      - containerPort: 6379
        name: redis
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/bash
          - -ec
          - /health/ping_readiness_local.sh 1
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        limits:
          cpu: 150m
          ephemeral-storage: 2Gi
          memory: 192Mi
        requests:
          cpu: 100m
          ephemeral-storage: 50Mi
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/bitnami/scripts/start-scripts
        name: start-scripts
      - mountPath: /health
        name: health
      - mountPath: /opt/bitnami/redis/secrets/
        name: redis-password
      - mountPath: /data
        name: redis-data
      - mountPath: /opt/bitnami/redis/mounted-etc
        name: config
      - mountPath: /opt/bitnami/redis/etc/
        name: empty-dir
        subPath: app-conf-dir
      - mountPath: /tmp
        name: empty-dir
        subPath: tmp-dir
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: redis-master-0
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
    serviceAccount: redis-master
    serviceAccountName: redis-master
    subdomain: redis-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: redis-data
      persistentVolumeClaim:
        claimName: redis-data-redis-master-0
    - configMap:
        defaultMode: 493
        name: redis-scripts
      name: start-scripts
    - configMap:
        defaultMode: 493
        name: redis-health
      name: health
    - name: redis-password
      secret:
        defaultMode: 420
        items:
        - key: redis-password
          path: redis-password
        secretName: redis
    - configMap:
        defaultMode: 420
        name: redis-configuration
      name: config
    - emptyDir: {}
      name: empty-dir
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:21Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T17:13:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:43Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:43Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T17:13:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://eba3ca6e7fbae772113f290ccd371f8b098e8f1f1da2b1e4899cce87f8d3be42
      image: registry-1.docker.io/bitnami/redis:latest
      imageID: docker-pullable://registry-1.docker.io/bitnami/redis@sha256:f01f5f06f4c0aadc78bd396109c16be5f46343b39840bdb32a64d9f57a2c7ce7
      lastState:
        terminated:
          containerID: docker://43c89dd4220bf43755bce960f777525659b8827ba496c527b830e4b4e014b28e
          exitCode: 0
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Completed
          startedAt: "2025-11-18T01:34:12Z"
      name: redis
      ready: true
      restartCount: 13
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:20Z"
      volumeMounts:
      - mountPath: /opt/bitnami/scripts/start-scripts
        name: start-scripts
      - mountPath: /health
        name: health
      - mountPath: /opt/bitnami/redis/secrets/
        name: redis-password
      - mountPath: /data
        name: redis-data
      - mountPath: /opt/bitnami/redis/mounted-etc
        name: config
      - mountPath: /opt/bitnami/redis/etc/
        name: empty-dir
      - mountPath: /tmp
        name: empty-dir
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.97
    podIPs:
    - ip: 192.168.194.97
    - ip: fd07:b51a:cc66:a::1df5
    qosClass: Burstable
    startTime: "2025-11-11T17:13:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580
      checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
      checksum/scripts: a9a88a19d473fec18995a3083aa78788c24d98ad662f8eb697f9426b962bbd61
      checksum/secret: f3b30bdb3e109ce14d4714de475ecb9f805ec15fa55fdf70e527ef53c0ddd1e6
    creationTimestamp: "2025-11-10T04:43:56Z"
    generateName: redis-replicas-
    labels:
      app.kubernetes.io/component: replica
      app.kubernetes.io/instance: redis
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 8.2.2
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: redis-replicas-747ddc9f46
      helm.sh/chart: redis-23.1.3
      statefulset.kubernetes.io/pod-name: redis-replicas-0
    name: redis-replicas-0
    namespace: llm-platform
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: redis-replicas
      uid: d0e4f22e-e47d-415c-9fca-5a7f228cc8b1
    resourceVersion: "7643477"
    uid: 229e3370-6ecd-480c-bc25-11988cfc91ea
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: replica
                app.kubernetes.io/instance: redis
                app.kubernetes.io/name: redis
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - args:
      - -ec
      - /opt/bitnami/scripts/start-scripts/start-replica.sh
      command:
      - /bin/bash
      env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: REDIS_REPLICATION_MODE
        value: replica
      - name: REDIS_MASTER_HOST
        value: redis-master-0.redis-headless.llm-platform.svc.cluster.local
      - name: REDIS_MASTER_PORT_NUMBER
        value: "6379"
      - name: ALLOW_EMPTY_PASSWORD
        value: "no"
      - name: REDIS_PASSWORD_FILE
        value: /opt/bitnami/redis/secrets/redis-password
      - name: REDIS_MASTER_PASSWORD_FILE
        value: /opt/bitnami/redis/secrets/redis-password
      - name: REDIS_TLS_ENABLED
        value: "no"
      - name: REDIS_PORT
        value: "6379"
      image: registry-1.docker.io/bitnami/redis:latest
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/bash
          - -ec
          - /health/ping_liveness_local_and_master.sh 5
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 6
      name: redis
      ports:
      - containerPort: 6379
        name: redis
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/bash
          - -ec
          - /health/ping_readiness_local_and_master.sh 1
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        limits:
          cpu: 150m
          ephemeral-storage: 2Gi
          memory: 192Mi
        requests:
          cpu: 100m
          ephemeral-storage: 50Mi
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
        seccompProfile:
          type: RuntimeDefault
      startupProbe:
        failureThreshold: 22
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: redis
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/bitnami/scripts/start-scripts
        name: start-scripts
      - mountPath: /health
        name: health
      - mountPath: /opt/bitnami/redis/secrets/
        name: redis-password
      - mountPath: /data
        name: redis-data
      - mountPath: /opt/bitnami/redis/mounted-etc
        name: config
      - mountPath: /opt/bitnami/redis/etc
        name: empty-dir
        subPath: app-conf-dir
      - mountPath: /tmp
        name: empty-dir
        subPath: tmp-dir
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: redis-replicas-0
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
    serviceAccount: redis-replica
    serviceAccountName: redis-replica
    subdomain: redis-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: redis-data
      persistentVolumeClaim:
        claimName: redis-data-redis-replicas-0
    - configMap:
        defaultMode: 493
        name: redis-scripts
      name: start-scripts
    - configMap:
        defaultMode: 493
        name: redis-health
      name: health
    - name: redis-password
      secret:
        defaultMode: 420
        items:
        - key: redis-password
          path: redis-password
        secretName: redis
    - configMap:
        defaultMode: 420
        name: redis-configuration
      name: config
    - emptyDir: {}
      name: empty-dir
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T17:13:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:48Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:48Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T17:13:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://14313b02be603ec33790f71b7bdb14ad290d704fdee36d239aeaa6156f134a78
      image: registry-1.docker.io/bitnami/redis:latest
      imageID: docker-pullable://registry-1.docker.io/bitnami/redis@sha256:f01f5f06f4c0aadc78bd396109c16be5f46343b39840bdb32a64d9f57a2c7ce7
      lastState:
        terminated:
          containerID: docker://97ea4f3e9b387a47288f2b4d57b58020d5ba05a1673efb727949b31237dc810d
          exitCode: 0
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Completed
          startedAt: "2025-11-18T01:34:12Z"
      name: redis
      ready: true
      restartCount: 14
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:21Z"
      volumeMounts:
      - mountPath: /opt/bitnami/scripts/start-scripts
        name: start-scripts
      - mountPath: /health
        name: health
      - mountPath: /opt/bitnami/redis/secrets/
        name: redis-password
      - mountPath: /data
        name: redis-data
      - mountPath: /opt/bitnami/redis/mounted-etc
        name: config
      - mountPath: /opt/bitnami/redis/etc
        name: empty-dir
      - mountPath: /tmp
        name: empty-dir
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.115
    podIPs:
    - ip: 192.168.194.115
    - ip: fd07:b51a:cc66:a::1e07
    qosClass: Burstable
    startTime: "2025-11-11T17:13:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-11T21:07:41Z"
    generateName: kafka-
    labels:
      app: kafka
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: kafka-c7758466b
      statefulset.kubernetes.io/pod-name: kafka-0
    name: kafka-0
    namespace: ossa-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: kafka
      uid: 9217b3d7-8ee9-47a9-ae61-4194abbcae7d
    resourceVersion: "7647740"
    uid: 3ed70dee-8adf-423b-ae69-786d5d6965fb
  spec:
    containers:
    - env:
      - name: KAFKA_NODE_ID
        value: "1"
      - name: KAFKA_PROCESS_ROLES
        value: broker,controller
      - name: KAFKA_LISTENERS
        value: PLAINTEXT://:9092,CONTROLLER://:9093
      - name: KAFKA_ADVERTISED_LISTENERS
        value: PLAINTEXT://kafka:9092
      - name: KAFKA_CONTROLLER_LISTENER_NAMES
        value: CONTROLLER
      - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
        value: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - name: KAFKA_CONTROLLER_QUORUM_VOTERS
        value: 1@kafka-0.kafka.ossa-agents.svc.cluster.local:9093
      - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
        value: "1"
      - name: KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
        value: "1"
      - name: KAFKA_TRANSACTION_STATE_LOG_MIN_ISR
        value: "1"
      - name: KAFKA_LOG_DIRS
        value: /tmp/kraft-combined-logs
      - name: CLUSTER_ID
        value: MkU3OEVBNTcwNTJENDM2Qk
      image: apache/kafka:latest
      imagePullPolicy: Always
      name: kafka
      ports:
      - containerPort: 9092
        name: plaintext
        protocol: TCP
      - containerPort: 9093
        name: controller
        protocol: TCP
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 256Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d9jvp
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: kafka-0
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    subdomain: kafka
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-d9jvp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:28Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T21:07:41Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T21:07:41Z"
      message: 'containers with unready status: [kafka]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T21:07:41Z"
      message: 'containers with unready status: [kafka]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T21:07:41Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - image: apache/kafka:latest
      imageID: ""
      lastState: {}
      name: kafka
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          message: 'Back-off pulling image "apache/kafka:latest": ErrImagePull: failed
            to register layer: rename /var/lib/docker/image/overlay2/layerdb/tmp/write-set-2765925655
            /var/lib/docker/image/overlay2/layerdb/sha256/2f825e55cfcb761528d072b66eda891d927a590b9f90b100a01e49a13868c644:
            file exists'
          reason: ImagePullBackOff
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d9jvp
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Pending
    podIP: 192.168.194.102
    podIPs:
    - ip: 192.168.194.102
    - ip: fd07:b51a:cc66:a::1dfa
    qosClass: Burstable
    startTime: "2025-11-11T21:07:41Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: ef488086761255c99c5bc79ce6c045c5952aaff1c871419df514bc9c4fdf2cca
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, memory request for
        container loki; cpu, memory limit for container loki'
      prometheus.io/port: http-metrics
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-11-11T02:56:44Z"
    generateName: loki-
    labels:
      app: loki
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: loki-774677948b
      name: loki
      release: loki
      statefulset.kubernetes.io/pod-name: loki-0
    name: loki-0
    namespace: ossa-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: loki
      uid: 9cc7ae8e-58e3-4b69-ac75-6a537ca5c13e
    resourceVersion: "7643744"
    uid: 83fab2b8-bad6-4f6e-94d9-82e83aa6cc98
  spec:
    affinity: {}
    containers:
    - args:
      - -config.file=/etc/loki/loki.yaml
      image: grafana/loki:2.6.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: loki
      ports:
      - containerPort: 3100
        name: http-metrics
        protocol: TCP
      - containerPort: 9095
        name: grpc
        protocol: TCP
      - containerPort: 7946
        name: memberlist-port
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: 100m
          memory: 256Mi
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /etc/loki
        name: config
      - mountPath: /data
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-q65sl
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: loki-0
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki
    serviceAccountName: loki
    subdomain: loki-headless
    terminationGracePeriodSeconds: 4800
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp
    - name: config
      secret:
        defaultMode: 420
        secretName: loki
    - emptyDir: {}
      name: storage
    - name: kube-api-access-q65sl
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:20Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T17:13:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:38:38Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:38:38Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T17:13:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://552b92dca2d7fe51db3982707892cf247c6a6ecabddb83d7ae15bf0bce73049a
      image: grafana/loki:2.6.1
      imageID: docker-pullable://grafana/loki@sha256:1ee60f980950b00e505bd564b40f720132a0653b110e993043bb5940673d060a
      lastState:
        terminated:
          containerID: docker://1bcf3dc526b66ef50e38583220e76aa67cd812a54e55811ed18f4e00947912b1
          exitCode: 0
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Completed
          startedAt: "2025-11-18T01:34:12Z"
      name: loki
      ready: true
      restartCount: 8
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:20Z"
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /etc/loki
        name: config
      - mountPath: /data
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-q65sl
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.103
    podIPs:
    - ip: 192.168.194.103
    - ip: fd07:b51a:cc66:a::1dfb
    qosClass: Burstable
    startTime: "2025-11-11T17:13:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: ab1cb3f63f759e3db8978c1acf24413428e072965c6d0eb46186c5a4e9c7de87
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, memory request for
        container promtail; cpu, memory limit for container promtail'
    creationTimestamp: "2025-11-11T21:07:43Z"
    generateName: promtail-
    labels:
      app.kubernetes.io/instance: promtail
      app.kubernetes.io/name: promtail
      controller-revision-hash: 7f58f74d8d
      pod-template-generation: "1"
    name: promtail-hkn2m
    namespace: ossa-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: promtail
      uid: 846ba7bd-2a5a-499b-8f71-f582b769b0ca
    resourceVersion: "7643138"
    uid: b7d74c7e-8d82-48b3-88f2-4bc5de010bb2
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - orbstack
    automountServiceAccountToken: true
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:3.5.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: 100m
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zqnbb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: promtail
    serviceAccountName: promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-zqnbb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T21:07:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T21:07:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://f242d645e87983139663be41acdc1d741e92889b613907fcac107776a256f765
      image: grafana/promtail:3.5.1
      imageID: docker-pullable://grafana/promtail@sha256:65bfae480b572854180c78f7dc567a4ad2ba548b0c410e696baa1e0fa6381299
      lastState:
        terminated:
          containerID: docker://ac1e8fd0a275b66db660be5579a538f0cd6798b10ef611e5d7ba3d541c4099a2
          exitCode: 0
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Completed
          startedAt: "2025-11-18T01:34:12Z"
      name: promtail
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:20Z"
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zqnbb
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.101
    podIPs:
    - ip: 192.168.194.101
    - ip: fd07:b51a:cc66:a::1df9
    qosClass: Burstable
    startTime: "2025-11-11T21:07:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 641ccac989f9a653a71e14216fd5ac5cef35d2867853bade1cd7aed136ba1eb1
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, memory request for
        container tempo; cpu, memory limit for container tempo'
    creationTimestamp: "2025-11-11T02:59:09Z"
    generateName: tempo-
    labels:
      app.kubernetes.io/instance: tempo
      app.kubernetes.io/name: tempo
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: tempo-7bc544f9cf
      statefulset.kubernetes.io/pod-name: tempo-0
    name: tempo-0
    namespace: ossa-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: tempo
      uid: d1f893ff-06a4-47dd-8de6-c95e1fbabd3d
    resourceVersion: "7643622"
    uid: e4560fef-37c0-4985-b25c-eefb65f382a0
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - -config.file=/conf/tempo.yaml
      - -mem-ballast-size-mbs=1024
      image: grafana/tempo:2.9.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 3200
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: tempo
      ports:
      - containerPort: 3200
        name: prom-metrics
        protocol: TCP
      - containerPort: 6831
        name: jaeger-thrift-c
        protocol: UDP
      - containerPort: 6832
        name: jaeger-thrift-b
        protocol: UDP
      - containerPort: 14268
        name: jaeger-thrift-h
        protocol: TCP
      - containerPort: 14250
        name: jaeger-grpc
        protocol: TCP
      - containerPort: 9411
        name: zipkin
        protocol: TCP
      - containerPort: 55680
        name: otlp-legacy
        protocol: TCP
      - containerPort: 4317
        name: otlp-grpc
        protocol: TCP
      - containerPort: 55681
        name: otlp-httplegacy
        protocol: TCP
      - containerPort: 4318
        name: otlp-http
        protocol: TCP
      - containerPort: 55678
        name: opencensus
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 3200
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: 100m
          memory: 256Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /conf
        name: tempo-conf
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l9fhn
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: tempo-0
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: tempo
    serviceAccountName: tempo
    subdomain: tempo-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: tempo
      name: tempo-conf
    - name: kube-api-access-l9fhn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:20Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T17:45:33Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:38:03Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:38:03Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T17:45:33Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://43638ebede37f7c9cda499073838e80961e007c82aff321d836bb0e0a8863e8d
      image: grafana/tempo:2.9.0
      imageID: docker-pullable://grafana/tempo@sha256:65a5789759435f1ef696f1953258b9bbdb18eb571d5ce711ff812d2e128288a4
      lastState:
        terminated:
          containerID: docker://7a7e2153a6836bb6a20ab7f36ec6d2597ad08211ecbaaffacd355b4f5e4164cf
          exitCode: 0
          finishedAt: "2025-11-18T01:36:42Z"
          reason: Completed
          startedAt: "2025-11-18T01:34:13Z"
      name: tempo
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:20Z"
      volumeMounts:
      - mountPath: /conf
        name: tempo-conf
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l9fhn
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.107
    podIPs:
    - ip: 192.168.194.107
    - ip: fd07:b51a:cc66:a::1dff
    qosClass: Burstable
    startTime: "2025-11-11T17:45:33Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-10T04:43:55Z"
    generateName: agent-gateway-64d665d798-
    labels:
      app: agent-gateway
      pod-template-hash: 64d665d798
      version: 0.2.0
    name: agent-gateway-64d665d798-vtsjm
    namespace: ossa-prod
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: agent-gateway-64d665d798
      uid: e9a989e6-fd08-4379-b9db-24577d1e03b7
    resourceVersion: "7643025"
    uid: e2c8bce8-b452-49b0-9dbb-93fddb1980ba
  spec:
    containers:
    - command:
      - node
      - -e
      - require('http').createServer((req,res)=>res.end(JSON.stringify({status:'healthy',service:'agent-gateway',agents:129}))).listen(3000)
      env:
      - name: PHOENIX_COLLECTOR_ENDPOINT
        value: http://phoenix.observability.svc.cluster.local:4318/v1/traces
      - name: NODE_ENV
        value: development
      image: node:20-alpine
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 3000
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: agent-gateway
      ports:
      - containerPort: 3000
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 3000
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qwzd7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-qwzd7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T17:14:13Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:37:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-11T17:14:13Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://63bfab00fcf3f733645be3c27dcfadf6749fecafb6d8af15d006ca1b8354ddb5
      image: node:20-alpine
      imageID: docker-pullable://node@sha256:6178e78b972f79c335df281f4b7674a2d85071aae2af020ffa39f0a770265435
      lastState:
        terminated:
          containerID: docker://25dd76f5f81cc6292d0bfec3f68b94ce220e49fc78577da41651cf7b4ddfa433
          exitCode: 137
          finishedAt: "2025-11-18T01:36:44Z"
          reason: Error
          startedAt: "2025-11-18T01:34:11Z"
      name: agent-gateway
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2025-11-18T01:37:21Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qwzd7
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.111
    podIPs:
    - ip: 192.168.194.111
    - ip: fd07:b51a:cc66:a::1e03
    qosClass: Burstable
    startTime: "2025-11-11T17:14:13Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-18T01:34:05Z"
    generateName: pod-cleanup-29390490-
    labels:
      app: pod-cleanup
      batch.kubernetes.io/controller-uid: 24f0ae88-28c0-4700-af79-57fe58317190
      batch.kubernetes.io/job-name: pod-cleanup-29390490
      controller-uid: 24f0ae88-28c0-4700-af79-57fe58317190
      job-name: pod-cleanup-29390490
    name: pod-cleanup-29390490-vlm9p
    namespace: pod-cleanup
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: pod-cleanup-29390490
      uid: 24f0ae88-28c0-4700-af79-57fe58317190
    resourceVersion: "7642604"
    uid: a077d0cd-2b70-450e-8653-4f31965e93be
  spec:
    containers:
    - command:
      - /bin/sh
      - -c
      - |
        echo "Starting pod cleanup..."

        # Clean up Succeeded pods older than 1 hour
        SUCCEEDED=$(kubectl get pods --all-namespaces --field-selector=status.phase==Succeeded -o json | jq -r '.items[] | select((.status.startTime // .metadata.creationTimestamp) | fromdateiso8601 < (now - 3600)) | "\(.metadata.namespace) \(.metadata.name)"')

        if [ -n "$SUCCEEDED" ]; then
          echo "$SUCCEEDED" | while read namespace name; do
            echo "Deleting succeeded pod: $namespace/$name"
            kubectl delete pod "$name" -n "$namespace" --grace-period=0 || true
          done
        fi

        # Clean up Failed pods older than 24 hours
        FAILED=$(kubectl get pods --all-namespaces --field-selector=status.phase==Failed -o json | jq -r '.items[] | select((.status.startTime // .metadata.creationTimestamp) | fromdateiso8601 < (now - 86400)) | "\(.metadata.namespace) \(.metadata.name)"')

        if [ -n "$FAILED" ]; then
          echo "$FAILED" | while read namespace name; do
            echo "Deleting failed pod: $namespace/$name"
            kubectl delete pod "$name" -n "$namespace" --grace-period=0 || true
          done
        fi

        # Clean up Evicted pods immediately
        EVICTED=$(kubectl get pods --all-namespaces --field-selector=status.phase==Failed -o json | jq -r '.items[] | select(.status.reason == "Evicted") | "\(.metadata.namespace) \(.metadata.name)"')

        if [ -n "$EVICTED" ]; then
          echo "$EVICTED" | while read namespace name; do
            echo "Deleting evicted pod: $namespace/$name"
            kubectl delete pod "$name" -n "$namespace" --grace-period=0 || true
          done
        fi

        echo "Pod cleanup completed"
      image: bitnami/kubectl:latest
      imagePullPolicy: Always
      name: cleanup
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 64Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-h7dsd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: pod-cleanup
    serviceAccountName: pod-cleanup
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-h7dsd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:34:22Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:34:05Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:34:21Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:34:21Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T01:34:05Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://0b67ce2450e90bf90f8615a289d5341010c054f6c33dd8a3ab71a954972f9ba2
      image: bitnami/kubectl:latest
      imageID: docker-pullable://bitnami/kubectl@sha256:5df7de31ece99cc40aa34d627a61b1bb4d17d1a3341e2f63a63b46b1136080cd
      lastState: {}
      name: cleanup
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: docker://0b67ce2450e90bf90f8615a289d5341010c054f6c33dd8a3ab71a954972f9ba2
          exitCode: 0
          finishedAt: "2025-11-18T01:34:20Z"
          reason: Completed
          startedAt: "2025-11-18T01:34:14Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-h7dsd
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Succeeded
    qosClass: Burstable
    startTime: "2025-11-18T01:34:05Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-18T02:00:00Z"
    generateName: pod-cleanup-29390520-
    labels:
      app: pod-cleanup
      batch.kubernetes.io/controller-uid: 591ceaf6-43c9-4180-bcdf-4eee763d0c22
      batch.kubernetes.io/job-name: pod-cleanup-29390520
      controller-uid: 591ceaf6-43c9-4180-bcdf-4eee763d0c22
      job-name: pod-cleanup-29390520
    name: pod-cleanup-29390520-shwxc
    namespace: pod-cleanup
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: pod-cleanup-29390520
      uid: 591ceaf6-43c9-4180-bcdf-4eee763d0c22
    resourceVersion: "7645959"
    uid: b81e62f6-dbc7-4ba6-976a-171ebe524ce6
  spec:
    containers:
    - command:
      - /bin/sh
      - -c
      - |
        echo "Starting pod cleanup..."

        # Clean up Succeeded pods older than 1 hour
        SUCCEEDED=$(kubectl get pods --all-namespaces --field-selector=status.phase==Succeeded -o json | jq -r '.items[] | select((.status.startTime // .metadata.creationTimestamp) | fromdateiso8601 < (now - 3600)) | "\(.metadata.namespace) \(.metadata.name)"')

        if [ -n "$SUCCEEDED" ]; then
          echo "$SUCCEEDED" | while read namespace name; do
            echo "Deleting succeeded pod: $namespace/$name"
            kubectl delete pod "$name" -n "$namespace" --grace-period=0 || true
          done
        fi

        # Clean up Failed pods older than 24 hours
        FAILED=$(kubectl get pods --all-namespaces --field-selector=status.phase==Failed -o json | jq -r '.items[] | select((.status.startTime // .metadata.creationTimestamp) | fromdateiso8601 < (now - 86400)) | "\(.metadata.namespace) \(.metadata.name)"')

        if [ -n "$FAILED" ]; then
          echo "$FAILED" | while read namespace name; do
            echo "Deleting failed pod: $namespace/$name"
            kubectl delete pod "$name" -n "$namespace" --grace-period=0 || true
          done
        fi

        # Clean up Evicted pods immediately
        EVICTED=$(kubectl get pods --all-namespaces --field-selector=status.phase==Failed -o json | jq -r '.items[] | select(.status.reason == "Evicted") | "\(.metadata.namespace) \(.metadata.name)"')

        if [ -n "$EVICTED" ]; then
          echo "$EVICTED" | while read namespace name; do
            echo "Deleting evicted pod: $namespace/$name"
            kubectl delete pod "$name" -n "$namespace" --grace-period=0 || true
          done
        fi

        echo "Pod cleanup completed"
      image: bitnami/kubectl:latest
      imagePullPolicy: Always
      name: cleanup
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 64Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gv2kq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: pod-cleanup
    serviceAccountName: pod-cleanup
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-gv2kq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T02:00:09Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T02:00:00Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T02:00:08Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T02:00:08Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T02:00:00Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://b5ccfc7869e4a21b5c7c6597d1d1ee7c12844aba0d351a3213760ba8eb9ec4f4
      image: bitnami/kubectl:latest
      imageID: docker-pullable://bitnami/kubectl@sha256:5df7de31ece99cc40aa34d627a61b1bb4d17d1a3341e2f63a63b46b1136080cd
      lastState: {}
      name: cleanup
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: docker://b5ccfc7869e4a21b5c7c6597d1d1ee7c12844aba0d351a3213760ba8eb9ec4f4
          exitCode: 0
          finishedAt: "2025-11-18T02:00:07Z"
          reason: Completed
          startedAt: "2025-11-18T02:00:03Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gv2kq
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.139.2
    hostIPs:
    - ip: 192.168.139.2
    - ip: fd07:b51a:cc66::2
    phase: Succeeded
    podIP: 192.168.194.119
    podIPs:
    - ip: 192.168.194.119
    - ip: fd07:b51a:cc66:a::1e0b
    qosClass: Burstable
    startTime: "2025-11-18T02:00:00Z"
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"frontend","namespace":"agent-chat"},"spec":{"ports":[{"port":3000,"protocol":"TCP","targetPort":80}],"selector":{"app":"frontend"},"type":"LoadBalancer"}}
    creationTimestamp: "2025-10-04T15:48:21Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    labels:
      dev.orbstack.domains: agent-chat.orb.local
    name: frontend
    namespace: agent-chat
    resourceVersion: "7642832"
    uid: 42f5eca3-abd7-4005-be5d-2ad5008a008d
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 192.168.194.164
    clusterIPs:
    - 192.168.194.164
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      nodePort: 30235
      port: 3000
      protocol: TCP
      targetPort: 80
    selector:
      app: frontend
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 192.168.139.2
        ipMode: VIP
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"librechat","namespace":"agent-chat"},"spec":{"ports":[{"port":3080,"targetPort":3080}],"selector":{"app":"librechat"},"type":"LoadBalancer"}}
    creationTimestamp: "2025-10-04T15:48:21Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    name: librechat
    namespace: agent-chat
    resourceVersion: "7491972"
    uid: 251848e0-1281-45b1-b4bd-9c36f0fe63a2
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 192.168.194.217
    clusterIPs:
    - 192.168.194.217
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - nodePort: 32432
      port: 3080
      protocol: TCP
      targetPort: 3080
    selector:
      app: librechat
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 192.168.139.2
        ipMode: VIP
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"mongo","namespace":"agent-chat"},"spec":{"ports":[{"port":27017,"targetPort":27017}],"selector":{"app":"mongo"}}}
    creationTimestamp: "2025-10-04T16:01:43Z"
    name: mongo
    namespace: agent-chat
    resourceVersion: "4628073"
    uid: c70170ff-3a8c-4852-8a55-e5a68e3c8779
  spec:
    clusterIP: 192.168.194.235
    clusterIPs:
    - 192.168.194.235
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 27017
      protocol: TCP
      targetPort: 27017
    selector:
      app: mongo
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"postgres","namespace":"agent-chat"},"spec":{"ports":[{"port":5432,"targetPort":5432}],"selector":{"app":"postgres"}}}
    creationTimestamp: "2025-10-04T15:48:20Z"
    name: postgres
    namespace: agent-chat
    resourceVersion: "4627384"
    uid: 0dd49aed-8579-4775-a874-a22d59d6e7ea
  spec:
    clusterIP: 192.168.194.154
    clusterIPs:
    - 192.168.194.154
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 5432
      protocol: TCP
      targetPort: 5432
    selector:
      app: postgres
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"redis","namespace":"agent-chat"},"spec":{"ports":[{"port":6379,"targetPort":6379}],"selector":{"app":"redis"}}}
    creationTimestamp: "2025-10-04T15:48:21Z"
    name: redis
    namespace: agent-chat
    resourceVersion: "4627402"
    uid: 83f5e95d-b35d-4575-b930-5bac4442ee4d
  spec:
    clusterIP: 192.168.194.189
    clusterIPs:
    - 192.168.194.189
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 6379
      protocol: TCP
      targetPort: 6379
    selector:
      app: redis
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"gateway-service","namespace":"agent-studio"},"spec":{"ports":[{"port":80,"targetPort":80}],"selector":{"app":"gateway"}}}
    creationTimestamp: "2025-10-22T05:43:38Z"
    labels:
      dev.orbstack.domains: agent-studio-gateway.orb.local
    name: gateway-service
    namespace: agent-studio
    resourceVersion: "5738793"
    uid: 92339a5a-3857-45b8-9d87-1c1552146376
  spec:
    clusterIP: 192.168.194.248
    clusterIPs:
    - 192.168.194.248
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 80
      protocol: TCP
      targetPort: 80
    selector:
      app: gateway
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"monitoring-service","namespace":"agent-studio"},"spec":{"ports":[{"port":80,"targetPort":80}],"selector":{"app":"monitoring"}}}
    creationTimestamp: "2025-10-22T05:43:38Z"
    labels:
      dev.orbstack.domains: agent-studio-monitoring.orb.local
    name: monitoring-service
    namespace: agent-studio
    resourceVersion: "5738799"
    uid: a747a328-05fe-416f-ac49-d7ec170e9dd2
  spec:
    clusterIP: 192.168.194.239
    clusterIPs:
    - 192.168.194.239
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 80
      protocol: TCP
      targetPort: 80
    selector:
      app: monitoring
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"orchestration-service","namespace":"agent-studio"},"spec":{"ports":[{"port":80,"targetPort":80}],"selector":{"app":"orchestration"}}}
    creationTimestamp: "2025-10-22T05:43:38Z"
    labels:
      dev.orbstack.domains: agent-studio-orchestration.orb.local
    name: orchestration-service
    namespace: agent-studio
    resourceVersion: "5738796"
    uid: 34a0ab8c-73c1-46ff-aff7-e8bf0360ba25
  spec:
    clusterIP: 192.168.194.180
    clusterIPs:
    - 192.168.194.180
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 80
      protocol: TCP
      targetPort: 80
    selector:
      app: orchestration
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"web-service","namespace":"agent-studio"},"spec":{"ports":[{"port":80,"targetPort":80}],"selector":{"app":"web"}}}
    creationTimestamp: "2025-10-22T05:43:38Z"
    labels:
      dev.orbstack.domains: agent-studio.orb.local
    name: web-service
    namespace: agent-studio
    resourceVersion: "5738790"
    uid: 5086231f-82ee-4a9e-a398-a80f98a7bf84
  spec:
    clusterIP: 192.168.194.243
    clusterIPs:
    - 192.168.194.243
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 80
      protocol: TCP
      targetPort: 80
    selector:
      app: web
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{"prometheus.io/path":"/metrics","prometheus.io/port":"8000","prometheus.io/scrape":"true"},"labels":{"app":"aiflow-social-agent"},"name":"aiflow-social-agent","namespace":"agents-staging"},"spec":{"ports":[{"name":"http","port":8000,"protocol":"TCP","targetPort":"http"},{"name":"metrics","port":9090,"protocol":"TCP","targetPort":"metrics"}],"selector":{"app":"aiflow-social-agent"},"sessionAffinity":"None","type":"ClusterIP"}}
      prometheus.io/path: /metrics
      prometheus.io/port: "8000"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-10-23T11:44:46Z"
    labels:
      app: aiflow-social-agent
    name: aiflow-social-agent
    namespace: agents-staging
    resourceVersion: "5437596"
    uid: 8d0c9312-5344-4f1f-9276-5379c9bc5f69
  spec:
    clusterIP: 192.168.194.192
    clusterIPs:
    - 192.168.194.192
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8000
      protocol: TCP
      targetPort: http
    - name: metrics
      port: 9090
      protocol: TCP
      targetPort: metrics
    selector:
      app: aiflow-social-agent
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"mlflow"},"name":"mlflow","namespace":"ai-ml"},"spec":{"ports":[{"name":"http","port":5000,"targetPort":5000}],"selector":{"app":"mlflow"},"type":"LoadBalancer"}}
    creationTimestamp: "2025-10-15T14:26:35Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    labels:
      app: mlflow
    name: mlflow
    namespace: ai-ml
    resourceVersion: "7482285"
    uid: 01455b95-ae28-4c14-b0d7-3c458c347593
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 192.168.194.228
    clusterIPs:
    - 192.168.194.228
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      nodePort: 32072
      port: 5000
      protocol: TCP
      targetPort: 5000
    selector:
      app: mlflow
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 192.168.139.2
        ipMode: VIP
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"apidog-runner"},"name":"apidog-runner","namespace":"apidog"},"spec":{"ports":[{"name":"http","nodePort":30524,"port":4524,"protocol":"TCP","targetPort":4524}],"selector":{"app":"apidog-runner"},"type":"NodePort"}}
    creationTimestamp: "2025-10-21T01:25:31Z"
    labels:
      app: apidog-runner
    name: apidog-runner
    namespace: apidog
    resourceVersion: "5283924"
    uid: ace63a2e-7f6a-4de2-8687-82f0b8672926
  spec:
    clusterIP: 192.168.194.252
    clusterIPs:
    - 192.168.194.252
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      nodePort: 30524
      port: 4524
      protocol: TCP
      targetPort: 4524
    selector:
      app: apidog-runner
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{"tailscale.com/expose":"true"},"labels":{"app":"agent-router","version":"v1"},"name":"agent-router","namespace":"default"},"spec":{"ports":[{"name":"http","port":8080,"protocol":"TCP","targetPort":8080}],"selector":{"app":"agent-router"},"sessionAffinity":"None","type":"ClusterIP"}}
      tailscale.com/expose: "true"
    creationTimestamp: "2025-11-03T19:18:34Z"
    labels:
      app: agent-router
      version: v1
    name: agent-router
    namespace: default
    resourceVersion: "6156086"
    uid: 74352319-d3ef-4784-bc87-9c30ff0c8bec
  spec:
    clusterIP: 192.168.194.210
    clusterIPs:
    - 192.168.194.210
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app: agent-router
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-10-22T05:43:21Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "5376988"
    uid: fa4d73a6-0731-4d08-9af5-189314532044
  spec:
    clusterIP: 192.168.194.129
    clusterIPs:
    - 192.168.194.129
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 26443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"ossa-website"},"name":"ossa-website","namespace":"default"},"spec":{"ports":[{"name":"http","nodePort":30080,"port":80,"protocol":"TCP","targetPort":3000}],"selector":{"app":"ossa-website"},"sessionAffinity":"None","type":"NodePort"}}
    creationTimestamp: "2025-11-18T01:04:03Z"
    labels:
      app: ossa-website
    name: ossa-website
    namespace: default
    resourceVersion: "7640016"
    uid: 4a030755-aff7-4563-b9ae-b2b477e8ada9
  spec:
    clusterIP: 192.168.194.250
    clusterIPs:
    - 192.168.194.250
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      nodePort: 30080
      port: 80
      protocol: TCP
      targetPort: 3000
    selector:
      app: ossa-website
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{"service.beta.kubernetes.io/aws-load-balancer-type":"nlb"},"labels":{"app":"design-system-api","environment":"production","managed-by":"kustomize","version":"1.0.0"},"name":"design-system-api-service","namespace":"design-system-api"},"spec":{"ports":[{"name":"http","port":80,"protocol":"TCP","targetPort":3001}],"selector":{"app":"design-system-api","environment":"production","managed-by":"kustomize","version":"1.0.0"},"type":"ClusterIP"}}
      service.beta.kubernetes.io/aws-load-balancer-type: nlb
    creationTimestamp: "2025-10-07T02:23:21Z"
    labels:
      app: design-system-api
      dev.orbstack.domains: design-system-api.orb.local
      environment: production
      managed-by: kustomize
      version: 1.0.0
    name: design-system-api-service
    namespace: design-system-api
    resourceVersion: "5738802"
    uid: fcd44645-d41b-4ee0-b786-371aca7d4cc2
  spec:
    clusterIP: 192.168.194.151
    clusterIPs:
    - 192.168.194.151
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 3001
    selector:
      app: design-system-api
      environment: production
      managed-by: kustomize
      version: 1.0.0
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2025-09-07T03:15:30Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.13.2
      helm.sh/chart: ingress-nginx-4.13.2
    name: ingress-nginx-controller
    namespace: ingress-nginx
    resourceVersion: "7642923"
    uid: 2227b54c-abf2-42b1-b239-761942266595
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 192.168.194.249
    clusterIPs:
    - 192.168.194.249
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: http
      name: http
      nodePort: 31928
      port: 80
      protocol: TCP
      targetPort: http
    - appProtocol: https
      name: https
      nodePort: 30889
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 192.168.139.2
        ipMode: VIP
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2025-09-07T03:15:30Z"
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.13.2
      helm.sh/chart: ingress-nginx-4.13.2
    name: ingress-nginx-controller-admission
    namespace: ingress-nginx
    resourceVersion: "3861506"
    uid: 1f06f7e9-2d1d-47b5-a20b-d1098ce9b141
  spec:
    clusterIP: 192.168.194.186
    clusterIPs:
    - 192.168.194.186
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: https
      name: https-webhook
      port: 443
      protocol: TCP
      targetPort: webhook
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      deployment-wave: "3"
      generated-by: agent-swarm
      issues-closed: "79"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{"deployment-wave":"3","generated-by":"agent-swarm","issues-closed":"79"},"labels":{"managed-by":"agent-buildkit","ossa-version":"v0.2.3","platform":"kagent"},"name":"khook","namespace":"khook-system"},"spec":{"ports":[{"port":443,"targetPort":8443}],"selector":{"app":"khook","managed-by":"agent-buildkit","ossa-version":"v0.2.3","platform":"kagent"}}}
    creationTimestamp: "2025-11-18T01:36:03Z"
    labels:
      managed-by: agent-buildkit
      ossa-version: v0.2.3
      platform: kagent
    name: khook
    namespace: khook-system
    resourceVersion: "7642408"
    uid: 88ff821e-cede-4b33-8764-243b6097ae43
  spec:
    clusterIP: 192.168.194.226
    clusterIPs:
    - 192.168.194.226
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 443
      protocol: TCP
      targetPort: 8443
    selector:
      app: khook
      managed-by: agent-buildkit
      ossa-version: v0.2.3
      platform: kagent
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/4xST4vbPhD9Kj/mLPtnJ47tCHooGwpLoQTS9lL2MJYnjWpbEtLEJQR/96LEy6bdpu3JFu8P783MGdDpz+SDtgYkjDkI6LRpQcKO/KgVgYCBGFtkBHkGNMYysrYmxKdtvpHiQJx6bVOFzD2l2v6vowOIu7j9bsgnX8cOJHTLcIOMufjvvTbtm7dta81fLQwOBBKsbxJlPbUm/JMkOFRR1x0bSsIpMA0gwHk7EB/oGCLbWc8gYZ2vlq+woDy6aMD+SDAJ6LGh/jKSrg4JOvdsfk0Uf70hpota9cfA5JMwj3i2+ZU2d3uwnjYfdn/odcBwiEmzolxQsVpXTatQIVbZsq6LbF/VZbHKUFVZ1exxEfPO3jcR741lEhAcqVhtzv24BQn5epHmZZ3m6yLNlzWIFzSA/PIbfN9mlWxWOUqlylKirLMskxLhSYB273DQvaaL9nE7FiDip7wBT1vba3UCCVtPe/KbI/Y7RtXF7VjPUXp+LnbtdF3hanlZH1tle5DwabOFSdwyE1buHvvjw0/sgdhr9eIdr+M1/0lAoJ4UW3/nJKZp+hEAAP//AnbWPnwDAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: orb-coredns
      objectset.rio.cattle.io/owner-namespace: kube-system
      prometheus.io/port: "9153"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-12-22T01:30:13Z"
    labels:
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: CoreDNS
      objectset.rio.cattle.io/hash: 90462e4597bdcacaa7038840f786450ac707bfa2
    name: kube-dns
    namespace: kube-system
    resourceVersion: "280"
    uid: c3f4f627-84de-4182-98d3-0f6a5946f014
  spec:
    clusterIP: 192.168.194.138
    clusterIPs:
    - 192.168.194.138
    - fd07:b51a:cc66:a:8000::a
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    - IPv6
    ipFamilyPolicy: PreferDualStack
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    - name: metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"k8s-app":"metrics-server"},"name":"metrics-server","namespace":"kube-system"},"spec":{"ports":[{"appProtocol":"https","name":"https","port":443,"protocol":"TCP","targetPort":"https"}],"selector":{"k8s-app":"metrics-server"}}}
    creationTimestamp: "2025-09-07T02:48:49Z"
    labels:
      k8s-app: metrics-server
    name: metrics-server
    namespace: kube-system
    resourceVersion: "3860165"
    uid: 655b52de-22c0-46e0-9e8d-574a3b12739d
  spec:
    clusterIP: 192.168.194.244
    clusterIPs:
    - 192.168.194.244
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: https
      name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      k8s-app: metrics-server
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"clickhouse","component":"langfuse"},"name":"langfuse-clickhouse","namespace":"langfuse"},"spec":{"ports":[{"name":"http","port":8123,"protocol":"TCP","targetPort":8123},{"name":"native","port":9000,"protocol":"TCP","targetPort":9000}],"selector":{"app":"clickhouse","component":"langfuse"},"type":"ClusterIP"}}
    creationTimestamp: "2025-11-08T03:17:43Z"
    labels:
      app: clickhouse
      component: langfuse
    name: langfuse-clickhouse
    namespace: langfuse
    resourceVersion: "6420101"
    uid: 37b4f7c5-4499-41f0-bcd9-b2294bdb8c3c
  spec:
    clusterIP: 192.168.194.152
    clusterIPs:
    - 192.168.194.152
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8123
      protocol: TCP
      targetPort: 8123
    - name: native
      port: 9000
      protocol: TCP
      targetPort: 9000
    selector:
      app: clickhouse
      component: langfuse
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"langfuse","component":"web"},"name":"langfuse-web","namespace":"langfuse"},"spec":{"ports":[{"name":"http","port":3000,"protocol":"TCP","targetPort":3000}],"selector":{"app":"langfuse","component":"web"},"type":"ClusterIP"}}
    creationTimestamp: "2025-11-05T03:16:17Z"
    labels:
      app: langfuse
      component: web
    name: langfuse-web
    namespace: langfuse
    resourceVersion: "6235008"
    uid: 0657853c-273d-4005-ae0e-adcef226591c
  spec:
    clusterIP: 192.168.194.148
    clusterIPs:
    - 192.168.194.148
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 3000
      protocol: TCP
      targetPort: 3000
    selector:
      app: langfuse
      component: web
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"agent-brain","namespace":"llm-platform"},"spec":{"ports":[{"name":"http","port":3002,"targetPort":3002},{"name":"metrics","port":9090,"targetPort":9090}],"selector":{"app":"agent-brain"},"type":"ClusterIP"}}
    creationTimestamp: "2025-10-21T20:24:31Z"
    name: agent-brain
    namespace: llm-platform
    resourceVersion: "5360736"
    uid: 7577d12d-b60f-46a2-a8b6-40c95afca394
  spec:
    clusterIP: 192.168.194.251
    clusterIPs:
    - 192.168.194.251
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 3002
      protocol: TCP
      targetPort: 3002
    - name: metrics
      port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app: agent-brain
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"compliance-engine","namespace":"llm-platform"},"spec":{"ports":[{"name":"http","port":3100,"targetPort":3100}],"selector":{"app":"compliance-engine"},"type":"ClusterIP"}}
    creationTimestamp: "2025-10-21T20:24:25Z"
    name: compliance-engine
    namespace: llm-platform
    resourceVersion: "5341690"
    uid: f7cefb34-5dea-41c2-8232-2f6d701f4f1c
  spec:
    clusterIP: 192.168.194.181
    clusterIPs:
    - 192.168.194.181
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 3100
      protocol: TCP
      targetPort: 3100
    selector:
      app: compliance-engine
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"postgresql","namespace":"llm-platform"},"spec":{"ports":[{"port":5432,"targetPort":5432}],"selector":{"app":"postgresql"},"type":"ClusterIP"}}
      meta.helm.sh/release-name: postgresql
      meta.helm.sh/release-namespace: llm-platform
    creationTimestamp: "2025-08-21T18:01:42Z"
    labels:
      app.kubernetes.io/instance: postgresql
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: latest
      dev.orbstack.domains: postgres.orb.local
      helm.sh/chart: postgresql-0.1.0
    name: postgresql
    namespace: llm-platform
    resourceVersion: "3871688"
    uid: 88d74453-e785-4a19-a688-600671276cac
  spec:
    clusterIP: 192.168.194.196
    clusterIPs:
    - 192.168.194.196
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 5432
      protocol: TCP
      targetPort: 5432
    selector:
      app: postgresql
      app.kubernetes.io/instance: postgresql
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"qdrant","namespace":"llm-platform"},"spec":{"externalName":"192.168.139.2","ports":[{"port":6333,"protocol":"TCP","targetPort":6333}],"type":"ExternalName"}}
    creationTimestamp: "2025-10-09T23:44:46Z"
    name: qdrant
    namespace: llm-platform
    resourceVersion: "4915927"
    uid: ee300937-0ea6-4aa1-a55e-990863842982
  spec:
    externalName: 192.168.139.2
    ports:
    - port: 6333
      protocol: TCP
      targetPort: 6333
    sessionAffinity: None
    type: ExternalName
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: redis
      meta.helm.sh/release-namespace: llm-platform
    creationTimestamp: "2025-10-16T03:33:33Z"
    labels:
      app.kubernetes.io/instance: redis
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 8.2.2
      helm.sh/chart: redis-23.1.3
    name: redis-headless
    namespace: llm-platform
    resourceVersion: "5133215"
    uid: 9e449c72-8767-4a2f-8202-f24400989b97
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-redis
      port: 6379
      protocol: TCP
      targetPort: redis
    selector:
      app.kubernetes.io/instance: redis
      app.kubernetes.io/name: redis
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: redis
      meta.helm.sh/release-namespace: llm-platform
    creationTimestamp: "2025-10-16T03:33:33Z"
    labels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: redis
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 8.2.2
      helm.sh/chart: redis-23.1.3
    name: redis-master
    namespace: llm-platform
    resourceVersion: "5133222"
    uid: def272c9-f5b1-453a-8cbb-909919e4b859
  spec:
    clusterIP: 192.168.194.145
    clusterIPs:
    - 192.168.194.145
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-redis
      port: 6379
      protocol: TCP
      targetPort: redis
    selector:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: redis
      app.kubernetes.io/name: redis
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: redis
      meta.helm.sh/release-namespace: llm-platform
    creationTimestamp: "2025-10-16T03:33:33Z"
    labels:
      app.kubernetes.io/component: replica
      app.kubernetes.io/instance: redis
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 8.2.2
      helm.sh/chart: redis-23.1.3
    name: redis-replicas
    namespace: llm-platform
    resourceVersion: "5133219"
    uid: 172e4199-ce7e-46ed-8c12-21bb3248d43f
  spec:
    clusterIP: 192.168.194.160
    clusterIPs:
    - 192.168.194.160
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-redis
      port: 6379
      protocol: TCP
      targetPort: redis
    selector:
      app.kubernetes.io/component: replica
      app.kubernetes.io/instance: redis
      app.kubernetes.io/name: redis
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"workflow-engine","namespace":"llm-platform"},"spec":{"ports":[{"name":"http","port":3001,"targetPort":3001},{"name":"metrics","port":9090,"targetPort":9090}],"selector":{"app":"workflow-engine"},"type":"ClusterIP"}}
    creationTimestamp: "2025-10-21T20:24:19Z"
    name: workflow-engine
    namespace: llm-platform
    resourceVersion: "5341652"
    uid: 26f6e41b-509c-4047-8883-ab1334b43dc5
  spec:
    clusterIP: 192.168.194.242
    clusterIPs:
    - 192.168.194.242
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 3001
      protocol: TCP
      targetPort: 3001
    - name: metrics
      port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app: workflow-engine
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app.kubernetes.io/name":"monitoring-placeholder","monitoring":"enabled"},"name":"monitoring-placeholder","namespace":"monitoring"},"spec":{"ports":[{"name":"metrics","port":8080}],"selector":{"app.kubernetes.io/name":"monitoring-placeholder"}}}
    creationTimestamp: "2025-10-31T19:48:34Z"
    labels:
      app.kubernetes.io/name: monitoring-placeholder
      monitoring: enabled
    name: monitoring-placeholder
    namespace: monitoring
    resourceVersion: "5956189"
    uid: 17a31429-e0f7-4ee2-93cf-761aff583e63
  spec:
    clusterIP: 192.168.194.188
    clusterIPs:
    - 192.168.194.188
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: metrics
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/name: monitoring-placeholder
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"neo4j"},"name":"neo4j","namespace":"observability"},"spec":{"ports":[{"name":"http","port":7474,"protocol":"TCP","targetPort":7474},{"name":"bolt","port":7687,"protocol":"TCP","targetPort":7687}],"selector":{"app":"neo4j"},"type":"ClusterIP"}}
    creationTimestamp: "2025-10-20T00:31:29Z"
    labels:
      app: neo4j
    name: neo4j
    namespace: observability
    resourceVersion: "5240720"
    uid: a3cf862e-b799-4840-963a-50d4d6d44874
  spec:
    clusterIP: 192.168.194.222
    clusterIPs:
    - 192.168.194.222
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 7474
      protocol: TCP
      targetPort: 7474
    - name: bolt
      port: 7687
      protocol: TCP
      targetPort: 7687
    selector:
      app: neo4j
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"phoenix","dev.orbstack.domains":"phoenix.infrastructure.orb.local"},"name":"phoenix","namespace":"observability"},"spec":{"ports":[{"name":"http","port":80,"protocol":"TCP","targetPort":6006},{"name":"otlp-grpc","port":4317,"protocol":"TCP","targetPort":4317},{"name":"otlp-http","port":4318,"protocol":"TCP","targetPort":4318}],"selector":{"app":"phoenix"},"type":"ClusterIP"}}
    creationTimestamp: "2025-10-28T14:28:29Z"
    labels:
      app: phoenix
      dev.orbstack.domains: phoenix.infrastructure.orb.local
    name: phoenix
    namespace: observability
    resourceVersion: "5742964"
    uid: e8afcfe3-b6bd-4d2f-8bc5-3594a7203bc6
  spec:
    clusterIP: 192.168.194.155
    clusterIPs:
    - 192.168.194.155
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 6006
    - name: otlp-grpc
      port: 4317
      protocol: TCP
      targetPort: 4317
    - name: otlp-http
      port: 4318
      protocol: TCP
      targetPort: 4318
    selector:
      app: phoenix
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"agent-router","component":"gateway"},"name":"agent-router","namespace":"ossa-agents"},"spec":{"ports":[{"name":"http","port":3000,"protocol":"TCP","targetPort":3000}],"selector":{"app":"agent-router"},"type":"ClusterIP"}}
    creationTimestamp: "2025-10-10T13:46:57Z"
    labels:
      app: agent-router
      component: gateway
      dev.orbstack.domains: agent-router.orb.local
    name: agent-router
    namespace: ossa-agents
    resourceVersion: "5738859"
    uid: 1ae9dd40-e655-4817-9926-fea88c34dc13
  spec:
    clusterIP: 192.168.194.247
    clusterIPs:
    - 192.168.194.247
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 3000
    selector:
      app: agent-router
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"agent-router","component":"gateway"},"name":"agent-router-external","namespace":"ossa-agents"},"spec":{"ports":[{"name":"http","nodePort":30000,"port":3000,"protocol":"TCP","targetPort":3000}],"selector":{"app":"agent-router"},"type":"NodePort"}}
    creationTimestamp: "2025-10-10T13:46:57Z"
    labels:
      app: agent-router
      component: gateway
    name: agent-router-external
    namespace: ossa-agents
    resourceVersion: "4936393"
    uid: 276802ba-6edf-4956-8910-d22447dc5ce2
  spec:
    clusterIP: 192.168.194.197
    clusterIPs:
    - 192.168.194.197
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      nodePort: 30000
      port: 3000
      protocol: TCP
      targetPort: 3000
    selector:
      app: agent-router
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{"prometheus.io/path":"/metrics","prometheus.io/port":"9090","prometheus.io/scrape":"true"},"labels":{"app":"agent-router"},"name":"agent-router-metrics","namespace":"ossa-agents"},"spec":{"ports":[{"name":"metrics","port":9090,"protocol":"TCP","targetPort":3000}],"selector":{"app":"agent-router"},"type":"ClusterIP"}}
      prometheus.io/path: /metrics
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-10-10T14:03:20Z"
    labels:
      app: agent-router
    name: agent-router-metrics
    namespace: ossa-agents
    resourceVersion: "4937435"
    uid: 55f29837-fab9-436b-9c40-7db2eb3dc6f3
  spec:
    clusterIP: 192.168.194.149
    clusterIPs:
    - 192.168.194.149
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: metrics
      port: 9090
      protocol: TCP
      targetPort: 3000
    selector:
      app: agent-router
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"grafana"},"name":"grafana-service","namespace":"ossa-agents"},"spec":{"ports":[{"name":"grafana-http","port":3000,"protocol":"TCP","targetPort":3000}],"selector":{"app":"grafana"},"type":"LoadBalancer"}}
    creationTimestamp: "2025-11-04T02:02:52Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    labels:
      app: grafana
    name: grafana-service
    namespace: ossa-agents
    resourceVersion: "6171930"
    uid: bc805f8c-4a8f-45b4-aec1-a75cca865851
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 192.168.194.253
    clusterIPs:
    - 192.168.194.253
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: grafana-http
      nodePort: 32225
      port: 3000
      protocol: TCP
      targetPort: 3000
    selector:
      app: grafana
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"kafka","namespace":"ossa-agents"},"spec":{"clusterIP":"None","ports":[{"name":"plaintext","port":9092},{"name":"controller","port":9093}],"selector":{"app":"kafka"}}}
    creationTimestamp: "2025-10-10T13:13:16Z"
    name: kafka
    namespace: ossa-agents
    resourceVersion: "4935071"
    uid: 67c0b8f1-6cf3-4116-999a-241c9926c0c1
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: plaintext
      port: 9092
      protocol: TCP
      targetPort: 9092
    - name: controller
      port: 9093
      protocol: TCP
      targetPort: 9093
    selector:
      app: kafka
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kafka
      meta.helm.sh/release-namespace: ossa-agents
    creationTimestamp: "2025-10-10T13:01:24Z"
    labels:
      app.kubernetes.io/component: controller-eligible
      app.kubernetes.io/instance: kafka
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: kafka
      app.kubernetes.io/version: 4.0.0
      helm.sh/chart: kafka-32.4.3
    name: kafka-controller-headless
    namespace: ossa-agents
    resourceVersion: "4934243"
    uid: a5429c07-2240-4a50-9852-e4f0642d7743
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-interbroker
      port: 9094
      protocol: TCP
      targetPort: interbroker
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: client
    - name: tcp-controller
      port: 9093
      protocol: TCP
      targetPort: controller
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: controller-eligible
      app.kubernetes.io/instance: kafka
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: kafka
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"langfuse","namespace":"ossa-agents"},"spec":{"ports":[{"port":3000,"targetPort":3000}],"selector":{"app":"langfuse"},"type":"LoadBalancer"}}
    creationTimestamp: "2025-10-10T04:25:43Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    labels:
      dev.orbstack.domains: langfuse.orb.local
    name: langfuse
    namespace: ossa-agents
    resourceVersion: "5738878"
    uid: 3cff550a-42c2-4d34-a377-db721bdc67a1
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 192.168.194.206
    clusterIPs:
    - 192.168.194.206
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      nodePort: 31139
      port: 80
      protocol: TCP
      targetPort: 3000
    selector:
      app: langfuse
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"langfuse-postgres","namespace":"ossa-agents"},"spec":{"ports":[{"port":5432,"targetPort":5432}],"selector":{"app":"langfuse-postgres"},"type":"ClusterIP"}}
    creationTimestamp: "2025-10-10T04:25:43Z"
    name: langfuse-postgres
    namespace: ossa-agents
    resourceVersion: "4926380"
    uid: 5f2ae28a-998f-4a44-adad-21977c6371c8
  spec:
    clusterIP: 192.168.194.157
    clusterIPs:
    - 192.168.194.157
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 5432
      protocol: TCP
      targetPort: 5432
    selector:
      app: langfuse-postgres
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"litellm"},"name":"litellm","namespace":"ossa-agents"},"spec":{"ports":[{"name":"api","port":4000,"targetPort":4000},{"name":"metrics","port":4001,"targetPort":4001}],"selector":{"app":"litellm"},"type":"LoadBalancer"}}
    creationTimestamp: "2025-10-10T04:25:36Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    labels:
      app: litellm
    name: litellm
    namespace: ossa-agents
    resourceVersion: "7482282"
    uid: 66a5aca1-73dc-400a-90e0-c2468e034d23
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 192.168.194.237
    clusterIPs:
    - 192.168.194.237
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: api
      nodePort: 30256
      port: 4000
      protocol: TCP
      targetPort: 4000
    - name: metrics
      nodePort: 30866
      port: 4001
      protocol: TCP
      targetPort: 4001
    selector:
      app: litellm
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 192.168.139.2
        ipMode: VIP
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"litellm-postgres","namespace":"ossa-agents"},"spec":{"ports":[{"port":5432,"targetPort":5432}],"selector":{"app":"litellm-postgres"},"type":"ClusterIP"}}
    creationTimestamp: "2025-10-10T04:25:36Z"
    name: litellm-postgres
    namespace: ossa-agents
    resourceVersion: "4926308"
    uid: f5760a12-e604-4a05-aca9-d4d2d4b0522c
  spec:
    clusterIP: 192.168.194.215
    clusterIPs:
    - 192.168.194.215
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 5432
      protocol: TCP
      targetPort: 5432
    selector:
      app: litellm-postgres
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"litellm-redis","namespace":"ossa-agents"},"spec":{"ports":[{"port":6379,"targetPort":6379}],"selector":{"app":"litellm-redis"},"type":"ClusterIP"}}
    creationTimestamp: "2025-10-10T04:25:36Z"
    name: litellm-redis
    namespace: ossa-agents
    resourceVersion: "4926282"
    uid: 706f1662-77a1-4685-9a0b-d5324e935456
  spec:
    clusterIP: 192.168.194.246
    clusterIPs:
    - 192.168.194.246
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 6379
      protocol: TCP
      targetPort: 6379
    selector:
      app: litellm-redis
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: ossa-agents
    creationTimestamp: "2025-10-30T15:37:08Z"
    labels:
      app: loki
      app.kubernetes.io/managed-by: Helm
      chart: loki-2.16.0
      heritage: Helm
      release: loki
    name: loki
    namespace: ossa-agents
    resourceVersion: "5893982"
    uid: 6a8ae47d-ba4c-441b-9ee7-b53cc3215334
  spec:
    clusterIP: 192.168.194.187
    clusterIPs:
    - 192.168.194.187
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: http-metrics
    selector:
      app: loki
      release: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: ossa-agents
    creationTimestamp: "2025-10-30T15:37:08Z"
    labels:
      app: loki
      app.kubernetes.io/managed-by: Helm
      chart: loki-2.16.0
      heritage: Helm
      release: loki
      variant: headless
    name: loki-headless
    namespace: ossa-agents
    resourceVersion: "5893984"
    uid: 1c7741f6-994e-47b8-8c23-b17b7934070c
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: http-metrics
    selector:
      app: loki
      release: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: ossa-agents
    creationTimestamp: "2025-10-30T15:37:08Z"
    labels:
      app: loki
      app.kubernetes.io/managed-by: Helm
      chart: loki-2.16.0
      heritage: Helm
      release: loki
    name: loki-memberlist
    namespace: ossa-agents
    resourceVersion: "5893980"
    uid: 7f7c7c64-6705-45b3-b9ab-c7f698bbf8d6
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 7946
      protocol: TCP
      targetPort: memberlist-port
    publishNotReadyAddresses: true
    selector:
      app: loki
      release: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"postgres"},"name":"postgres-service","namespace":"ossa-agents"},"spec":{"ports":[{"port":5432,"protocol":"TCP","targetPort":5432}],"selector":{"app":"postgres"}}}
    creationTimestamp: "2025-09-09T21:18:14Z"
    labels:
      app: postgres
    name: postgres-service
    namespace: ossa-agents
    resourceVersion: "3939707"
    uid: fe27c99c-9f86-4d60-8735-44b295401511
  spec:
    clusterIP: 192.168.194.230
    clusterIPs:
    - 192.168.194.230
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 5432
      protocol: TCP
      targetPort: 5432
    selector:
      app: postgres
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"prometheus"},"name":"prometheus-service","namespace":"ossa-agents"},"spec":{"ports":[{"port":9090,"protocol":"TCP","targetPort":9090}],"selector":{"app":"prometheus"},"type":"ClusterIP"}}
    creationTimestamp: "2025-09-09T21:04:07Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    labels:
      app: prometheus
    name: prometheus-service
    namespace: ossa-agents
    resourceVersion: "7492061"
    uid: 4117498d-8dd7-4105-972f-5545b05ea331
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 192.168.194.146
    clusterIPs:
    - 192.168.194.146
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - nodePort: 31471
      port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app: prometheus
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 192.168.139.2
        ipMode: VIP
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"qdrant"},"name":"qdrant-service","namespace":"ossa-agents"},"spec":{"ports":[{"name":"http","port":6333,"protocol":"TCP","targetPort":6333},{"name":"grpc","port":6334,"protocol":"TCP","targetPort":6334}],"selector":{"app":"qdrant"},"type":"LoadBalancer"}}
    creationTimestamp: "2025-09-09T21:18:20Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    labels:
      app: qdrant
    name: qdrant-service
    namespace: ossa-agents
    resourceVersion: "7641486"
    uid: e2ff5ec3-8b53-4de5-b419-1acd00c84be4
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 192.168.194.213
    clusterIPs:
    - 192.168.194.213
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      nodePort: 30582
      port: 6333
      protocol: TCP
      targetPort: 6333
    - name: grpc
      nodePort: 30417
      port: 6334
      protocol: TCP
      targetPort: 6334
    selector:
      app: qdrant
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 192.168.139.2
        ipMode: VIP
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: tempo
      meta.helm.sh/release-namespace: ossa-agents
    creationTimestamp: "2025-10-30T15:36:09Z"
    labels:
      app.kubernetes.io/instance: tempo
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: tempo
      app.kubernetes.io/version: 2.9.0
      helm.sh/chart: tempo-1.24.0
    name: tempo
    namespace: ossa-agents
    resourceVersion: "5893846"
    uid: ef217685-bb5f-4be1-baf6-4ebaa0ea43fe
  spec:
    clusterIP: 192.168.194.209
    clusterIPs:
    - 192.168.194.209
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tempo-jaeger-thrift-compact
      port: 6831
      protocol: UDP
      targetPort: 6831
    - name: tempo-jaeger-thrift-binary
      port: 6832
      protocol: UDP
      targetPort: 6832
    - name: tempo-prom-metrics
      port: 3200
      protocol: TCP
      targetPort: 3200
    - name: tempo-jaeger-thrift-http
      port: 14268
      protocol: TCP
      targetPort: 14268
    - name: grpc-tempo-jaeger
      port: 14250
      protocol: TCP
      targetPort: 14250
    - name: tempo-zipkin
      port: 9411
      protocol: TCP
      targetPort: 9411
    - name: tempo-otlp-legacy
      port: 55680
      protocol: TCP
      targetPort: 55680
    - name: tempo-otlp-http-legacy
      port: 55681
      protocol: TCP
      targetPort: 55681
    - name: grpc-tempo-otlp
      port: 4317
      protocol: TCP
      targetPort: 4317
    - name: tempo-otlp-http
      port: 4318
      protocol: TCP
      targetPort: 4318
    - name: tempo-opencensus
      port: 55678
      protocol: TCP
      targetPort: 55678
    selector:
      app.kubernetes.io/instance: tempo
      app.kubernetes.io/name: tempo
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"agent-gateway","namespace":"ossa-prod"},"spec":{"ports":[{"port":80,"targetPort":3000}],"selector":{"app":"agent-gateway"},"type":"ClusterIP"}}
    creationTimestamp: "2025-10-07T14:51:23Z"
    labels:
      dev.orbstack.domains: ossa-gateway.orb.local
    name: agent-gateway
    namespace: ossa-prod
    resourceVersion: "5738819"
    uid: 16a11abb-1b7b-43cc-8eaa-38a7d4fa2fff
  spec:
    clusterIP: 192.168.194.159
    clusterIPs:
    - 192.168.194.159
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 80
      protocol: TCP
      targetPort: 3000
    selector:
      app: agent-gateway
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"phoenix","namespace":"phoenix"},"spec":{"loadBalancerIP":"192.168.139.3","ports":[{"name":"http","port":6006,"protocol":"TCP","targetPort":6006},{"name":"otlp-grpc","port":4317,"protocol":"TCP","targetPort":4317},{"name":"otlp-http","port":4318,"protocol":"TCP","targetPort":4318}],"selector":{"app":"phoenix"},"type":"LoadBalancer"}}
    creationTimestamp: "2025-10-09T20:34:12Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    name: phoenix
    namespace: phoenix
    resourceVersion: "7482229"
    uid: 378c73ce-0e06-4e22-b7fd-2b57e8d5057a
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 192.168.194.179
    clusterIPs:
    - 192.168.194.179
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    loadBalancerIP: 192.168.139.3
    ports:
    - name: http
      nodePort: 31987
      port: 6006
      protocol: TCP
      targetPort: 6006
    - name: otlp-grpc
      nodePort: 32506
      port: 4317
      protocol: TCP
      targetPort: 4317
    - name: otlp-http
      nodePort: 30739
      port: 4318
      protocol: TCP
      targetPort: 4318
    selector:
      app: phoenix
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 192.168.139.2
        ipMode: VIP
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"rfp-automation-backend","dev.orbstack.domains":"rfp-api.orb.local"},"name":"rfp-automation-backend","namespace":"rfp-automation"},"spec":{"ports":[{"name":"http","port":80,"targetPort":3001}],"selector":{"app":"rfp-automation-backend"},"type":"ClusterIP"}}
    creationTimestamp: "2025-10-28T12:43:00Z"
    labels:
      app: rfp-automation-backend
      dev.orbstack.domains: rfp-api.orb.local
    name: rfp-automation-backend
    namespace: rfp-automation
    resourceVersion: "5737842"
    uid: 3ede0fe7-b8c6-4eb7-92ca-ede3b18accc2
  spec:
    clusterIP: 192.168.194.147
    clusterIPs:
    - 192.168.194.147
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 3001
    selector:
      app: rfp-automation-backend
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"rfp-automation-frontend","dev.orbstack.domains":"rfp.orb.local"},"name":"rfp-automation-frontend","namespace":"rfp-automation"},"spec":{"ports":[{"name":"http","port":80,"targetPort":3002}],"selector":{"app":"rfp-automation-frontend"},"type":"ClusterIP"}}
    creationTimestamp: "2025-10-28T12:43:00Z"
    labels:
      app: rfp-automation-frontend
      dev.orbstack.domains: rfp.orb.local
    name: rfp-automation-frontend
    namespace: rfp-automation
    resourceVersion: "5737845"
    uid: d5fd2e55-3dd1-490d-8b77-2bf47a128002
  spec:
    clusterIP: 192.168.194.205
    clusterIPs:
    - 192.168.194.205
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 3002
    selector:
      app: rfp-automation-frontend
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"rfp-automation-postgres","app.kubernetes.io/managed-by":"kubectl","app.kubernetes.io/name":"rfp-automation","app.kubernetes.io/part-of":"llm-ecosystem"},"name":"rfp-automation-postgres","namespace":"rfp-automation"},"spec":{"ports":[{"name":"postgres","port":5432,"targetPort":5432}],"selector":{"app":"rfp-automation-postgres","app.kubernetes.io/managed-by":"kubectl","app.kubernetes.io/name":"rfp-automation","app.kubernetes.io/part-of":"llm-ecosystem"},"type":"ClusterIP"}}
    creationTimestamp: "2025-10-28T12:43:00Z"
    labels:
      app: rfp-automation-postgres
      app.kubernetes.io/managed-by: kubectl
      app.kubernetes.io/name: rfp-automation
      app.kubernetes.io/part-of: llm-ecosystem
    name: rfp-automation-postgres
    namespace: rfp-automation
    resourceVersion: "5735362"
    uid: bfaddccd-a612-4aa5-8185-ba4716613284
  spec:
    clusterIP: 192.168.194.207
    clusterIPs:
    - 192.168.194.207
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: postgres
      port: 5432
      protocol: TCP
      targetPort: 5432
    selector:
      app: rfp-automation-postgres
      app.kubernetes.io/managed-by: kubectl
      app.kubernetes.io/name: rfp-automation
      app.kubernetes.io/part-of: llm-ecosystem
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"rfp-automation-redis","app.kubernetes.io/managed-by":"kubectl","app.kubernetes.io/name":"rfp-automation","app.kubernetes.io/part-of":"llm-ecosystem"},"name":"rfp-automation-redis","namespace":"rfp-automation"},"spec":{"ports":[{"name":"redis","port":6379,"targetPort":6379}],"selector":{"app":"rfp-automation-redis","app.kubernetes.io/managed-by":"kubectl","app.kubernetes.io/name":"rfp-automation","app.kubernetes.io/part-of":"llm-ecosystem"},"type":"ClusterIP"}}
    creationTimestamp: "2025-10-28T12:43:00Z"
    labels:
      app: rfp-automation-redis
      app.kubernetes.io/managed-by: kubectl
      app.kubernetes.io/name: rfp-automation
      app.kubernetes.io/part-of: llm-ecosystem
    name: rfp-automation-redis
    namespace: rfp-automation
    resourceVersion: "5735366"
    uid: ec060186-6539-4609-b486-9a4d935df86d
  spec:
    clusterIP: 192.168.194.208
    clusterIPs:
    - 192.168.194.208
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: redis
      port: 6379
      protocol: TCP
      targetPort: 6379
    selector:
      app: rfp-automation-redis
      app.kubernetes.io/managed-by: kubectl
      app.kubernetes.io/name: rfp-automation
      app.kubernetes.io/part-of: llm-ecosystem
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"aiflow-agent"},"name":"aiflow-agent","namespace":"social-agents"},"spec":{"ports":[{"name":"http","port":8000,"protocol":"TCP","targetPort":8000},{"name":"metrics","port":9090,"protocol":"TCP","targetPort":9090}],"selector":{"app":"aiflow-agent"},"type":"ClusterIP"}}
    creationTimestamp: "2025-10-21T20:24:46Z"
    labels:
      app: aiflow-agent
    name: aiflow-agent
    namespace: social-agents
    resourceVersion: "5341784"
    uid: 5d2153a2-1762-4a4f-81ae-629e6e1390a6
  spec:
    clusterIP: 192.168.194.191
    clusterIPs:
    - 192.168.194.191
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8000
      protocol: TCP
      targetPort: 8000
    - name: metrics
      port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app: aiflow-agent
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app.kubernetes.io/component":"server","app.kubernetes.io/managed-by":"kustomize","app.kubernetes.io/name":"vault","app.kubernetes.io/part-of":"security-platform"},"name":"vault","namespace":"vault"},"spec":{"ports":[{"name":"http","port":8200,"protocol":"TCP","targetPort":8200},{"name":"https-internal","port":8201,"protocol":"TCP","targetPort":8201}],"selector":{"app.kubernetes.io/component":"server","app.kubernetes.io/name":"vault"},"type":"ClusterIP"}}
    creationTimestamp: "2025-10-08T13:16:41Z"
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/managed-by: kustomize
      app.kubernetes.io/name: vault
      app.kubernetes.io/part-of: security-platform
    name: vault
    namespace: vault
    resourceVersion: "4742152"
    uid: f084562d-efc4-46f9-bdf3-13ee5ac8dde7
  spec:
    clusterIP: 192.168.194.162
    clusterIPs:
    - 192.168.194.162
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8200
      protocol: TCP
      targetPort: 8200
    - name: https-internal
      port: 8201
      protocol: TCP
      targetPort: 8201
    selector:
      app.kubernetes.io/component: server
      app.kubernetes.io/name: vault
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app.kubernetes.io/component":"server","app.kubernetes.io/managed-by":"kustomize","app.kubernetes.io/name":"vault","app.kubernetes.io/part-of":"security-platform"},"name":"vault-internal","namespace":"vault"},"spec":{"clusterIP":"None","ports":[{"name":"http","port":8200,"targetPort":8200},{"name":"https-internal","port":8201,"targetPort":8201}],"publishNotReadyAddresses":true,"selector":{"app.kubernetes.io/component":"server","app.kubernetes.io/name":"vault"},"type":"ClusterIP"}}
    creationTimestamp: "2025-10-08T13:16:41Z"
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/managed-by: kustomize
      app.kubernetes.io/name: vault
      app.kubernetes.io/part-of: security-platform
    name: vault-internal
    namespace: vault
    resourceVersion: "4742155"
    uid: 8dbc3be4-8854-4b20-bcb9-10e8847d68ac
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8200
      protocol: TCP
      targetPort: 8200
    - name: https-internal
      port: 8201
      protocol: TCP
      targetPort: 8201
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: server
      app.kubernetes.io/name: vault
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app.kubernetes.io/component":"ui","app.kubernetes.io/managed-by":"kustomize","app.kubernetes.io/name":"vault","app.kubernetes.io/part-of":"security-platform"},"name":"vault-ui","namespace":"vault"},"spec":{"ports":[{"name":"https","port":443,"protocol":"TCP","targetPort":8200}],"selector":{"app.kubernetes.io/component":"server","app.kubernetes.io/name":"vault"},"type":"LoadBalancer"}}
    creationTimestamp: "2025-10-08T13:16:41Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    labels:
      app.kubernetes.io/component: ui
      app.kubernetes.io/managed-by: kustomize
      app.kubernetes.io/name: vault
      app.kubernetes.io/part-of: security-platform
      dev.orbstack.domains: vault.orb.local
    name: vault-ui
    namespace: vault
    resourceVersion: "5738822"
    uid: ddd0f82a-abfc-49f3-b6fc-18fad8336a33
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 192.168.194.245
    clusterIPs:
    - 192.168.194.245
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      nodePort: 30781
      port: 443
      protocol: TCP
      targetPort: 8200
    selector:
      app.kubernetes.io/component: server
      app.kubernetes.io/name: vault
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/7xUUW/jNgz+KwOfbTduclljYA9FWwzFdmmQ5PZyKApaYhItsiRItK9B4f8+SE07F5frFXsY/GBIJD+RHz/yCdCpv8gHZQ1UgM6Fs66EDPbKSKjgGqmxZkUMGTTEKJERqidAYywjK2tCPNr6bxIciAuvbCGQWVOh7JmKGJD90G6/GfL5tttDBWddmf3yhzLytxX5Tgn6aZzBhqCCjbeGycgP+QeHIgbhlgznYocMfQbCUypmrRoKjI2DyrRaZ6CxJv1uiTsMO6hAfBIXs1lZl9MxnstyNpvORuOL+tNkej4dz0aiFvLiVzkZQwahE8Ia9lZr8sV+HAZoxkoKpEmw9bE01IF+EhI68T0RHwg4zcQRKnRC1/kLYD4533wigTH7Yei+rSkPh8DUxNjgSESq/i3gCRpksfvzlUV07h30vs+AqXEamVLwQHEfaNH74P8fiwMqsGXb2NbwUdOXQsTT2u7JQJXam0F8A5UhH6D6+gRkuvQ/5rNaXj0s7pZryKBD3car8Wg0gj5747K8nP9+sxo4jYr0nb3xvL5ZrR8Wy7v13cBzfbX43uf9F5PP7WL4Xjk7L8rpRVHOJkU5nUB/n4FqcBttHo3YkT/ba+Uc+VzXVTcqJkUZ25KcFq3WC6uVOEAFt5u55YWnQIbhVZO6zlm4PKWSgbOen/l6pW9hPUMV7RnsbODh+TSGt2yF1S8c3GfgKdjWC4qCin0k0XrFh6uoiUdOQkSHtdKKFT2rTkqovsL8Zv1wef35dg73fR+RnFc2hWoMYX4cqzQreZzyXHjFSqCGk6+EQxCsw1AIhrhQrpsUyj1srP+GXg7ph/4+JTzU2XwwzZABW03+ZWtHpW02JBgqmNuV2JFsdVw2e4o9SDl6q6mIU+4NMYWo+wYDk4+71kWstKZuHlXgkPTxXyCPM5Y7jYZ+iPyMcXVk7VJKa8Kd0YfTAfdxClsnkWnFHpm2h0hrHGVltl+S4Xk/PX4x2KHSWGuCqowr6OAia8s3vmmqGblNTRet92R43jY1+ZdCJVSjDCQF5UmeMpl091mFcOJ6SSgPUI36/p8AAAD//0Yy8nmVBwAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Service
      objectset.rio.cattle.io/owner-name: frontend
      objectset.rio.cattle.io/owner-namespace: agent-chat
    creationTimestamp: "2025-10-04T15:48:21Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: c5c8991b163a2d19969038b54626390cbcd87d43
      svccontroller.k3s.cattle.io/nodeselector: "false"
      svccontroller.k3s.cattle.io/svcname: frontend
      svccontroller.k3s.cattle.io/svcnamespace: agent-chat
    name: svclb-frontend-42f5eca3
    namespace: kube-system
    resourceVersion: "7642833"
    uid: 651aa697-5081-436e-931d-f97b17e53ca5
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: svclb-frontend-42f5eca3
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: svclb-frontend-42f5eca3
          svccontroller.k3s.cattle.io/svcname: frontend
          svccontroller.k3s.cattle.io/svcnamespace: agent-chat
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: SRC_PORT
            value: "3000"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "3000"
          - name: DEST_IPS
            value: 192.168.194.164
          image: rancher/klipper-lb:v0.4.13
          imagePullPolicy: IfNotPresent
          name: lb-tcp-3000
          ports:
          - containerPort: 3000
            hostPort: 3000
            name: lb-tcp-3000
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          sysctls:
          - name: net.ipv4.ip_forward
            value: "1"
        serviceAccount: svclb
        serviceAccountName: svclb
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/7xUTW/jNhD9K8WcJVmOHTcR0EOQBEXQrmPY3l4WRjAixzZriiTIkTZGoP9eUHZSpXU+2sNCB4GcmTfk43vzBOjUH+SDsgYKQOfCoBlCAjtlJBRwg1RZsyCGBCpilMgIxROgMZaRlTUhLm35JwkOxJlXNhPIrClTdqAiBiRvxu13Qz7dNDsoYNAMk59+U0b+siDfKEEf1hmsCArYeFyjwTT8h7LgUMRaGwKmuCHDAdoEhKfuUktVUWCsHBSm1joBjSXpd6+6xbCFAs6lyKUYj8ZyMhmNSJaE6zVOJpdC/Ixncox0MREkc0ggNEJYw95qTT7bjUIPzVhJgTQJth4KWKMO9EFJaMSbhHyi7g1GjpChEbpM/wGcluIiP19fCDjkPUPs6pLSsA9MVYQIjkRk7u/7PEGFLLa/v5CKzn3cpG0TYKqcRqYOo6fHTzzcp3r8eIp7BGHNtrK14aMBroSIq6XdkYGi00ACsQkqQz5A8e0JyDTd/3iuxfz6YXY/X0ICDeo6bo3yPIc2eZUyv5r+ervoJeVZ9w1eZd7cLpYPs/n98r6Xubye/Tvn/Y5dzt2s3294eZYNJxfZ8HKcnZ2PoF0loCrcxJhHI7bkBzutnCOf6rJo8mycDUdwTJrVWs+sVmIPBdytp5ZnngIZhhfB6jJl4dLuKAk46/nA1wt9M+sZihhPYGsD99enMbxlK6x+5mCVgKdgay8o6iu+I4naK95fW8P0yJ0u0WGptGJFBxFKCcU3mN4uH65uvtxNYdW2Ecl5ZbtSjSFMj57rHJTGUZAKr1gJ1HCyS9gHwTr0hWCIM+Wacabcw9r67+hln35oV92B+zqb9qwOCbDV5J9HfFTaek2CoYCpXYgtyVpH2e8ovkF3Rm81ZdH73hBTiMKvMDD5OJFdxOpm2e2jCt1s+X+QR5OlTqOhN5EPGNdH1q6ktCbcG70/XbCKLqydRKYFe2Ta7COt0cvKbL52gcPUevxqsEGlsdQExTBOpL2LrM1f5XauZuS6e3RRe0+Gp3VVkn++qIQiT0BSUJ7kqZDp9r6oEE5szwnlHoq8bf8KAAD//5ovhObCBwAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Service
      objectset.rio.cattle.io/owner-name: grafana-service
      objectset.rio.cattle.io/owner-namespace: ossa-agents
    creationTimestamp: "2025-11-04T02:02:52Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 5dc0dc434d6633edbeaffa669cc7a2d4ae86ced0
      svccontroller.k3s.cattle.io/nodeselector: "false"
      svccontroller.k3s.cattle.io/svcname: grafana-service
      svccontroller.k3s.cattle.io/svcnamespace: ossa-agents
    name: svclb-grafana-service-bc805f8c
    namespace: kube-system
    resourceVersion: "7482326"
    uid: fabaf0c6-32d7-4764-86ec-cb0f9bcd80c8
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: svclb-grafana-service-bc805f8c
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: svclb-grafana-service-bc805f8c
          svccontroller.k3s.cattle.io/svcname: grafana-service
          svccontroller.k3s.cattle.io/svcnamespace: ossa-agents
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: SRC_PORT
            value: "3000"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "3000"
          - name: DEST_IPS
            value: 192.168.194.253
          image: rancher/klipper-lb:v0.4.13
          imagePullPolicy: IfNotPresent
          name: lb-tcp-3000
          ports:
          - containerPort: 3000
            hostPort: 3000
            name: lb-tcp-3000
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          sysctls:
          - name: net.ipv4.ip_forward
            value: "1"
        serviceAccount: svclb
        serviceAccountName: svclb
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberMisscheduled: 0
    numberReady: 0
    numberUnavailable: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/8RV32/iOBD+V07znKQEskAj3UPVVqfqbikC9l5WqBqcAXw4tmVPskVV/veTA23TW/pjT7pb8YBsz3xjf9/kmwdAK/8k56XRkANa68/qFCLYSV1ADldIpdFzYoigJMYCGSF/ANTaMLI02oelWf1Fgj1x4qRJBDIrSqQ5kwEDolfPzTdNLt7UO8jhrE6jX36Xuvh1Tq6Wgt7N01gS5CD1xpH3sd5IfR8Lo9kZpch9KN9bFN+BQBOBcNS+byFL8oylhVxXSkWgcEXqzVdv0W8hh9F68GkwLtaf1kMU6XhdiNGgPyyGo6xYjzAdZOn5MB1lQ4jA1+L53slu4Dto2hTkSZFg4yCHNSpP76T4WrzPzQcAXiXniO5roVbxazXifr8/Wn3KBBwSHtF21Ypiv/dMZcDylkRg8/mND1Aii+0fT0SjtT9QrWkiYCqtQqYWrNO3H1D1x4r9VCE67GHFpjSV5uPHcyFEWC3MjjTkbdNEEMqg1OQ85F8fgHTd/h+vOJ9d3k1vZwuIoEZVha1xD5roRcDsYvLb9bwT0kva39mLyKvr+eJuOrtd3HYiF5fT72PeqtdG3Ey71dLzfpIOx0l6niX97ByaZQSyxE04c6jFltzZTklrycVqlde9JEvSARyDppVSU6Ok2EMON+uJ4akjT5rhqaXVKmZh43EPIrDG8YGpJ+KmxjHk414EW+P5eXUq2xk2wqjHly8jcORN5QSFPgvakaic5P2l0Uz33PYnWlxJJVnSoRmLAvKvMLle3F1cfb6ZwLJpAkPvS5dlg/9Xu38U/InihZu8oV6WDbrytcuTAP+ZgMsALk2bqtD7ydFOW0+Mg+HHwkmWAhWcrOL3XrDy3RbQxIm0dZZIe7c27hu6oks9NMv2wl1zmHRcHCJgo8g9zvRgD+s1CYYcJmYutlRUKsydHQX+2zs6oygJbu40MfngVyV6PkxeG7DaiXV9Lz37tjf+DeTRG2OrUNOryAeMyyNrF0VhtL/Van86YRmss7IFMs3ZIdNmH2gNFiz15kt7cJhD91801igVrhRBnobRsreBtdmL2NaKGblqRReVc6R5UpUrco8PLSDvRVCQl46KU0e63fssvT+xPSMs9pD3mubvAAAA///m6j7WswkAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Service
      objectset.rio.cattle.io/owner-name: ingress-nginx-controller
      objectset.rio.cattle.io/owner-namespace: ingress-nginx
    creationTimestamp: "2025-09-07T03:15:30Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 7f3538df5f6ac18fdc7326d674df7a1341961746
      svccontroller.k3s.cattle.io/nodeselector: "false"
      svccontroller.k3s.cattle.io/svcname: ingress-nginx-controller
      svccontroller.k3s.cattle.io/svcnamespace: ingress-nginx
    name: svclb-ingress-nginx-controller-2227b54c
    namespace: kube-system
    resourceVersion: "7642925"
    uid: 8e1ee24a-9380-47e7-9b76-3360fc82ebfb
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: svclb-ingress-nginx-controller-2227b54c
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: svclb-ingress-nginx-controller-2227b54c
          svccontroller.k3s.cattle.io/svcname: ingress-nginx-controller
          svccontroller.k3s.cattle.io/svcnamespace: ingress-nginx
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: SRC_PORT
            value: "80"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "80"
          - name: DEST_IPS
            value: 192.168.194.249
          image: rancher/klipper-lb:v0.4.13
          imagePullPolicy: IfNotPresent
          name: lb-tcp-80
          ports:
          - containerPort: 80
            hostPort: 80
            name: lb-tcp-80
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - env:
          - name: SRC_PORT
            value: "443"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "443"
          - name: DEST_IPS
            value: 192.168.194.249
          image: rancher/klipper-lb:v0.4.13
          imagePullPolicy: IfNotPresent
          name: lb-tcp-443
          ports:
          - containerPort: 443
            hostPort: 443
            name: lb-tcp-443
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          sysctls:
          - name: net.ipv4.ip_forward
            value: "1"
        serviceAccount: svclb
        serviceAccountName: svclb
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "2"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/7xUUW/iOBf9K5/ucxICpUwa6Xuo2mpV7Q5FwOzLCFU3zgW8OLZl32SKqvz3lQN0Ug1lq31Y8YBin3Pse3zufQW08k9yXhoNOaC1ftAMIYKd1CXkcI9UGb0ghggqYiyREfJXQK0NI0ujffg0xV8k2BMnTppEILOiRJqBDBoQfbhvfmhy8abZQQ6DZhj973epy/8vyDVS0D/yNFYEOSjUm3XtP4f3FkUgGe8xxg1p9tBGIBx11SxlRZ6xspDrWqkIFBakLta4Rb+FHDJaf7meDEeTbFIMRylSdn01IfFlVE6ylG6yUTlOs4kYQwS+EcJodkYpcsnuyvfUtCnJkyLBxkEOa1RdYZcovhG/OvEJwgdWHLV8I1QRnxTjK7FeX1+nCAfAiburC4r93jNVgestieDVzwpeoUIW2z/ebERrL6i3bQRMlVXI1JF7mfvEG10W/w9t7HmBNZvK1JqPsb4VInwtzY405N0DRxAOQanJeci/vwLppvs/Xmgxv3uePc2XEEGDqg5LWQpt9A4wv53+9rDoQdKk+w3eIe8fFsvn2fxp+dRDLu9mv2IundchHmf904Y3o2Q4yZLhzTgZpRNoVxHICjdhz6EWW3KDnZLWkotVkTdpMk6GV3AEzWqlZkZJsYccHtdTwzNHnjTDWyBVEbOwcZZCBNY4Pjj1ZtzMOIY8SyPYGs8/v86xnWEjjDpVvorAkTe1ExRSFN6ORO0k7++MZnrhLn1osZBKsqRD1MoS8u8wfVg+395/fZzCqm2DknXSdFSF3k+PvdQ1SBx6OxZOshSo4Owpfu8FK99/fE2cSNuME2mf18b9QFf2bYd21V24n61pr4UhAjaK3GlYh3St1yQYcpiahdhSWasQ9R0F77s7OqMoCa3tNDH5EPYKPZMLI9YGrW44PbxI382Mfyd5bKzYKtT0ofJB4+7o2m1ZGu2ftNqfJ6xC59W2RKYFO2Ta7IOtoX+l3nzrNg5D6eWbxgalwkIR5MMwd/Y2uDZ/h+06mZHr7tFF7RxpntZVQe5UaAl5GkFJXjoqz23pbu2r9P7M8pyw3EOetu3fAQAA///QX+kYjAcAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Service
      objectset.rio.cattle.io/owner-name: langfuse
      objectset.rio.cattle.io/owner-namespace: ossa-agents
    creationTimestamp: "2025-10-10T04:25:43Z"
    generation: 2
    labels:
      objectset.rio.cattle.io/hash: 8ef75612686b120ae8536ec72d680e982d4086c4
      svccontroller.k3s.cattle.io/nodeselector: "false"
      svccontroller.k3s.cattle.io/svcname: langfuse
      svccontroller.k3s.cattle.io/svcnamespace: ossa-agents
    name: svclb-langfuse-3cff550a
    namespace: kube-system
    resourceVersion: "7482339"
    uid: 1f0f3d2a-6fe1-4dbe-a30f-6dea22bbec01
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: svclb-langfuse-3cff550a
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: svclb-langfuse-3cff550a
          svccontroller.k3s.cattle.io/svcname: langfuse
          svccontroller.k3s.cattle.io/svcnamespace: ossa-agents
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: SRC_PORT
            value: "80"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "80"
          - name: DEST_IPS
            value: 192.168.194.206
          image: rancher/klipper-lb:v0.4.13
          imagePullPolicy: IfNotPresent
          name: lb-tcp-80
          ports:
          - containerPort: 80
            hostPort: 80
            name: lb-tcp-80
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          sysctls:
          - name: net.ipv4.ip_forward
            value: "1"
        serviceAccount: svclb
        serviceAccountName: svclb
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberMisscheduled: 0
    numberReady: 0
    numberUnavailable: 1
    observedGeneration: 2
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/7xUwW7jNhD9lWLOkiLZ2awtoIcgGxRBu45he3tZGMGIGtusKZIgR9oYgf69IO2kCupNgh4KHQRy5j2Sb97ME6CVf5Lz0mgoAa31F10BCeylrqGEL0iN0UtiSKAhxhoZoXwC1NowsjTah6Wp/iLBnjhz0mQCmRVl0lzIwAHJT+PmhyaXbrs9lHDRFckvv0td/7ok10lB7+I0NgQlKFk5EjvkDwG8RRFQuCXNaYT1CQhH8TUr2ZBnbCyUulUqAYUVqTffuEO/gxIm06vP4yqfFJdXo/F4Un0a49VGVKPRtBgJzPNplWM1zieQgO+EMJqdUYpcth/7AZs2NXlSJNg4KGGDytM7EN+JM0p8AHFeihOX74Sq0hfGdPSpmFxOKIdjxjN231aU+oNnagLYWxJBrH+e8AQNstj98aIjWvsWfd8nwNRYhUwRPXDdB6r0Dvv/qORADWzZNKbVfHL2tRBhtTJ70lDGGicQzkCpyXkovz8B6S7+TxdaLm4e5veLFSTQoWrD1jif5NAnr1IW17PfbpeDpDyL38WrzC+3y9XDfHG/uh9krm7m/855+8SYczcfnldMR1lxNcmK6WU2Kj5Dv05ANrgNMYda7Mhd7JW0llyqqrLLs8usGMMpad4qNTdKigOUcLeZGZ478qQZXnypqpSFTeNVErDG8VGvF/nmxjGUIZ7Azngers9zOMNGGPWswToBR960TlCwVKgjidZJPtwYzfTI0YposZJKsqSj7+oayu8wu109XH/5ejeDdd8HJuukiVCF3s9OrRXbJQ2tngonWQpUcPYUf/CClR8aQRNn0naXmbQPG+N+oKuH8kO/jhce+mw26GhIgI0i9zy7g9M2GxIMJczMUuyoblWYOHsKNYh3dEZRFhrdaWLywfcNeiYXBq4NXHFW3T5Kzz76479QnnostQo1/ZT5yHFzUu26ro3291odzgPWoQtbWyPTkh0ybQ9B1tDKUm+/xcBxRD1+09ihVFgpgrIIQ+hgg2qLV7mxqxm5jUUXrXOkedY2Fbnnh9ZQ5gnU5KWj+lxIx72v0vsz2wvC+gBl3vd/BwAA//9NM+qjmwcAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Service
      objectset.rio.cattle.io/owner-name: librechat
      objectset.rio.cattle.io/owner-namespace: agent-chat
    creationTimestamp: "2025-10-04T15:48:21Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 89673b081462338b53a6fcb22912ca009b0ab308
      svccontroller.k3s.cattle.io/nodeselector: "false"
      svccontroller.k3s.cattle.io/svcname: librechat
      svccontroller.k3s.cattle.io/svcnamespace: agent-chat
    name: svclb-librechat-251848e0
    namespace: kube-system
    resourceVersion: "7642884"
    uid: bd295bdd-4096-4d42-a946-964f520339c9
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: svclb-librechat-251848e0
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: svclb-librechat-251848e0
          svccontroller.k3s.cattle.io/svcname: librechat
          svccontroller.k3s.cattle.io/svcnamespace: agent-chat
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: SRC_PORT
            value: "3080"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "3080"
          - name: DEST_IPS
            value: 192.168.194.217
          image: rancher/klipper-lb:v0.4.13
          imagePullPolicy: IfNotPresent
          name: lb-tcp-3080
          ports:
          - containerPort: 3080
            hostPort: 3080
            name: lb-tcp-3080
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          sysctls:
          - name: net.ipv4.ip_forward
            value: "1"
        serviceAccount: svclb
        serviceAccountName: svclb
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/8xVUWvrOBP9Kx/zbLt2kyap4XsobVnK7k1Dkrsvl1DG8iTRRpaENPZtKP7vi9y06+5N2rKwlyUPwdKZI+mc4cwToJW/k/PSaMgBrfVnTQYR7KQuIYcbpMroBTFEUBFjiYyQPwFqbRhZGu3Dpyn+IMGeOHHSJAKZFSXSnMnAAdHJffNdk4s3zQ5yOGuy6H+/Sl3+f0GukYI+rNNYEeSgJJNS1afg3qIINcZ7jHFDmj20EQhH3WOWsiLPWFnIda1UBAoLUu8+cYt+G5Sj82wwHon1cDKgYVqOR8X48iIbZ5N0XE4uhhcpjfF8MoYIfCOE0eyMUuSS3cD32LQpyZMiwcZBDmtUnj4o8Y34QYhP4E8ocaDyjVBFfCCMRyO8QIGhL/qlu7qg2O89UxVKvSURlPrr/k9QIYvtb68iorWnyds2AqbKKmTqanv99gmD3uX+eRL2hMCaTWVqzYeGvhIifC3NjjTknbcRhENQanIe8m9PQLrp/g/3WcyvH2b38yVE0KCqw9IwTVNoozeQ+dX0l9tFD5Qm3e/sDfLmdrF8mM3vl/c95PJ69iPm/RM7zN2sf152eZ5ko0mSXQ6T88EY2lUEssJN2HOoxZbc2U5Ja8nFqsibNBkm2QAOoFmt1MwoKfaQw916anjmyJNmeG1IVcQsbNxdJQJrHD/r9SrfzDiGPOxHsDWe+9/HOZxhI4x60WAVgSNvaico9FPwkUTtJO+vjWZ65K4P0WIhlWRJz01XlpB/g+nt8uHq5svdFFZtG5T6lI3ZT7cx++/YmH1gY/Y3G7NTHP+ajatALk1XqtD76SEau8CLQ1LHwkmWAhUcPcXvvWDl+42giRNpm2Ei7cPauO/oyr780K66C/fjYtpLZIiAjSL3MnlDYKzXJBhymJqF2FJZqzAwdhQ86O7ojKIkRLXTxORDflXomVwYmDZwdaPm9lH6bgT8M8pDVsZWoaaTzM8c1wfVrsrSaH+v1f54wSqEaW1LZFqwQ6bNPsgaIlnqzddu43nIPH7V2KBUWCiCPAuDZG+DavM32C6cGbnuTBe1c6R5WlcFuZeHlpCnEZTkpaPy2Jbu1r5I748szwnLPeRp2/4ZAAD//7ddCeFZCQAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Service
      objectset.rio.cattle.io/owner-name: litellm
      objectset.rio.cattle.io/owner-namespace: ossa-agents
    creationTimestamp: "2025-10-10T04:25:36Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: ae21376cf483e40d76b795171807d85450e7a287
      svccontroller.k3s.cattle.io/nodeselector: "false"
      svccontroller.k3s.cattle.io/svcname: litellm
      svccontroller.k3s.cattle.io/svcnamespace: ossa-agents
    name: svclb-litellm-66a5aca1
    namespace: kube-system
    resourceVersion: "7642999"
    uid: 5df05f0c-cce5-49e4-9dc2-74759520e9ec
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: svclb-litellm-66a5aca1
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: svclb-litellm-66a5aca1
          svccontroller.k3s.cattle.io/svcname: litellm
          svccontroller.k3s.cattle.io/svcnamespace: ossa-agents
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: SRC_PORT
            value: "4000"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "4000"
          - name: DEST_IPS
            value: 192.168.194.237
          image: rancher/klipper-lb:v0.4.13
          imagePullPolicy: IfNotPresent
          name: lb-tcp-4000
          ports:
          - containerPort: 4000
            hostPort: 4000
            name: lb-tcp-4000
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - env:
          - name: SRC_PORT
            value: "4001"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "4001"
          - name: DEST_IPS
            value: 192.168.194.237
          image: rancher/klipper-lb:v0.4.13
          imagePullPolicy: IfNotPresent
          name: lb-tcp-4001
          ports:
          - containerPort: 4001
            hostPort: 4001
            name: lb-tcp-4001
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          sysctls:
          - name: net.ipv4.ip_forward
            value: "1"
        serviceAccount: svclb
        serviceAccountName: svclb
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/7xUTW/jNhD9K8WcJVnyR9cW0EOQBEXQrmPY3l4WRjCixjFriiTIkRIj0H8vKDup0nU+0MPCB0OcN4/DN2/mCdDKv8h5aTTkgNb6QZNBBHupS8jhCqkyekUMEVTEWCIj5E+AWhtGlkb78GmKv0mwJ06cNIlAZkWJNAMZOCB6M24eNLn4vtlDDoMmi375Q+rytxW5Rgr6ME9jRZBDpbbKPHwK7S2KkIIyrhS0EQhH3SvWsiLPWFnIda1UBAoLUu++bYd+BzlkX2g6JBrirNxm43Q0GhWT6QRFQSS+pFuRUjFLJ9MRROAbIYxmZ5Qil+xHvsemTUmeFAk2DnLYovL0QYpvxH8V+AT8BwlOHL4RqoiPTHGajSeTYjaBY/g5Z18XFPuDZ6pCprckgkL/1v0EFbLY/fkiHlr7JnfbRsBUWYVMXWrPX5/oy3vUP0G43vuxZlOZWvPJuBdChK+12ZOGvGtlBIEepSbnIf/+BKSb7v9UyGp5ebe4Xa4hggZVHY4maZpCG72CLC/mv1+veqA06X6DV8ir69X6brG8Xd/2kOvLxY+Y92/sMDeL/n3ZbJhkv06TbDZOhsMptJsIZIX3IeZQix25wV5Ja8nFqsibNBknWfB+B1rUSi2MkuIAOdxs54YXjjxphhcbqiJmYeOulAiscXzU60W+hXEMeYhHsDOe+9/nOZxhI4x61mATgSNvaico+Cj0kUTtJB8ujWZ65M5/aLGQSrKko9nKEvLvML9e311cfb2Zw6ZtA5N10nSpCr2fnyapG5A4THQsnGQpUMHZW/zBC1a+bwRNnEjbjBNp77bGPaAr+/JDu+kK7vts3htgiICNIve8moPTtlsSDDnMzUrsqKxVWCx7Cj3oanRGURJG22li8sHyFXomF1aqDVzdSrp+lJ5954//Q3kar9gq1PQm85Hj8qTaRVka7W+1OpxP2IQprG2JTCt2yHR/CLKGKZb6/lsXOC6lx28aG5QKC0WQZ2HzHGxQbfkK2001I9dd00XtHGme11VB7vmhJeRpBCV56ag8F9Ld2Vfp/ZnjJWF5gDxt238CAAD//wC1uH96BwAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Service
      objectset.rio.cattle.io/owner-name: mlflow
      objectset.rio.cattle.io/owner-namespace: ai-ml
    creationTimestamp: "2025-10-15T14:26:35Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 17e82ee2a9df140333b585acbeec70fc0eb90583
      svccontroller.k3s.cattle.io/nodeselector: "false"
      svccontroller.k3s.cattle.io/svcname: mlflow
      svccontroller.k3s.cattle.io/svcnamespace: ai-ml
    name: svclb-mlflow-01455b95
    namespace: kube-system
    resourceVersion: "7642826"
    uid: 6f0a1430-6027-47f9-b9d3-0c9804b45059
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: svclb-mlflow-01455b95
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: svclb-mlflow-01455b95
          svccontroller.k3s.cattle.io/svcname: mlflow
          svccontroller.k3s.cattle.io/svcnamespace: ai-ml
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: SRC_PORT
            value: "5000"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "5000"
          - name: DEST_IPS
            value: 192.168.194.228
          image: rancher/klipper-lb:v0.4.13
          imagePullPolicy: IfNotPresent
          name: lb-tcp-5000
          ports:
          - containerPort: 5000
            hostPort: 5000
            name: lb-tcp-5000
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          sysctls:
          - name: net.ipv4.ip_forward
            value: "1"
        serviceAccount: svclb
        serviceAccountName: svclb
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/8xW328iNxD+V6p5XjYQCIGV+hAlURW1RxBwfTmhaNaeBBevbdmze0HR/u+VHZLu3ZEfqtRcxQOyZ+bz+PtG+/kB0Kk/yQdlDRSAzoWjZgAZbJWRUMAFUmXNkhgyqIhRIiMUD4DGWEZW1oS4tOVfJDgQ517ZXCCzplzZIxUxIHsxbr8a8r27ZgsFHDWD7JfflZG/Lsk3StCbdQYrggLcxpJR9+9KDw5Ft6bNQHhKF1mpigJj5aAwtdYZaCxJv3q9DYYNFEAoqV+KkTwe9U/6NMXxaHoyPD6RYnxyPBmeTMuhLFGOIYPQCGENe6s1+Xw7DB00YyUF0iTYeijgFnWgN0pCI34g4R35B1jYw4RG6LK33+8NTyfidJiU6JZt65J6YReYqlgaHInI0j+9P0CFLDZ/PBOIzr0M3rYZMFVOI1Oq7czZO8R5Fftj6OuQgDXbytaG90N8JkRcreyWDBRJ0wziAagM+QDFlwcg06T/fS/LxfnN/Hqxggwa1HXcGvf7Y2izb1IWZ7PfLpedpH6efkf9bubF5XJ1M19cr647mavz+Y85r5+Ycq7m3fMG0+N8MJ7kg+koH5xOoV1noCq8izGPRmzIH221co58T5dF089H+WAI+6R5rfXcaiV2UMDV7czy3FMgw/A8jLrssXC91EoGznp+5OuZvrn1DEWMZ7CxgbvrwxjeshVWP3GwzsBTsLUXFGcp6kii9op359Yw3XOaQXRYKq1Y0ePASQnFF5hdrm7OLj5dzWDdtpGpt2UcDQenHyvj9yf+RBlTK6/IGONdGR/XhzF+uoyTD5dx8v+RcfKGjJPvZJy8hPGfybiO4MqmUo0hzPbuljyrF422J7xiJVDDwVPCLgjWoTsIhjhXrhnlyt3cWv8VvezSD+06Ndz96s86pgoZsNXknx5N8bt/e0uCoYCZXYoNyVpHw9pS1CD16K2mPLqtN8QUogVVGJh8fOu4iJVeCpf3KnBI8/FvIPd213MaDb2I/IhxvmftTEprwrXRu8MF6+iJtZPItGSPTHe7SGt0VWXuPqfA4zvh/rPBBpXGUhMUg/gW2LnI2uKb3OSxjFwn0UXtPRme1VVJ/umiEop+BpKC8iQPhUza+6RCOLC9IJQ7KPpt+3cAAAD//3pitr8UCwAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Service
      objectset.rio.cattle.io/owner-name: phoenix
      objectset.rio.cattle.io/owner-namespace: phoenix
    creationTimestamp: "2025-10-09T20:34:12Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: eade0bc4d24050e9a6495325dc6528359b3dbad6
      svccontroller.k3s.cattle.io/nodeselector: "false"
      svccontroller.k3s.cattle.io/svcname: phoenix
      svccontroller.k3s.cattle.io/svcnamespace: phoenix
    name: svclb-phoenix-378c73ce
    namespace: kube-system
    resourceVersion: "7642996"
    uid: f1839462-abed-4f3b-9d0c-472883b776ea
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: svclb-phoenix-378c73ce
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: svclb-phoenix-378c73ce
          svccontroller.k3s.cattle.io/svcname: phoenix
          svccontroller.k3s.cattle.io/svcnamespace: phoenix
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: SRC_PORT
            value: "6006"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "6006"
          - name: DEST_IPS
            value: 192.168.194.179
          image: rancher/klipper-lb:v0.4.13
          imagePullPolicy: IfNotPresent
          name: lb-tcp-6006
          ports:
          - containerPort: 6006
            hostPort: 6006
            name: lb-tcp-6006
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - env:
          - name: SRC_PORT
            value: "4317"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "4317"
          - name: DEST_IPS
            value: 192.168.194.179
          image: rancher/klipper-lb:v0.4.13
          imagePullPolicy: IfNotPresent
          name: lb-tcp-4317
          ports:
          - containerPort: 4317
            hostPort: 4317
            name: lb-tcp-4317
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - env:
          - name: SRC_PORT
            value: "4318"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "4318"
          - name: DEST_IPS
            value: 192.168.194.179
          image: rancher/klipper-lb:v0.4.13
          imagePullPolicy: IfNotPresent
          name: lb-tcp-4318
          ports:
          - containerPort: 4318
            hostPort: 4318
            name: lb-tcp-4318
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          sysctls:
          - name: net.ipv4.ip_forward
            value: "1"
        serviceAccount: svclb
        serviceAccountName: svclb
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/8RUQW/jNhP9Kx/mLMmSo/iLBfQQJEERtOsYtreXhRGMqLHNmiIJcqSNEei/F5SdVME62bSXQgeBnDdvyMc38wxo5R/kvDQaCkBr/ajNIIK91BUUcItUG70khghqYqyQEYpnQK0NI0ujfVia8k8S7IkTJ00ikFlRIs1IBg6I3o2b75pcvG33UMCozaL//SZ19cuSXCsF/TRPY01QgHWmJt5R42P/DzK9RRHSjfcY45Y0e+giEI76e61kTZ6xtlDoRqkIFJakPrztDv0OCrik8fjicjOZpvlVNhHjdFzmY1GVOM3GlxPMJ9NLolSkEIFvhTCanVGKXLK/8AM2bSrypEiwcVDABpWnn6T4VnykySdS3xHlxOpbocr4R+44z7L/59OrCo7QF5Z9U1LsD56pDizekgj6/X2rZ6iRxe73V2nR2k/V6boImGqrkKmnGXjzEy/42TL/idwDpbBhU5tG86klroUIq5XZk4ait0QEoQhKTc5D8e0ZSLf9/3S05eLmcf6wWEEELaombE3TaQpd9AayuJ79erccgNKk/0ZvkLd3y9XjfPGwehggVzfzHzEfV+wx9/NhvWw6TrLJVZJN8yTLJ9CtI5A1bkPMoRY7cqO9ktaSi1VZtGmSJ9kFnEDzRqm5UVIcoID7zczw3JEnzfBqXlXGLGzcHyUCaxwf9XqVb24cQxHiEeyM5+H6PIczbIRRLxqsI3DkTeMEBZeFdyTROMmHG6OZnrh3J1ospZIs6WjFqoLiG8zuVo/Xt1/uZ7DuusBknTR9qkLvZ6f+61spDpMhFk6yFKjgbBV/8IKVHxpBEyfStnki7ePGuO/oqqH80K37Aw99Nhu0PUTARpF7GfrBaZsNCYYCZmYpdlQ1Kth+T+EN+jM6oygJQ8BpYvLB+DV6JhcGtA1c/Wi7e5K+nzP/jvLUZLFVqOld5iPHzUm166oy2j9odTifsA5d2NgKmZbskGl7CLKGXpZ6+7UPHMfX01eNLUqFpSIosjCXDjaotniD7buakZv+0UXjHGmeNXVJ7uWiFRRpBBV56ag6F9L93hfp/ZntBWF1gCLtur8CAAD//+1CoiPUBwAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Service
      objectset.rio.cattle.io/owner-name: prometheus-service
      objectset.rio.cattle.io/owner-namespace: ossa-agents
    creationTimestamp: "2025-11-04T02:05:05Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 5e2235f6904816c202b42cdba91256a4695ee0c0
      svccontroller.k3s.cattle.io/nodeselector: "false"
      svccontroller.k3s.cattle.io/svcname: prometheus-service
      svccontroller.k3s.cattle.io/svcnamespace: ossa-agents
    name: svclb-prometheus-service-4117498d
    namespace: kube-system
    resourceVersion: "7642990"
    uid: 5d8d5bd2-9d04-4351-b1ad-bdbf4aa3fb18
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: svclb-prometheus-service-4117498d
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: svclb-prometheus-service-4117498d
          svccontroller.k3s.cattle.io/svcname: prometheus-service
          svccontroller.k3s.cattle.io/svcnamespace: ossa-agents
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: SRC_PORT
            value: "9090"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "9090"
          - name: DEST_IPS
            value: 192.168.194.146
          image: rancher/klipper-lb:v0.4.13
          imagePullPolicy: IfNotPresent
          name: lb-tcp-9090
          ports:
          - containerPort: 9090
            hostPort: 9090
            name: lb-tcp-9090
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          sysctls:
          - name: net.ipv4.ip_forward
            value: "1"
        serviceAccount: svclb
        serviceAccountName: svclb
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/8xVUW/jNgz+KwOfbTeJvVtiYA9FWwzFdmmQ5PZyCApaphstsqRJtK9B4f8+yEk7d0uu3YAdhjwEFsmP0vcRH58ArfyVnJdGQw5orb9oxxDBTuoScrhGqo1eEUMENTGWyAj5E6DWhpGl0T58muI3EuyJEydNIpBZUSLNhQwYEJ2Nmy+aXPzQ7iCHi3Ycffez1OWPK3KtFPRmncaaIIffS4eaY/8PqrxFEUqN9xjjA2n20EUgHPVvWsuaPGNtIdeNUhEoLEh99aVb9FvIYTab4WwkJtMUZ9VoOk3FJC2zjLCYjLIP1Q+UjqZVVk4hAt8KYTQ7oxS5ZJf6AZo2JXlSJNg4yKFC5emNEt+Kc3y8o+wMIUdE3wpVxK9xY5pU1fckUjikPSPsmoJiv/dMdUDwlkTg7c/XPEGNLLa/vFCK1r7Zo+siYKqtQqYeYjCL71DtPS2+Ob0DdrBhU5tG83H0L4UIX2uzIw15L38EoQlKTc5D/vkJSLf9//Faq+XV/eJuuYYIWlRNOPqQpil00auU5eX8p5vVIGmU9L+L0TDz+ma1vl8s79Z3g8z11eLvOV/v2OfcLob9xrNJMv4wTcazLJmMU+g2EcgaH0LMoRZbchc7Ja0lF6sib0dJloyDOn3SolFqYZQUe8jhtpobXjjypBlehlUVMQsb91eJwBrHB75e6FsYx5CHeARb43n4fRrDGTbCqGcONhE48qZxgsJ0BR1JNE7y/spopkfupxItFlJJlnQYwbKE/DPMb9b3l9cfb+ew6brA1LtkzL65jNn/R8bsDRmzv8iYncP4z2TcBHBp+lKF3s+Pttm7YBzMPBZOshSo4GQXv/eClR8OgiZOpG2zRNr7yrgv6Moh/dBt+gsP7WI+cGuIgI0i97yjg2FUFQmGHOZmJbZUNiq4146CBv0dnVGUBP92mph88K8aPZMLO9UGrH4b3TxK36+Hfwd59MrYKtR0FvmAcXVk7bIsjfZ3Wu1PF2yCmTa2RKYVO2R62AdagyVL/fCpDxw2z+MnjS1KhYUiyMdhrextYG35Krc3Z0ZuetFF4xxpnjd1Qe75oSXkowhK8tJReSqk+7OP0vsTx0vCcg/5qOv+CAAA//+6+bsogwkAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Service
      objectset.rio.cattle.io/owner-name: qdrant-service
      objectset.rio.cattle.io/owner-namespace: ossa-agents
    creationTimestamp: "2025-09-09T21:18:20Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 999a90c283a9f0883c23d44eab2046f7e308f4d8
      svccontroller.k3s.cattle.io/nodeselector: "false"
      svccontroller.k3s.cattle.io/svcname: qdrant-service
      svccontroller.k3s.cattle.io/svcnamespace: ossa-agents
    name: svclb-qdrant-service-e2ff5ec3
    namespace: kube-system
    resourceVersion: "7642951"
    uid: 00b568f3-00ba-4a9d-b578-afcde76a380c
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: svclb-qdrant-service-e2ff5ec3
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: svclb-qdrant-service-e2ff5ec3
          svccontroller.k3s.cattle.io/svcname: qdrant-service
          svccontroller.k3s.cattle.io/svcnamespace: ossa-agents
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: SRC_PORT
            value: "6333"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "6333"
          - name: DEST_IPS
            value: 192.168.194.213
          image: rancher/klipper-lb:v0.4.13
          imagePullPolicy: IfNotPresent
          name: lb-tcp-6333
          ports:
          - containerPort: 6333
            hostPort: 6333
            name: lb-tcp-6333
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - env:
          - name: SRC_PORT
            value: "6334"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "6334"
          - name: DEST_IPS
            value: 192.168.194.213
          image: rancher/klipper-lb:v0.4.13
          imagePullPolicy: IfNotPresent
          name: lb-tcp-6334
          ports:
          - containerPort: 6334
            hostPort: 6334
            name: lb-tcp-6334
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          sysctls:
          - name: net.ipv4.ip_forward
            value: "1"
        serviceAccount: svclb
        serviceAccountName: svclb
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/7xUwW7jNhD9lWLOkmLZcpwV0EOQBEXQrmPY3l4WRjCiRjFriiTIkTZGoH8vKNupFvFmgx4WPhgi3zwOH9+bF0Ar/ybnpdGQA1rrL9oUIthJXUIOt0i10StiiKAmxhIZIX8B1NowsjTah09T/EOCPXHipEkEMitKpLmQgQOiH+6bb5pc/NTuIIeLNo1++1Pq8vcVuVYK+mmdxpoghxYbxXEjP4T3FsVrEXQRCEf9PdayJs9YW8h1o1QECgtS795ui34LOUyrWZZdzkQqZmNRTcciHWfpCKd4NZmKCqvLy1lKRDOIwLdCGM3OKEUu2U38gE2bkjwpEmwc5FCh8vSTEt+Ktxp8oOCNCEcW3wpVxCeuuCzLUXU1RjgATlW7pqDY7z1THWq9JRFU+q/3F6iRxfavVwHR2nfYuy4CptoqZOqLBz77wOu8T/5LBByogA2b2jSajya+FiJ8rc2ONOT9o0YQ6FFqch7yry9Auu3/j62sljePi4flGiJoUTVhKcsm0EXfIZbX8z/uVgPMKOl/F6Mh8vZutX5cLB/WDwPk+mbxFvPugT3kfjE8Lv00TtLLqyT9lCXjbArdJgJZ41PYc6jFltzFTklrycWqyNtRkiXpBI6gRaPUwigp9pDDfTU3vHDkSTO8mlEVMQsbh04isMbxQaxX7RbGMeRZNolgazwPPs8SOMNGGHW6/iYCR940TlBwUXhBEo2TvL8xmumZe/ehxUIqyZIOVitLyL/C/G79eH37+X4Om64LTNZJ05cq9H5+zFIfkDikOhZOshSo4Owpfu8FKz+0gCZOpG2zRNrHyrhv6Mqh9NBt+oaHDpsPIgwRsFHkTgM6eKyqSDDkMDcrsaWyUWG47Cjo3/fojKIkRNtpYvLB7DV6JhfGqg1c/Vi6e5aefe+N/0N5DFZsFWr6IfOB4+ao2nVZGu0ftNqfL9iE/DW2RKYVO2R62gdZQ36lfvrSbxyG0vMXjS1KhYUiyNMwd/Y2qLb8DtvnmZGb/tFF4xxpnjd1Qe500RLyUQQleemoPLel+7XP0vszy0vCcg/5qOv+DQAA///X4g6SgAcAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Service
      objectset.rio.cattle.io/owner-name: vault-ui
      objectset.rio.cattle.io/owner-namespace: vault
    creationTimestamp: "2025-10-08T13:16:41Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 5f74467c1c72cf52c12410a5a835cfaf6671eee7
      svccontroller.k3s.cattle.io/nodeselector: "false"
      svccontroller.k3s.cattle.io/svcname: vault-ui
      svccontroller.k3s.cattle.io/svcnamespace: vault
    name: svclb-vault-ui-ddd0f82a
    namespace: kube-system
    resourceVersion: "7482324"
    uid: 86c325f2-0818-439e-aa88-e297113ca88c
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: svclb-vault-ui-ddd0f82a
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: svclb-vault-ui-ddd0f82a
          svccontroller.k3s.cattle.io/svcname: vault-ui
          svccontroller.k3s.cattle.io/svcnamespace: vault
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: SRC_PORT
            value: "443"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "443"
          - name: DEST_IPS
            value: 192.168.194.245
          image: rancher/klipper-lb:v0.4.13
          imagePullPolicy: IfNotPresent
          name: lb-tcp-443
          ports:
          - containerPort: 443
            hostPort: 443
            name: lb-tcp-443
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          sysctls:
          - name: net.ipv4.ip_forward
            value: "1"
        serviceAccount: svclb
        serviceAccountName: svclb
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberMisscheduled: 0
    numberReady: 0
    numberUnavailable: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: promtail
      meta.helm.sh/release-namespace: ossa-agents
    creationTimestamp: "2025-10-30T15:36:06Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: promtail
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: promtail
      app.kubernetes.io/version: 3.5.1
      helm.sh/chart: promtail-6.17.0
    name: promtail
    namespace: ossa-agents
    resourceVersion: "7643139"
    uid: 846ba7bd-2a5a-499b-8f71-f582b769b0ca
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: promtail
        app.kubernetes.io/name: promtail
    template:
      metadata:
        annotations:
          checksum/config: ab1cb3f63f759e3db8978c1acf24413428e072965c6d0eb46186c5a4e9c7de87
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: promtail
          app.kubernetes.io/name: promtail
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - -config.file=/etc/promtail/promtail.yaml
          env:
          - name: HOSTNAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: docker.io/grafana/promtail:3.5.1
          imagePullPolicy: IfNotPresent
          name: promtail
          ports:
          - containerPort: 3101
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/promtail
            name: config
          - mountPath: /run/promtail
            name: run
          - mountPath: /var/lib/docker/containers
            name: containers
            readOnly: true
          - mountPath: /var/log/pods
            name: pods
            readOnly: true
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 0
          runAsUser: 0
        serviceAccount: promtail
        serviceAccountName: promtail
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: promtail
        - hostPath:
            path: /run/promtail
            type: ""
          name: run
        - hostPath:
            path: /var/lib/docker/containers
            type: ""
          name: containers
        - hostPath:
            path: /var/log/pods
            type: ""
          name: pods
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"frontend"},"name":"drupal-frontend","namespace":"agent-chat"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"frontend"}},"template":{"metadata":{"labels":{"app":"frontend"}},"spec":{"containers":[{"env":[{"name":"DRUPAL_DATABASE_HOST","value":"postgres.agent-chat.local.bluefly.io"},{"name":"DRUPAL_DATABASE_PORT","value":"5432"},{"name":"DRUPAL_DATABASE_NAME","value":"llm_platform"},{"name":"DRUPAL_DATABASE_USERNAME","value":"llm_user"},{"name":"DRUPAL_DATABASE_PASSWORD","value":"llm_password"}],"image":"drupal:10-apache","livenessProbe":{"httpGet":{"path":"/","port":80},"initialDelaySeconds":60,"periodSeconds":10},"name":"drupal","ports":[{"containerPort":80,"name":"http"}],"readinessProbe":{"httpGet":{"path":"/","port":80},"initialDelaySeconds":30,"periodSeconds":5},"resources":{"limits":{"cpu":"500m","memory":"1Gi"},"requests":{"cpu":"250m","memory":"512Mi"}},"volumeMounts":[{"mountPath":"/var/www/html/sites/default/files","name":"drupal-files"}]}],"volumes":[{"emptyDir":{},"name":"drupal-files"}]}}}}
    creationTimestamp: "2025-10-07T15:48:07Z"
    generation: 1
    labels:
      app: frontend
    name: drupal-frontend
    namespace: agent-chat
    resourceVersion: "7643567"
    uid: ba12b426-1941-4a4b-92c9-f6f1217d96c1
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: frontend
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: frontend
      spec:
        containers:
        - env:
          - name: DRUPAL_DATABASE_HOST
            value: postgres.agent-chat.local.bluefly.io
          - name: DRUPAL_DATABASE_PORT
            value: "5432"
          - name: DRUPAL_DATABASE_NAME
            value: llm_platform
          - name: DRUPAL_DATABASE_USERNAME
            value: llm_user
          - name: DRUPAL_DATABASE_PASSWORD
            value: llm_password
          image: drupal:10-apache
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 80
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: drupal
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 80
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/www/html/sites/default/files
            name: drupal-files
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: drupal-files
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-10-07T15:48:07Z"
      lastUpdateTime: "2025-10-07T15:49:12Z"
      message: ReplicaSet "drupal-frontend-7ccd54b7ff" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-18T01:37:54Z"
      lastUpdateTime: "2025-11-18T01:37:54Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "7"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"librechat","namespace":"agent-chat"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"librechat"}},"template":{"metadata":{"labels":{"app":"librechat"}},"spec":{"containers":[{"envFrom":[{"configMapRef":{"name":"librechat-config"}}],"image":"ghcr.io/danny-avila/librechat:latest","name":"librechat","ports":[{"containerPort":3080}],"volumeMounts":[{"mountPath":"/app/uploads","name":"librechat-storage"}]}],"volumes":[{"emptyDir":{},"name":"librechat-storage"}]}}}}
    creationTimestamp: "2025-10-04T15:48:21Z"
    generation: 8
    name: librechat
    namespace: agent-chat
    resourceVersion: "7560440"
    uid: 6dc728b1-0d13-45c6-a695-99dfeb74ae2f
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: librechat
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-11-16T07:14:30-05:00"
        creationTimestamp: null
        labels:
          app: librechat
      spec:
        containers:
        - env:
          - name: PHOENIX_COLLECTOR_ENDPOINT
            value: http://phoenix.observability.svc.cluster.local:4318/v1/traces
          - name: NODE_ENV
            value: development
          envFrom:
          - configMapRef:
              name: librechat-config
          image: ghcr.io/danny-avila/librechat:latest
          imagePullPolicy: Always
          name: librechat
          ports:
          - containerPort: 3080
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/uploads
            name: librechat-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: librechat-storage
  status:
    conditions:
    - lastTransitionTime: "2025-11-11T03:47:18Z"
      lastUpdateTime: "2025-11-16T12:14:35Z"
      message: ReplicaSet "librechat-6b6667b7d4" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-17T04:43:55Z"
      lastUpdateTime: "2025-11-17T04:43:55Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 8
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"mongo","namespace":"agent-chat"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"mongo"}},"template":{"metadata":{"labels":{"app":"mongo"}},"spec":{"containers":[{"image":"mongo:6","name":"mongo","ports":[{"containerPort":27017}],"volumeMounts":[{"mountPath":"/data/db","name":"mongo-storage"}]}],"volumes":[{"emptyDir":{},"name":"mongo-storage"}]}}}}
    creationTimestamp: "2025-10-04T16:01:43Z"
    generation: 3
    name: mongo
    namespace: agent-chat
    resourceVersion: "7560432"
    uid: bcc612de-e55b-4894-a512-8b8a09596de7
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: mongo
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: mongo
      spec:
        containers:
        - image: mongo:6
          imagePullPolicy: IfNotPresent
          name: mongo
          ports:
          - containerPort: 27017
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data/db
            name: mongo-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: mongo-storage
  status:
    conditions:
    - lastTransitionTime: "2025-11-11T03:47:15Z"
      lastUpdateTime: "2025-11-11T03:47:16Z"
      message: ReplicaSet "mongo-74c6b77b75" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-17T04:43:54Z"
      lastUpdateTime: "2025-11-17T04:43:54Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"postgres","namespace":"agent-chat"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"postgres"}},"template":{"metadata":{"labels":{"app":"postgres"}},"spec":{"containers":[{"envFrom":[{"configMapRef":{"name":"postgres-config"}},{"secretRef":{"name":"postgres-secret"}}],"image":"postgres:14-alpine","name":"postgres","ports":[{"containerPort":5432}],"volumeMounts":[{"mountPath":"/var/lib/postgresql/data","name":"postgres-storage"}]}],"volumes":[{"emptyDir":{},"name":"postgres-storage"}]}}}}
    creationTimestamp: "2025-10-04T15:48:20Z"
    generation: 3
    name: postgres
    namespace: agent-chat
    resourceVersion: "7560436"
    uid: 3d5c0ad0-818e-481e-b868-1c32f3769ffd
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: postgres
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: postgres
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: postgres-config
          - secretRef:
              name: postgres-secret
          image: postgres:14-alpine
          imagePullPolicy: IfNotPresent
          name: postgres
          ports:
          - containerPort: 5432
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/postgresql/data
            name: postgres-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: postgres-storage
  status:
    conditions:
    - lastTransitionTime: "2025-11-11T03:47:17Z"
      lastUpdateTime: "2025-11-11T03:47:17Z"
      message: ReplicaSet "postgres-6dc6d9858f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-17T04:43:53Z"
      lastUpdateTime: "2025-11-17T04:43:53Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"redis","namespace":"agent-chat"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"redis"}},"template":{"metadata":{"labels":{"app":"redis"}},"spec":{"containers":[{"image":"redis:7-alpine","name":"redis","ports":[{"containerPort":6379}],"volumeMounts":[{"mountPath":"/data","name":"redis-storage"}]}],"volumes":[{"emptyDir":{},"name":"redis-storage"}]}}}}
    creationTimestamp: "2025-10-04T15:48:21Z"
    generation: 3
    name: redis
    namespace: agent-chat
    resourceVersion: "7560438"
    uid: 5a71615b-e260-4a9d-8ab4-9d1ea3a2cbaf
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: redis
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: redis
      spec:
        containers:
        - image: redis:7-alpine
          imagePullPolicy: IfNotPresent
          name: redis
          ports:
          - containerPort: 6379
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: redis-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: redis-storage
  status:
    conditions:
    - lastTransitionTime: "2025-11-11T03:47:16Z"
      lastUpdateTime: "2025-11-11T03:47:16Z"
      message: ReplicaSet "redis-844949cd94" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-17T04:43:53Z"
      lastUpdateTime: "2025-11-17T04:43:53Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"gateway","component":"api"},"name":"gateway","namespace":"agent-studio"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"gateway"}},"template":{"metadata":{"labels":{"app":"gateway"}},"spec":{"containers":[{"image":"nginx:alpine","name":"gateway","ports":[{"containerPort":80}],"resources":{"limits":{"cpu":"200m","memory":"256Mi"},"requests":{"cpu":"100m","memory":"128Mi"}}}]}}}}
    creationTimestamp: "2025-10-22T05:43:36Z"
    generation: 5
    labels:
      app: gateway
      component: api
    name: gateway
    namespace: agent-studio
    resourceVersion: "7642958"
    uid: c0acd696-5826-42e4-9378-44355e550b07
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: gateway
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-31T07:14:10-04:00"
        creationTimestamp: null
        labels:
          app: gateway
      spec:
        containers:
        - env:
          - name: PHOENIX_COLLECTOR_ENDPOINT
            value: http://phoenix.observability.svc.cluster.local:4318/v1/traces
          - name: NODE_ENV
            value: development
          image: nginx:alpine
          imagePullPolicy: IfNotPresent
          name: gateway
          ports:
          - containerPort: 80
            protocol: TCP
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-10-22T05:43:36Z"
      lastUpdateTime: "2025-10-31T11:15:51Z"
      message: ReplicaSet "gateway-c7c6db957" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-18T01:37:22Z"
      lastUpdateTime: "2025-11-18T01:37:22Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 5
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"aiflow-social-agent"},"name":"aiflow-social-agent","namespace":"agents-staging"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"aiflow-social-agent"}},"template":{"metadata":{"labels":{"app":"aiflow-social-agent"}},"spec":{"containers":[{"command":["sh","-c","while true; do echo 'Mock AIFlow agent running'; sleep 30; done"],"env":[{"name":"AGENT_ID","value":"social-agent-aiflow"}],"image":"python:3.11-slim","name":"aiflow-agent","ports":[{"containerPort":8000,"name":"http"}],"resources":{"limits":{"cpu":"200m","memory":"256Mi"},"requests":{"cpu":"50m","memory":"128Mi"}}}]}}}}
    creationTimestamp: "2025-10-23T11:46:09Z"
    generation: 2
    labels:
      app: aiflow-social-agent
    name: aiflow-social-agent
    namespace: agents-staging
    resourceVersion: "7642932"
    uid: e178ddb6-c308-4c1f-97bd-ded31875b7b6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: aiflow-social-agent
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: aiflow-social-agent
      spec:
        containers:
        - command:
          - sh
          - -c
          - while true; do echo 'Mock AIFlow agent running'; sleep 30; done
          env:
          - name: AGENT_ID
            value: social-agent-aiflow
          - name: PHOENIX_COLLECTOR_ENDPOINT
            value: http://phoenix.observability.svc.cluster.local:4318/v1/traces
          - name: NODE_ENV
            value: development
          image: python:3.11-slim
          imagePullPolicy: IfNotPresent
          name: aiflow-agent
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-10-23T11:46:09Z"
      lastUpdateTime: "2025-10-28T14:45:42Z"
      message: ReplicaSet "aiflow-social-agent-5fbbb8c656" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-18T01:37:22Z"
      lastUpdateTime: "2025-11-18T01:37:22Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"mlflow","component":"ml-tracking"},"name":"mlflow","namespace":"ai-ml"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"mlflow"}},"template":{"metadata":{"labels":{"app":"mlflow"}},"spec":{"containers":[{"env":[{"name":"MLFLOW_BACKEND_STORE_URI","value":"sqlite:///mlflow/mlflow.db"},{"name":"MLFLOW_DEFAULT_ARTIFACT_ROOT","value":"/mlflow/artifacts"},{"name":"MLFLOW_HOST","value":"0.0.0.0"},{"name":"MLFLOW_PORT","value":"5000"}],"image":"ghcr.io/mlflow/mlflow:v2.9.2","livenessProbe":{"httpGet":{"path":"/health","port":5000},"initialDelaySeconds":30,"periodSeconds":10},"name":"mlflow","ports":[{"containerPort":5000,"name":"http","protocol":"TCP"}],"readinessProbe":{"httpGet":{"path":"/health","port":5000},"initialDelaySeconds":15,"periodSeconds":5},"resources":{"limits":{"cpu":"500m","memory":"2Gi"},"requests":{"cpu":"250m","memory":"1Gi"}},"volumeMounts":[{"mountPath":"/mlflow","name":"mlflow-data"}]}],"volumes":[{"name":"mlflow-data","persistentVolumeClaim":{"claimName":"mlflow-data"}}]}}}}
    creationTimestamp: "2025-10-15T14:26:35Z"
    generation: 2
    labels:
      app: mlflow
      component: ml-tracking
    name: mlflow
    namespace: ai-ml
    resourceVersion: "6175819"
    uid: 61890152-b062-46a6-9442-3393bf68d672
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: mlflow
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: mlflow
      spec:
        containers:
        - env:
          - name: MLFLOW_BACKEND_STORE_URI
            value: sqlite:///mlflow/mlflow.db
          - name: MLFLOW_DEFAULT_ARTIFACT_ROOT
            value: /mlflow/artifacts
          - name: MLFLOW_HOST
            value: 0.0.0.0
          - name: MLFLOW_PORT
            value: "5000"
          image: ghcr.io/mlflow/mlflow:v2.9.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: mlflow
          ports:
          - containerPort: 5000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /mlflow
            name: mlflow-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: mlflow-data
          persistentVolumeClaim:
            claimName: mlflow-data
  status:
    conditions:
    - lastTransitionTime: "2025-11-04T03:28:46Z"
      lastUpdateTime: "2025-11-04T03:28:46Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-04T03:28:46Z"
      lastUpdateTime: "2025-11-04T03:28:46Z"
      message: ReplicaSet "mlflow-79cbb69488" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"apidog-runner"},"name":"apidog-runner","namespace":"apidog"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"apidog-runner"}},"template":{"metadata":{"labels":{"app":"apidog-runner"}},"spec":{"containers":[{"env":[{"name":"TZ","value":"America/New_York"},{"name":"SERVER_APP_BASE_URL","value":"https://api.apidog.com"},{"name":"TEAM_ID","valueFrom":{"secretKeyRef":{"key":"TEAM_ID","name":"apidog-runner-config"}}},{"name":"RUNNER_ID","valueFrom":{"secretKeyRef":{"key":"RUNNER_ID","name":"apidog-runner-config"}}},{"name":"ACCESS_TOKEN","valueFrom":{"secretKeyRef":{"key":"ACCESS_TOKEN","name":"apidog-runner-config"}}}],"image":"apidog/self-hosted-general-runner:latest","imagePullPolicy":"Always","livenessProbe":{"httpGet":{"path":"/","port":4524},"initialDelaySeconds":30,"periodSeconds":10},"name":"runner","ports":[{"containerPort":4524,"name":"http","protocol":"TCP"}],"readinessProbe":{"httpGet":{"path":"/","port":4524},"initialDelaySeconds":5,"periodSeconds":5},"resources":{"limits":{"cpu":"500m","memory":"512Mi"},"requests":{"cpu":"100m","memory":"256Mi"}}}]}}}}
    creationTimestamp: "2025-10-21T00:18:41Z"
    generation: 1
    labels:
      app: apidog-runner
    name: apidog-runner
    namespace: apidog
    resourceVersion: "7643093"
    uid: bf46971b-8349-4fab-b23f-3b53edcafecc
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: apidog-runner
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: apidog-runner
      spec:
        containers:
        - env:
          - name: TZ
            value: America/New_York
          - name: SERVER_APP_BASE_URL
            value: https://api.apidog.com
          - name: TEAM_ID
            valueFrom:
              secretKeyRef:
                key: TEAM_ID
                name: apidog-runner-config
          - name: RUNNER_ID
            valueFrom:
              secretKeyRef:
                key: RUNNER_ID
                name: apidog-runner-config
          - name: ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                key: ACCESS_TOKEN
                name: apidog-runner-config
          image: apidog/self-hosted-general-runner:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 4524
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: runner
          ports:
          - containerPort: 4524
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 4524
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-10-21T00:18:41Z"
      lastUpdateTime: "2025-10-21T00:20:40Z"
      message: ReplicaSet "apidog-runner-587c6b45b7" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-18T01:37:28Z"
      lastUpdateTime: "2025-11-18T01:37:28Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"argo-server","namespace":"argo"},"spec":{"selector":{"matchLabels":{"app":"argo-server"}},"template":{"metadata":{"labels":{"app":"argo-server"}},"spec":{"containers":[{"args":["server"],"env":[],"image":"quay.io/argoproj/argocli:v3.5.2","name":"argo-server","ports":[{"containerPort":2746,"name":"web"}],"readinessProbe":{"httpGet":{"path":"/","port":2746,"scheme":"HTTPS"},"initialDelaySeconds":10,"periodSeconds":20},"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"drop":["ALL"]},"readOnlyRootFilesystem":true,"runAsNonRoot":true},"volumeMounts":[{"mountPath":"/tmp","name":"tmp"}]}],"nodeSelector":{"kubernetes.io/os":"linux"},"securityContext":{"runAsNonRoot":true},"serviceAccountName":"argo-server","volumes":[{"emptyDir":{},"name":"tmp"}]}}}}
    creationTimestamp: "2025-11-13T19:40:07Z"
    generation: 2
    name: argo-server
    namespace: argo
    resourceVersion: "7643412"
    uid: d3f2e9d3-6a46-45a7-9ae0-836beb3eaaa3
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: argo-server
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: argo-server
      spec:
        containers:
        - args:
          - server
          image: quay.io/argoproj/argocli:v3.5.2
          imagePullPolicy: IfNotPresent
          name: argo-server
          ports:
          - containerPort: 2746
            name: web
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 2746
              scheme: HTTPS
            initialDelaySeconds: 10
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
        serviceAccount: argo-server
        serviceAccountName: argo-server
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-14T08:01:42Z"
      lastUpdateTime: "2025-11-14T08:01:42Z"
      message: ReplicaSet "argo-server-54467f6857" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-18T01:37:45Z"
      lastUpdateTime: "2025-11-18T01:37:45Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"workflow-controller","namespace":"argo"},"spec":{"selector":{"matchLabels":{"app":"workflow-controller"}},"template":{"metadata":{"labels":{"app":"workflow-controller"}},"spec":{"containers":[{"args":[],"command":["workflow-controller"],"env":[{"name":"LEADER_ELECTION_IDENTITY","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.name"}}}],"image":"quay.io/argoproj/workflow-controller:v3.5.2","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/healthz","port":6060},"initialDelaySeconds":90,"periodSeconds":60,"timeoutSeconds":30},"name":"workflow-controller","ports":[{"containerPort":9090,"name":"metrics"},{"containerPort":6060}],"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"drop":["ALL"]},"readOnlyRootFilesystem":true,"runAsNonRoot":true}}],"nodeSelector":{"kubernetes.io/os":"linux"},"priorityClassName":"workflow-controller","securityContext":{"runAsNonRoot":true},"serviceAccountName":"argo"}}}}
    creationTimestamp: "2025-11-13T19:40:07Z"
    generation: 2
    name: workflow-controller
    namespace: argo
    resourceVersion: "7642994"
    uid: df8dd0f5-dea4-4efa-b3e3-5aec5f9c562a
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: workflow-controller
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: workflow-controller
      spec:
        containers:
        - command:
          - workflow-controller
          env:
          - name: LEADER_ELECTION_IDENTITY
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: quay.io/argoproj/workflow-controller:v3.5.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6060
              scheme: HTTP
            initialDelaySeconds: 90
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: workflow-controller
          ports:
          - containerPort: 9090
            name: metrics
            protocol: TCP
          - containerPort: 6060
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: workflow-controller
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
        serviceAccount: argo
        serviceAccountName: argo
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-13T19:40:07Z"
      lastUpdateTime: "2025-11-13T23:00:08Z"
      message: ReplicaSet "workflow-controller-855c5bfc48" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-18T01:37:22Z"
      lastUpdateTime: "2025-11-18T01:37:22Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"agent-router","version":"v1"},"name":"agent-router","namespace":"default"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"agent-router"}},"strategy":{"rollingUpdate":{"maxSurge":1,"maxUnavailable":0},"type":"RollingUpdate"},"template":{"metadata":{"annotations":{"prometheus.io/path":"/metrics","prometheus.io/port":"8080","prometheus.io/scrape":"true"},"labels":{"app":"agent-router","version":"v1"}},"spec":{"affinity":{"podAntiAffinity":{"preferredDuringSchedulingIgnoredDuringExecution":[{"podAffinityTerm":{"labelSelector":{"matchExpressions":[{"key":"app","operator":"In","values":["agent-router"]}]},"topologyKey":"kubernetes.io/hostname"},"weight":100}]}},"containers":[{"env":[{"name":"RUNTIME","value":"kubernetes"},{"name":"MAX_CONCURRENT_AGENTS","value":"50"},{"name":"LOG_LEVEL","value":"info"},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}}],"image":"agent-buildkit/worker:latest","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/health","port":"http"},"initialDelaySeconds":30,"periodSeconds":10,"timeoutSeconds":5},"name":"router","ports":[{"containerPort":8080,"name":"http","protocol":"TCP"}],"readinessProbe":{"failureThreshold":2,"httpGet":{"path":"/ready","port":"http"},"initialDelaySeconds":10,"periodSeconds":5,"timeoutSeconds":3},"resources":{"limits":{"cpu":"1000m","memory":"2Gi"},"requests":{"cpu":"100m","memory":"512Mi"}},"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"drop":["ALL"]},"readOnlyRootFilesystem":true},"volumeMounts":[{"mountPath":"/tmp","name":"tmp"},{"mountPath":"/app/cache","name":"cache"}]}],"securityContext":{"fsGroup":1000,"runAsNonRoot":true,"runAsUser":1000},"serviceAccountName":"agent-router","volumes":[{"emptyDir":{},"name":"tmp"},{"emptyDir":{},"name":"cache"}]}}}}
    creationTimestamp: "2025-11-03T19:23:56Z"
    generation: 2
    labels:
      app: agent-router
      version: v1
    name: agent-router
    namespace: default
    resourceVersion: "6756369"
    uid: 6500c1ca-c99d-4294-87c6-92f1603aef9f
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: agent-router
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: agent-router
          version: v1
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - agent-router
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: RUNTIME
            value: kubernetes
          - name: MAX_CONCURRENT_AGENTS
            value: "50"
          - name: LOG_LEVEL
            value: info
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: agent-buildkit/worker:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: router
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 2
            httpGet:
              path: /ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/cache
            name: cache
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: agent-router
        serviceAccountName: agent-router
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: cache
  status:
    conditions:
    - lastTransitionTime: "2025-11-11T17:09:01Z"
      lastUpdateTime: "2025-11-11T17:09:01Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-11T17:09:01Z"
      lastUpdateTime: "2025-11-11T17:09:01Z"
      message: ReplicaSet "agent-router-c68d69f7f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"agent-tracer-api"},"name":"agent-tracer-api","namespace":"default"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"agent-tracer-api"}},"template":{"metadata":{"labels":{"app":"agent-tracer-api"}},"spec":{"containers":[{"args":["-c","echo 'server {\n  listen 80;\n  server_name _;\n  location /health {\n    add_header Content-Type application/json;\n    return 200 \"{\\\"status\\\":\\\"healthy\\\",\\\"service\\\":\\\"agent-tracer-api\\\"}\";\n  }\n  location /api/v1/traces {\n    add_header Content-Type application/json;\n    return 200 \"{\\\"traces\\\":[],\\\"message\\\":\\\"Agent Tracer API\\\"}\";\n  }\n  location / {\n    return 200 \"Agent Tracer API - Service Running\";\n  }\n}' \u003e /etc/nginx/conf.d/default.conf\nnginx -g \"daemon off;\"\n"],"command":["/bin/sh"],"image":"nginx:alpine","name":"agent-tracer-api","ports":[{"containerPort":80}],"resources":{"limits":{"cpu":"100m","memory":"128Mi"},"requests":{"cpu":"50m","memory":"64Mi"}}}]}}}}
    creationTimestamp: "2025-10-06T23:40:47Z"
    generation: 2
    labels:
      app: agent-tracer-api
    name: agent-tracer-api
    namespace: default
    resourceVersion: "5361748"
    uid: 5173bd3b-aaa0-4560-966d-842f4eb2dfeb
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: agent-tracer-api
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: agent-tracer-api
      spec:
        containers:
        - args:
          - -c
          - |
            echo 'server {
              listen 80;
              server_name _;
              location /health {
                add_header Content-Type application/json;
                return 200 "{\"status\":\"healthy\",\"service\":\"agent-tracer-api\"}";
              }
              location /api/v1/traces {
                add_header Content-Type application/json;
                return 200 "{\"traces\":[],\"message\":\"Agent Tracer API\"}";
              }
              location / {
                return 200 "Agent Tracer API - Service Running";
              }
            }' > /etc/nginx/conf.d/default.conf
            nginx -g "daemon off;"
          command:
          - /bin/sh
          image: nginx:alpine
          imagePullPolicy: IfNotPresent
          name: agent-tracer-api
          ports:
          - containerPort: 80
            protocol: TCP
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastTransitionTime: "2025-10-06T23:40:48Z"
      lastUpdateTime: "2025-10-06T23:40:52Z"
      message: ReplicaSet "agent-tracer-api-754f68bdd4" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-10-21T14:17:38Z"
      lastUpdateTime: "2025-10-21T14:17:38Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"agent-tracer-dashboard"},"name":"agent-tracer-dashboard","namespace":"default"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"agent-tracer-dashboard"}},"template":{"metadata":{"labels":{"app":"agent-tracer-dashboard"}},"spec":{"containers":[{"args":["-c","echo 'server {\n  listen 80;\n  server_name _;\n  location /health {\n    add_header Content-Type application/json;\n    return 200 \"{\\\"status\\\":\\\"healthy\\\",\\\"service\\\":\\\"agent-tracer-dashboard\\\"}\";\n  }\n  location / {\n    add_header Content-Type text/html;\n    return 200 \"\u003chtml\u003e\u003cbody\u003e\u003ch1\u003eAgent Tracer Dashboard\u003c/h1\u003e\u003cp\u003eService is running\u003c/p\u003e\u003cp\u003eStatus: Healthy\u003c/p\u003e\u003c/body\u003e\u003c/html\u003e\";\n  }\n}' \u003e /etc/nginx/conf.d/default.conf\nnginx -g \"daemon off;\"\n"],"command":["/bin/sh"],"image":"nginx:alpine","name":"agent-tracer-dashboard","ports":[{"containerPort":80}],"resources":{"limits":{"cpu":"100m","memory":"128Mi"},"requests":{"cpu":"50m","memory":"64Mi"}}}]}}}}
    creationTimestamp: "2025-10-06T23:40:48Z"
    generation: 2
    labels:
      app: agent-tracer-dashboard
    name: agent-tracer-dashboard
    namespace: default
    resourceVersion: "5361746"
    uid: f9b90c64-dd05-4e37-8a1f-342139149858
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: agent-tracer-dashboard
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: agent-tracer-dashboard
      spec:
        containers:
        - args:
          - -c
          - |
            echo 'server {
              listen 80;
              server_name _;
              location /health {
                add_header Content-Type application/json;
                return 200 "{\"status\":\"healthy\",\"service\":\"agent-tracer-dashboard\"}";
              }
              location / {
                add_header Content-Type text/html;
                return 200 "<html><body><h1>Agent Tracer Dashboard</h1><p>Service is running</p><p>Status: Healthy</p></body></html>";
              }
            }' > /etc/nginx/conf.d/default.conf
            nginx -g "daemon off;"
          command:
          - /bin/sh
          image: nginx:alpine
          imagePullPolicy: IfNotPresent
          name: agent-tracer-dashboard
          ports:
          - containerPort: 80
            protocol: TCP
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastTransitionTime: "2025-10-06T23:40:48Z"
      lastUpdateTime: "2025-10-06T23:40:52Z"
      message: ReplicaSet "agent-tracer-dashboard-94b6c49d6" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-10-21T14:17:39Z"
      lastUpdateTime: "2025-10-21T14:17:39Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      buildkit.ai/auto-deploy: "true"
      deployment.kubernetes.io/revision: "1"
      gitlab.com/project: llm/npm/agent-buildkit
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"buildkit.ai/auto-deploy":"true","gitlab.com/project":"llm/npm/agent-buildkit"},"labels":{"app":"ci-fix-agent","category":"automation","environment":"production","ossa.ai-agent-version":"1.0.0","ossa.ai-capability":"pipeline-repair","ossa.ai-domain":"infrastructure","ossa.ai-subdomain":"ci-cd","ossa.ai-version":"ossa-v0.2.3","team":"platform"},"name":"ci-fix-agent","namespace":"default"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"ci-fix-agent"}},"template":{"metadata":{"labels":{"app":"ci-fix-agent","category":"automation","environment":"production","ossa.ai-agent-version":"1.0.0","ossa.ai-capability":"pipeline-repair","ossa.ai-domain":"infrastructure","ossa.ai-subdomain":"ci-cd","ossa.ai-version":"ossa-v0.2.3","team":"platform"}},"spec":{"containers":[{"env":[{"name":"AGENT_NAME","value":"ci-fix-agent"},{"name":"AGENT_VERSION","value":"1.0.0"},{"name":"OSSA_VERSION","value":"ossa/v0.2.3"},{"name":"AGENT_ROLE","value":"You are a CI/CD pipeline expert. You analyze failed GitLab CI pipelines, identify root causes, and automatically fix common issues like dependency conflicts, test failures, and configuration errors."},{"name":"LLM_PROVIDER","value":"anthropic"},{"name":"LLM_MODEL","value":"claude-3-5-sonnet-20241022"},{"name":"LLM_TEMPERATURE","value":"0.1"},{"name":"OSSA_TOOLS","value":"[{\"type\":\"http\",\"name\":\"gitlab_api\"},{\"type\":\"kubernetes\"},{\"type\":\"function\",\"name\":\"ci_analyzer\"}]"}],"image":"node:alpine","name":"agent","resources":{"limits":{"cpu":"200m","memory":"256Mi"},"requests":{"cpu":"100m","memory":"128Mi"}}}]}}}}
    creationTimestamp: "2025-11-17T16:41:55Z"
    generation: 2
    labels:
      app: ci-fix-agent
      category: automation
      environment: production
      ossa.ai-agent-version: 1.0.0
      ossa.ai-capability: pipeline-repair
      ossa.ai-domain: infrastructure
      ossa.ai-subdomain: ci-cd
      ossa.ai-version: ossa-v0.2.3
      team: platform
    name: ci-fix-agent
    namespace: default
    resourceVersion: "7628106"
    uid: 9eb82414-c06b-4915-88db-b93724561a14
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: ci-fix-agent
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: ci-fix-agent
          category: automation
          environment: production
          ossa.ai-agent-version: 1.0.0
          ossa.ai-capability: pipeline-repair
          ossa.ai-domain: infrastructure
          ossa.ai-subdomain: ci-cd
          ossa.ai-version: ossa-v0.2.3
          team: platform
      spec:
        containers:
        - env:
          - name: AGENT_NAME
            value: ci-fix-agent
          - name: AGENT_VERSION
            value: 1.0.0
          - name: OSSA_VERSION
            value: ossa/v0.2.3
          - name: AGENT_ROLE
            value: You are a CI/CD pipeline expert. You analyze failed GitLab CI pipelines,
              identify root causes, and automatically fix common issues like dependency
              conflicts, test failures, and configuration errors.
          - name: LLM_PROVIDER
            value: anthropic
          - name: LLM_MODEL
            value: claude-3-5-sonnet-20241022
          - name: LLM_TEMPERATURE
            value: "0.1"
          - name: OSSA_TOOLS
            value: '[{"type":"http","name":"gitlab_api"},{"type":"kubernetes"},{"type":"function","name":"ci_analyzer"}]'
          image: node:alpine
          imagePullPolicy: IfNotPresent
          name: agent
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastTransitionTime: "2025-11-17T23:43:50Z"
      lastUpdateTime: "2025-11-17T23:43:50Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-17T23:43:51Z"
      lastUpdateTime: "2025-11-17T23:43:51Z"
      message: ReplicaSet "ci-fix-agent-5b6d9b8d5" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"git-cleanup-agent","environment":"production","ossa.ai-agent-version":"1.0.0","ossa.ai-capability":"repository-maintenance","ossa.ai-domain":"infrastructure","ossa.ai-subdomain":"version-control","ossa.ai-version":"ossa-v0.2.3","team":"devops"},"name":"git-cleanup-agent","namespace":"default"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"git-cleanup-agent"}},"template":{"metadata":{"labels":{"app":"git-cleanup-agent","environment":"production","ossa.ai-agent-version":"1.0.0","ossa.ai-capability":"repository-maintenance","ossa.ai-domain":"infrastructure","ossa.ai-subdomain":"version-control","ossa.ai-version":"ossa-v0.2.3","team":"devops"}},"spec":{"containers":[{"env":[{"name":"AGENT_NAME","value":"git-cleanup-agent"},{"name":"AGENT_VERSION","value":"1.0.0"},{"name":"OSSA_VERSION","value":"ossa/v0.2.3"},{"name":"AGENT_ROLE","value":"You are a Git repository cleanup specialist. You analyze repositories, identify stale branches, clean up merge conflicts, and maintain repository health."},{"name":"LLM_PROVIDER","value":"anthropic"},{"name":"LLM_MODEL","value":"claude-3-5-sonnet-20241022"},{"name":"LLM_TEMPERATURE","value":"0.2"},{"name":"OSSA_TOOLS","value":"[{\"type\":\"kubernetes\"},{\"type\":\"function\",\"name\":\"git_operations\"}]"}],"image":"alpine/git:latest","name":"agent","resources":{"limits":{"cpu":"100m","memory":"128Mi"},"requests":{"cpu":"50m","memory":"64Mi"}}}]}}}}
    creationTimestamp: "2025-11-17T16:41:55Z"
    generation: 2
    labels:
      app: git-cleanup-agent
      environment: production
      ossa.ai-agent-version: 1.0.0
      ossa.ai-capability: repository-maintenance
      ossa.ai-domain: infrastructure
      ossa.ai-subdomain: version-control
      ossa.ai-version: ossa-v0.2.3
      team: devops
    name: git-cleanup-agent
    namespace: default
    resourceVersion: "7628433"
    uid: 89865299-0325-4b66-8dfb-764c421c4ff5
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: git-cleanup-agent
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: git-cleanup-agent
          environment: production
          ossa.ai-agent-version: 1.0.0
          ossa.ai-capability: repository-maintenance
          ossa.ai-domain: infrastructure
          ossa.ai-subdomain: version-control
          ossa.ai-version: ossa-v0.2.3
          team: devops
      spec:
        containers:
        - env:
          - name: AGENT_NAME
            value: git-cleanup-agent
          - name: AGENT_VERSION
            value: 1.0.0
          - name: OSSA_VERSION
            value: ossa/v0.2.3
          - name: AGENT_ROLE
            value: You are a Git repository cleanup specialist. You analyze repositories,
              identify stale branches, clean up merge conflicts, and maintain repository
              health.
          - name: LLM_PROVIDER
            value: anthropic
          - name: LLM_MODEL
            value: claude-3-5-sonnet-20241022
          - name: LLM_TEMPERATURE
            value: "0.2"
          - name: OSSA_TOOLS
            value: '[{"type":"kubernetes"},{"type":"function","name":"git_operations"}]'
          image: alpine/git:latest
          imagePullPolicy: Always
          name: agent
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastTransitionTime: "2025-11-17T23:43:56Z"
      lastUpdateTime: "2025-11-17T23:43:56Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-17T23:43:57Z"
      lastUpdateTime: "2025-11-17T23:43:57Z"
      message: ReplicaSet "git-cleanup-agent-56cdf98687" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"ossa","type":"orchestrator"},"name":"ossa","namespace":"default"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"ossa"}},"template":{"metadata":{"labels":{"app":"ossa"}},"spec":{"containers":[{"env":[{"name":"NODE_ENV","value":"production"},{"name":"PORT","value":"3000"}],"image":"ossa:latest","name":"ossa","ports":[{"containerPort":3000}]}]}}}}
    creationTimestamp: "2025-10-01T05:31:06Z"
    generation: 3
    labels:
      app: ossa
      type: orchestrator
    name: ossa
    namespace: default
    resourceVersion: "6617456"
    uid: 2242c3f4-b965-4576-923f-1f51c9aa0d1b
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: ossa
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: ossa
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: PORT
            value: "3000"
          image: ossa:latest
          imagePullPolicy: Always
          name: ossa
          ports:
          - containerPort: 3000
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastTransitionTime: "2025-10-22T03:57:03Z"
      lastUpdateTime: "2025-10-22T03:57:03Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-10-22T03:57:05Z"
      lastUpdateTime: "2025-11-10T04:43:53Z"
      message: ReplicaSet "ossa-68dc75d46b" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 3
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"ossa-website","component":"frontend"},"name":"ossa-website","namespace":"default"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"ossa-website"}},"template":{"metadata":{"labels":{"app":"ossa-website","component":"frontend"}},"spec":{"containers":[{"image":"ossa-website:latest","imagePullPolicy":"Never","livenessProbe":{"failureThreshold":5,"httpGet":{"path":"/","port":3000},"initialDelaySeconds":60,"periodSeconds":15,"timeoutSeconds":10},"name":"website","ports":[{"containerPort":3000,"name":"http","protocol":"TCP"}],"readinessProbe":{"failureThreshold":5,"httpGet":{"path":"/","port":3000},"initialDelaySeconds":30,"periodSeconds":10,"timeoutSeconds":5},"resources":{"limits":{"cpu":"500m","memory":"256Mi"},"requests":{"cpu":"50m","memory":"64Mi"}}}]}}}}
    creationTimestamp: "2025-11-17T19:39:59Z"
    generation: 7
    labels:
      app: ossa-website
      component: frontend
    name: ossa-website
    namespace: default
    resourceVersion: "7640952"
    uid: 501234a6-65f6-488e-9806-5407be1ae79f
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: ossa-website
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-11-17T20:00:13-05:00"
        creationTimestamp: null
        labels:
          app: ossa-website
          component: frontend
      spec:
        containers:
        - image: ossa-website:latest
          imagePullPolicy: Never
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 10
          name: website
          ports:
          - containerPort: 3000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastTransitionTime: "2025-11-18T01:12:20Z"
      lastUpdateTime: "2025-11-18T01:12:20Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-17T23:44:14Z"
      lastUpdateTime: "2025-11-18T01:12:20Z"
      message: ReplicaSet "ossa-website-9f7c9b69b" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 7
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      buildkit.ai/success-rate: 95%
      buildkit.ai/target-errors: "8189"
      deployment.kubernetes.io/revision: "1"
      docs.url: https://gitlab.bluefly.io/llm/npm/agent-buildkit/-/wikis/typescript-fixer
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"buildkit.ai/success-rate":"95%","buildkit.ai/target-errors":"8189","docs.url":"https://gitlab.bluefly.io/llm/npm/agent-buildkit/-/wikis/typescript-fixer"},"labels":{"app":"typescript-fixer-agent","category":"code-quality","environment":"production","ossa.ai-agent-version":"1.0.0","ossa.ai-capability":"typescript-fixing","ossa.ai-domain":"infrastructure","ossa.ai-subdomain":"code-quality","ossa.ai-version":"ossa-v0.2.3","priority":"high","team":"platform"},"name":"typescript-fixer-agent","namespace":"default"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"typescript-fixer-agent"}},"template":{"metadata":{"labels":{"app":"typescript-fixer-agent","category":"code-quality","environment":"production","ossa.ai-agent-version":"1.0.0","ossa.ai-capability":"typescript-fixing","ossa.ai-domain":"infrastructure","ossa.ai-subdomain":"code-quality","ossa.ai-version":"ossa-v0.2.3","priority":"high","team":"platform"}},"spec":{"containers":[{"env":[{"name":"AGENT_NAME","value":"typescript-fixer-agent"},{"name":"AGENT_VERSION","value":"1.0.0"},{"name":"OSSA_VERSION","value":"ossa/v0.2.3"},{"name":"AGENT_ROLE","value":"You are a TypeScript expert specializing in automated error fixing. You analyze TypeScript compilation errors, understand their context, and apply intelligent fixes including type annotations, import corrections, interface updates, and refactoring. You handle type mismatches, missing definitions, strict mode violations, and configuration issues."},{"name":"LLM_PROVIDER","value":"anthropic"},{"name":"LLM_MODEL","value":"claude-3-5-sonnet-20241022"},{"name":"LLM_TEMPERATURE","value":"0.3"},{"name":"OSSA_TOOLS","value":"[{\"type\":\"mcp\"},{\"type\":\"function\",\"name\":\"typescript_compiler\"},{\"type\":\"function\",\"name\":\"code_refactor\"},{\"type\":\"kubernetes\"}]"}],"image":"node:22-alpine","name":"agent","resources":{"limits":{"cpu":"500m","memory":"512Mi"},"requests":{"cpu":"250m","memory":"256Mi"}}}]}}}}
    creationTimestamp: "2025-11-17T16:41:55Z"
    generation: 2
    labels:
      app: typescript-fixer-agent
      category: code-quality
      environment: production
      ossa.ai-agent-version: 1.0.0
      ossa.ai-capability: typescript-fixing
      ossa.ai-domain: infrastructure
      ossa.ai-subdomain: code-quality
      ossa.ai-version: ossa-v0.2.3
      priority: high
      team: platform
    name: typescript-fixer-agent
    namespace: default
    resourceVersion: "7629646"
    uid: d52d80a6-4279-409a-b3ee-b740e6fbd72b
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: typescript-fixer-agent
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: typescript-fixer-agent
          category: code-quality
          environment: production
          ossa.ai-agent-version: 1.0.0
          ossa.ai-capability: typescript-fixing
          ossa.ai-domain: infrastructure
          ossa.ai-subdomain: code-quality
          ossa.ai-version: ossa-v0.2.3
          priority: high
          team: platform
      spec:
        containers:
        - env:
          - name: AGENT_NAME
            value: typescript-fixer-agent
          - name: AGENT_VERSION
            value: 1.0.0
          - name: OSSA_VERSION
            value: ossa/v0.2.3
          - name: AGENT_ROLE
            value: You are a TypeScript expert specializing in automated error fixing.
              You analyze TypeScript compilation errors, understand their context,
              and apply intelligent fixes including type annotations, import corrections,
              interface updates, and refactoring. You handle type mismatches, missing
              definitions, strict mode violations, and configuration issues.
          - name: LLM_PROVIDER
            value: anthropic
          - name: LLM_MODEL
            value: claude-3-5-sonnet-20241022
          - name: LLM_TEMPERATURE
            value: "0.3"
          - name: OSSA_TOOLS
            value: '[{"type":"mcp"},{"type":"function","name":"typescript_compiler"},{"type":"function","name":"code_refactor"},{"type":"kubernetes"}]'
          image: node:22-alpine
          imagePullPolicy: IfNotPresent
          name: agent
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 250m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastTransitionTime: "2025-11-17T23:44:18Z"
      lastUpdateTime: "2025-11-17T23:44:18Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-17T23:44:19Z"
      lastUpdateTime: "2025-11-17T23:44:19Z"
      message: ReplicaSet "typescript-fixer-agent-7cc6dbfbd4" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"dragonfly"},"name":"dragonfly","namespace":"dragonfly-system"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"dragonfly"}},"template":{"metadata":{"labels":{"app":"dragonfly"}},"spec":{"containers":[{"args":["--requirepass=dragonfly-secret","--maxmemory=8gb","--cache_mode=true","--snapshot_cron=\"0 */6 * * *\"","--dbfilename=dump.rdb","--logtostderr"],"image":"docker.dragonflydb.io/dragonflydb/dragonfly:latest","livenessProbe":{"exec":{"command":["redis-cli","-a","dragonfly-secret","ping"]},"initialDelaySeconds":30,"periodSeconds":10},"name":"dragonfly","ports":[{"containerPort":6379,"name":"redis"}],"readinessProbe":{"exec":{"command":["redis-cli","-a","dragonfly-secret","ping"]},"initialDelaySeconds":5,"periodSeconds":5},"resources":{"limits":{"cpu":"4","memory":"8Gi"},"requests":{"cpu":"500m","memory":"2Gi"}},"volumeMounts":[{"mountPath":"/data","name":"data"}]}],"volumes":[{"emptyDir":{},"name":"data"}]}}}}
    creationTimestamp: "2025-11-13T19:37:41Z"
    generation: 3
    labels:
      app: dragonfly
    name: dragonfly
    namespace: dragonfly-system
    resourceVersion: "7089740"
    uid: 48f3e9f4-a25c-4f52-8599-079990e1ed9c
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: dragonfly
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: dragonfly
      spec:
        containers:
        - args:
          - --requirepass=dragonfly-secret
          - --maxmemory=8gb
          - --cache_mode=true
          - --snapshot_cron="0 */6 * * *"
          - --dbfilename=dump.rdb
          - --logtostderr
          image: docker.dragonflydb.io/dragonflydb/dragonfly:latest
          imagePullPolicy: Always
          livenessProbe:
            exec:
              command:
              - redis-cli
              - -a
              - dragonfly-secret
              - ping
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: dragonfly
          ports:
          - containerPort: 6379
            name: redis
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - redis-cli
              - -a
              - dragonfly-secret
              - ping
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "4"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: data
  status:
    conditions:
    - lastTransitionTime: "2025-11-14T08:01:19Z"
      lastUpdateTime: "2025-11-14T08:01:19Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-14T08:01:20Z"
      lastUpdateTime: "2025-11-14T08:01:20Z"
      message: ReplicaSet "dragonfly-6ff4d57c84" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 3
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"agent-orchestrator"},"name":"agent-orchestrator","namespace":"drupal-agents"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"agent-orchestrator"}},"template":{"metadata":{"labels":{"app":"agent-orchestrator"}},"spec":{"containers":[{"args":["-c","php -S 0.0.0.0:8080 -t /app"],"command":["/bin/bash"],"env":[{"name":"DRUPAL_ROOT","value":"/app"}],"image":"drupal:11-apache","name":"orchestrator","ports":[{"containerPort":8080,"name":"http"}],"resources":{"limits":{"cpu":"500m","memory":"512Mi"},"requests":{"cpu":"250m","memory":"256Mi"}},"volumeMounts":[{"mountPath":"/app","name":"app-volume"}]}],"volumes":[{"hostPath":{"path":"/Users/flux423/Sites/Cannabis/indoorplantkingdom.com","type":"Directory"},"name":"app-volume"}]}}}}
    creationTimestamp: "2025-11-14T17:11:32Z"
    generation: 1
    labels:
      app: agent-orchestrator
    name: agent-orchestrator
    namespace: drupal-agents
    resourceVersion: "7642937"
    uid: 6120b5c5-38c1-44da-b720-d691f36c27ac
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: agent-orchestrator
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: agent-orchestrator
      spec:
        containers:
        - args:
          - -c
          - php -S 0.0.0.0:8080 -t /app
          command:
          - /bin/bash
          env:
          - name: DRUPAL_ROOT
            value: /app
          image: drupal:11-apache
          imagePullPolicy: IfNotPresent
          name: orchestrator
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 250m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app
            name: app-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /Users/flux423/Sites/Cannabis/indoorplantkingdom.com
            type: Directory
          name: app-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-14T17:11:32Z"
      lastUpdateTime: "2025-11-14T17:11:41Z"
      message: ReplicaSet "agent-orchestrator-7847c9cb8c" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-18T01:37:22Z"
      lastUpdateTime: "2025-11-18T01:37:22Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: goldilocks
      meta.helm.sh/release-namespace: goldilocks
    creationTimestamp: "2025-11-10T04:40:38Z"
    generation: 2
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: goldilocks
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: goldilocks
      helm.sh/chart: goldilocks-10.1.0
    name: goldilocks-controller
    namespace: goldilocks
    resourceVersion: "6763301"
    uid: c15e39bc-c71a-4614-9332-680703b66939
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: goldilocks
        app.kubernetes.io/name: goldilocks
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: goldilocks
          app.kubernetes.io/name: goldilocks
      spec:
        containers:
        - command:
          - /goldilocks
          - controller
          - -v2
          image: us-docker.pkg.dev/fairwinds-ops/oss/goldilocks:v4.14.1
          imagePullPolicy: Always
          name: goldilocks
          resources:
            requests:
              cpu: 25m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 10324
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: goldilocks-controller
        serviceAccountName: goldilocks-controller
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastTransitionTime: "2025-11-11T17:13:46Z"
      lastUpdateTime: "2025-11-11T17:13:46Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-11T17:13:46Z"
      lastUpdateTime: "2025-11-11T17:13:46Z"
      message: ReplicaSet "goldilocks-controller-55c758c6dd" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: goldilocks
      meta.helm.sh/release-namespace: goldilocks
    creationTimestamp: "2025-11-10T04:40:38Z"
    generation: 2
    labels:
      app.kubernetes.io/component: dashboard
      app.kubernetes.io/instance: goldilocks
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: goldilocks
      helm.sh/chart: goldilocks-10.1.0
    name: goldilocks-dashboard
    namespace: goldilocks
    resourceVersion: "6763306"
    uid: 1b86e4f5-985d-4f2b-9fb7-751bd61bce94
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: dashboard
        app.kubernetes.io/instance: goldilocks
        app.kubernetes.io/name: goldilocks
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: dashboard
          app.kubernetes.io/instance: goldilocks
          app.kubernetes.io/name: goldilocks
      spec:
        containers:
        - command:
          - /goldilocks
          - dashboard
          - --exclude-containers=linkerd-proxy,istio-proxy
          - -v2
          image: us-docker.pkg.dev/fairwinds-ops/oss/goldilocks:v4.14.1
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: goldilocks
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 25m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 10324
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: goldilocks-dashboard
        serviceAccountName: goldilocks-dashboard
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastTransitionTime: "2025-11-11T17:14:03Z"
      lastUpdateTime: "2025-11-11T17:14:03Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-11T17:13:58Z"
      lastUpdateTime: "2025-11-11T17:14:03Z"
      message: ReplicaSet "goldilocks-dashboard-84f44f47f4" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: goldilocks
      meta.helm.sh/release-namespace: goldilocks
    creationTimestamp: "2025-11-10T04:40:38Z"
    generation: 2
    labels:
      app.kubernetes.io/component: admission-controller
      app.kubernetes.io/instance: goldilocks
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: vpa
      app.kubernetes.io/version: 1.4.1
      helm.sh/chart: vpa-4.8.1
    name: goldilocks-vpa-admission-controller
    namespace: goldilocks
    resourceVersion: "6763308"
    uid: 8561f404-3a9e-4bc9-98cf-8400fe627719
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: admission-controller
        app.kubernetes.io/instance: goldilocks
        app.kubernetes.io/name: vpa
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: admission-controller
          app.kubernetes.io/instance: goldilocks
          app.kubernetes.io/name: vpa
      spec:
        containers:
        - args:
          - --register-webhook=false
          - --webhook-service=goldilocks-vpa-webhook
          - --client-ca-file=/etc/tls-certs/ca
          - --tls-cert-file=/etc/tls-certs/cert
          - --tls-private-key=/etc/tls-certs/key
          env:
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: registry.k8s.io/autoscaling/vpa-admission-controller:1.4.1
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /health-check
              port: metrics
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          name: vpa
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8944
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 120
            httpGet:
              path: /health-check
              port: metrics
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            requests:
              cpu: 50m
              memory: 200Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/tls-certs
            name: tls-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: goldilocks-vpa-admission-controller
        serviceAccountName: goldilocks-vpa-admission-controller
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-certs
          secret:
            defaultMode: 420
            secretName: goldilocks-vpa-tls-secret
  status:
    conditions:
    - lastTransitionTime: "2025-11-11T17:14:13Z"
      lastUpdateTime: "2025-11-11T17:14:13Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-11T17:14:13Z"
      lastUpdateTime: "2025-11-11T17:14:13Z"
      message: ReplicaSet "goldilocks-vpa-admission-controller-7b66c67dd6" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: goldilocks
      meta.helm.sh/release-namespace: goldilocks
    creationTimestamp: "2025-11-10T04:40:38Z"
    generation: 2
    labels:
      app.kubernetes.io/component: recommender
      app.kubernetes.io/instance: goldilocks
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: vpa
      app.kubernetes.io/version: 1.4.1
      helm.sh/chart: vpa-4.8.1
    name: goldilocks-vpa-recommender
    namespace: goldilocks
    resourceVersion: "6763310"
    uid: c8a075f0-071f-4c18-99b3-29aaaa231e43
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: recommender
        app.kubernetes.io/instance: goldilocks
        app.kubernetes.io/name: vpa
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: recommender
          app.kubernetes.io/instance: goldilocks
          app.kubernetes.io/name: vpa
      spec:
        containers:
        - args:
          - --pod-recommendation-min-cpu-millicores=15
          - --pod-recommendation-min-memory-mb=100
          - --v=4
          image: registry.k8s.io/autoscaling/vpa-recommender:1.4.1
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /health-check
              port: metrics
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          name: vpa
          ports:
          - containerPort: 8942
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 120
            httpGet:
              path: /health-check
              port: metrics
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            requests:
              cpu: 50m
              memory: 500Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: goldilocks-vpa-recommender
        serviceAccountName: goldilocks-vpa-recommender
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastTransitionTime: "2025-11-11T17:14:14Z"
      lastUpdateTime: "2025-11-11T17:14:14Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-11T17:14:14Z"
      lastUpdateTime: "2025-11-11T17:14:14Z"
      message: ReplicaSet "goldilocks-vpa-recommender-bcc897f7d" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2025-09-07T03:15:30Z"
    generation: 1
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.13.2
      helm.sh/chart: ingress-nginx-4.13.2
    name: ingress-nginx-controller
    namespace: ingress-nginx
    resourceVersion: "7643146"
    uid: 5cccec23-6da0-4af6-b96b-e779aece2a9a
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: ingress-nginx
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: ingress-nginx
          app.kubernetes.io/part-of: ingress-nginx
          app.kubernetes.io/version: 1.13.2
          helm.sh/chart: ingress-nginx-4.13.2
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - /nginx-ingress-controller
          - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
          - --election-id=ingress-nginx-leader
          - --controller-class=k8s.io/ingress-nginx
          - --ingress-class=nginx
          - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
          - --validating-webhook=:8443
          - --validating-webhook-certificate=/usr/local/certificates/cert
          - --validating-webhook-key=/usr/local/certificates/key
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: LD_PRELOAD
            value: /usr/local/lib/libmimalloc.so
          image: registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /wait-shutdown
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: controller
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            name: https
            protocol: TCP
          - containerPort: 8443
            name: webhook
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 90Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsGroup: 82
            runAsNonRoot: true
            runAsUser: 101
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /usr/local/certificates/
            name: webhook-cert
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: ingress-nginx
        serviceAccountName: ingress-nginx
        terminationGracePeriodSeconds: 300
        volumes:
        - name: webhook-cert
          secret:
            defaultMode: 420
            secretName: ingress-nginx-admission
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-09-07T03:15:30Z"
      lastUpdateTime: "2025-09-07T03:15:49Z"
      message: ReplicaSet "ingress-nginx-controller-5b8bf5478" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-18T01:37:34Z"
      lastUpdateTime: "2025-11-18T01:37:34Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: keda
      meta.helm.sh/release-namespace: keda
    creationTimestamp: "2025-10-21T15:54:48Z"
    generation: 1
    labels:
      app: keda-admission-webhooks
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: keda
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: keda-admission-webhooks
      app.kubernetes.io/part-of: keda-operator
      app.kubernetes.io/version: 2.18.0
      helm.sh/chart: keda-2.18.0
      name: keda-admission-webhooks
    name: keda-admission-webhooks
    namespace: keda
    resourceVersion: "7643404"
    uid: 4135a2e3-ea8b-418b-9af3-cf2b56b90ccb
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: keda-admission-webhooks
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: keda-admission-webhooks
          app.kubernetes.io/component: operator
          app.kubernetes.io/instance: keda
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: keda-admission-webhooks
          app.kubernetes.io/part-of: keda-operator
          app.kubernetes.io/version: 2.18.0
          helm.sh/chart: keda-2.18.0
          name: keda-admission-webhooks
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --zap-log-level=info
          - --zap-encoder=console
          - --zap-time-encoding=rfc3339
          - --cert-dir=/certs
          - --health-probe-bind-address=:8081
          - --metrics-bind-address=:8080
          command:
          - /keda-admission-webhooks
          env:
          - name: WATCH_NAMESPACE
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: ghcr.io/kedacore/keda-admission-webhooks:2.18.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 25
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: keda-admission-webhooks
          ports:
          - containerPort: 9443
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 3
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1000Mi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /certs
            name: certificates
            readOnly: true
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
        serviceAccount: keda-webhook
        serviceAccountName: keda-webhook
        terminationGracePeriodSeconds: 30
        volumes:
        - name: certificates
          secret:
            defaultMode: 420
            secretName: kedaorg-certs
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-10-21T15:54:48Z"
      lastUpdateTime: "2025-10-21T16:00:50Z"
      message: ReplicaSet "keda-admission-webhooks-6656fbbb89" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-18T01:37:44Z"
      lastUpdateTime: "2025-11-18T01:37:44Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: keda
      meta.helm.sh/release-namespace: keda
    creationTimestamp: "2025-10-21T15:54:48Z"
    generation: 1
    labels:
      app: keda-operator
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: keda
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: keda-operator
      app.kubernetes.io/part-of: keda-operator
      app.kubernetes.io/version: 2.18.0
      helm.sh/chart: keda-2.18.0
      name: keda-operator
    name: keda-operator
    namespace: keda
    resourceVersion: "7643342"
    uid: 2f68bc20-fd19-41cf-b1fe-4637309eabb0
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: keda-operator
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: keda-operator
          app.kubernetes.io/component: operator
          app.kubernetes.io/instance: keda
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: keda-operator
          app.kubernetes.io/part-of: keda-operator
          app.kubernetes.io/version: 2.18.0
          helm.sh/chart: keda-2.18.0
          name: keda-operator
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --leader-elect
          - --disable-compression=true
          - --zap-log-level=info
          - --zap-encoder=console
          - --zap-time-encoding=rfc3339
          - --enable-webhook-patching=true
          - --cert-dir=/certs
          - --enable-cert-rotation=true
          - --cert-secret-name=kedaorg-certs
          - --operator-service-name=keda-operator
          - --metrics-server-service-name=keda-operator-metrics-apiserver
          - --webhooks-service-name=keda-admission-webhooks
          - --k8s-cluster-name=kubernetes-default
          - --k8s-cluster-domain=cluster.local
          - --enable-prometheus-metrics=false
          command:
          - /keda
          env:
          - name: WATCH_NAMESPACE
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: OPERATOR_NAME
            value: keda-operator
          - name: KEDA_HTTP_DEFAULT_TIMEOUT
            value: "3000"
          - name: KEDA_HTTP_MIN_TLS_VERSION
            value: TLS12
          image: ghcr.io/kedacore/keda:2.18.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 25
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: keda-operator
          ports:
          - containerPort: 9666
            name: metricsservice
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 3
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1000Mi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /certs
            name: certificates
            readOnly: true
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
        serviceAccount: keda-operator
        serviceAccountName: keda-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: certificates
          secret:
            defaultMode: 420
            optional: true
            secretName: kedaorg-certs
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-10-21T15:54:48Z"
      lastUpdateTime: "2025-10-21T15:57:44Z"
      message: ReplicaSet "keda-operator-6687649c57" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-18T01:37:42Z"
      lastUpdateTime: "2025-11-18T01:37:42Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: keda
      meta.helm.sh/release-namespace: keda
    creationTimestamp: "2025-10-21T15:54:48Z"
    generation: 1
    labels:
      app: keda-operator-metrics-apiserver
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: keda
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: keda-operator-metrics-apiserver
      app.kubernetes.io/part-of: keda-operator
      app.kubernetes.io/version: 2.18.0
      helm.sh/chart: keda-2.18.0
    name: keda-operator-metrics-apiserver
    namespace: keda
    resourceVersion: "7643078"
    uid: 8fff03a4-21d9-4872-b452-969e56adb6d7
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: keda-operator-metrics-apiserver
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: keda-operator-metrics-apiserver
          app.kubernetes.io/component: operator
          app.kubernetes.io/instance: keda
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: keda-operator-metrics-apiserver
          app.kubernetes.io/part-of: keda-operator
          app.kubernetes.io/version: 2.18.0
          helm.sh/chart: keda-2.18.0
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --secure-port=6443
          - --logtostderr=true
          - --stderrthreshold=ERROR
          - --disable-compression=true
          - --metrics-service-address=keda-operator.keda.svc.cluster.local:9666
          - --client-ca-file=/certs/ca.crt
          - --tls-cert-file=/certs/tls.crt
          - --tls-private-key-file=/certs/tls.key
          - --cert-dir=/certs
          - --v=0
          command:
          - /keda-adapter
          env:
          - name: WATCH_NAMESPACE
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: KEDA_HTTP_DEFAULT_TIMEOUT
            value: "3000"
          - name: KEDA_HTTP_MIN_TLS_VERSION
            value: TLS12
          image: ghcr.io/kedacore/keda-metrics-apiserver:2.18.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6443
              scheme: HTTPS
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: keda-operator-metrics-apiserver
          ports:
          - containerPort: 6443
            name: https
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 6443
              scheme: HTTPS
            initialDelaySeconds: 5
            periodSeconds: 3
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1000Mi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /certs
            name: certificates
            readOnly: true
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
        serviceAccount: keda-metrics-server
        serviceAccountName: keda-metrics-server
        terminationGracePeriodSeconds: 30
        volumes:
        - name: certificates
          secret:
            defaultMode: 420
            secretName: kedaorg-certs
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-10-21T15:54:48Z"
      lastUpdateTime: "2025-10-21T15:57:35Z"
      message: ReplicaSet "keda-operator-metrics-apiserver-74bcc6f665" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-18T01:37:27Z"
      lastUpdateTime: "2025-11-18T01:37:27Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment-wave: "3"
      deployment.kubernetes.io/revision: "1"
      generated-by: agent-swarm
      issues-closed: "79"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"deployment-wave":"3","generated-by":"agent-swarm","issues-closed":"79"},"labels":{"app":"khook","managed-by":"agent-buildkit","ossa-version":"v0.2.3","platform":"kagent"},"name":"khook","namespace":"khook-system"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"khook","managed-by":"agent-buildkit","ossa-version":"v0.2.3","platform":"kagent"}},"template":{"metadata":{"annotations":{"deployment-wave":"3","generated-by":"agent-swarm","issues-closed":"79"},"labels":{"app":"khook","managed-by":"agent-buildkit","ossa-version":"v0.2.3","platform":"kagent"}},"spec":{"containers":[{"env":[{"name":"KUBERNETES_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"WEBHOOK_PORT","value":"8443"},{"name":"KAGENT_ENABLED","value":"true"},{"name":"NATS_URL","value":"nats://nats.kagent-system.svc.cluster.local:4222"}],"image":"ghcr.io/kagent/khook:latest","name":"khook","ports":[{"containerPort":8443,"name":"webhook"}],"resources":{"limits":{"cpu":"200m","memory":"256Mi"},"requests":{"cpu":"50m","memory":"128Mi"}},"volumeMounts":[{"mountPath":"/etc/webhook/certs","name":"webhook-certs","readOnly":true}]}],"serviceAccountName":"khook","volumes":[{"name":"webhook-certs","secret":{"secretName":"khook-webhook-certs"}}]}}}}
    creationTimestamp: "2025-11-13T01:12:13Z"
    generation: 11
    labels:
      app: khook
      managed-by: agent-buildkit
      ossa-version: v0.2.3
      platform: kagent
    name: khook
    namespace: khook-system
    resourceVersion: "7511812"
    uid: a9094e0f-ea00-422e-ae6b-2dda10c088ba
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: khook
        managed-by: agent-buildkit
        ossa-version: v0.2.3
        platform: kagent
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          deployment-wave: "3"
          generated-by: agent-swarm
          issues-closed: "79"
        creationTimestamp: null
        labels:
          app: khook
          managed-by: agent-buildkit
          ossa-version: v0.2.3
          platform: kagent
      spec:
        containers:
        - env:
          - name: KUBERNETES_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: WEBHOOK_PORT
            value: "8443"
          - name: KAGENT_ENABLED
            value: "true"
          - name: NATS_URL
            value: nats://nats.kagent-system.svc.cluster.local:4222
          image: ghcr.io/kagent/khook:latest
          imagePullPolicy: Always
          name: khook
          ports:
          - containerPort: 8443
            name: webhook
            protocol: TCP
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/webhook/certs
            name: webhook-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: khook
        serviceAccountName: khook
        terminationGracePeriodSeconds: 30
        volumes:
        - name: webhook-certs
          secret:
            defaultMode: 420
            secretName: khook-webhook-certs
  status:
    conditions:
    - lastTransitionTime: "2025-11-13T08:00:07Z"
      lastUpdateTime: "2025-11-13T08:00:07Z"
      message: ReplicaSet "khook-674bc6d8c9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-17T08:00:41Z"
      lastUpdateTime: "2025-11-17T08:00:41Z"
      message: Deployment does not have minimum availability.
      reason: MinimumReplicasUnavailable
      status: "False"
      type: Available
    observedGeneration: 11
    replicas: 1
    unavailableReplicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xU328aRxD+V6p5voMjdoJ9Uh9ScBUrMUUmzkuEomFvDrbs7Wx397CRdf97NXeAIfWPVuoTx+7Mt9/M9808Ajr9jXzQbCEHdC70NwNIYK1tATmMyRneVmQjJFBRxAIjQv4IaC1HjJptkL+8+JNUDBR7XnNPYYyGepr7WkAgefGe7y35dLlZQw7rs3B0sxkkv3zWtvj1Y1GwfRPCYkWQA/tFqthTYcO/SgkOleSt6wWlYRsiVdAkYHBBpi1sfRFSdG4f0uHKp7cUKQja7ukRexpPZq88u8Kwghwus/MP7+j8/eVwUShUiMPs7OLiPCuHFx/O32eohtlwUeI7IbLDfirpFdLBkRLKnjZa9PykQ2S//aIrHSHPEghkSEX2ElRhVKsvr5XZCGT0GGm5bWHZGG2Xd67ASB3Ew53FDWqDC0OQD5oE4tYJs9uTWDmnypl93pGNXm10c1SUYhtRW/IB8u+PgH4pH5AqtiUk0Keo+rsu9UWJUhuCeQK6wqUw8mjViny/0t5L2N4l+9980BtkPXF+mzGtjZmy0WoLOVyXE45TT6Ebg39o4tjHjtaB5ZR9hPz92SF6F+k5smIDOdyNp9Akb6SkUbnTtK+jZ9MuB0eJFUWvVXgmcZ6Ap8C1V9Q23Yg3QidKxV6KPTvPbrRI5umvmkJ3q1wNOQyyrGrXwC502EaKSKRqr+N2xDbSQ2z3gzF8P/V6ow0t6SooNO22gLxEEygBhQ4X2uioOypYFCLo5Orrj9+uJ+Mfs6vbb9ejK9Gw8OzkDo2BecsMiz+s2d4yx9+1od0I5NHX1CSwYVNXdMO13alSyecUowzfsVGOtbSlXqZdJjy9sMd8GaOv6hC5OoJq/6dvIM5FisKGg8fGVGJtWntxQbOjST1dNhwgB6Nt/SAaOa+5bbzBECYdga4bqTJ1iORT5XXUCg2ITH6jFX1USoqZ/GzjyIb8fqV/f4Q1CbHRLr9dw6EtIQF2Ein84OpBi0mkR1SWpCLkMOGZWlFRG6m8g5GqUs+Geqf1iI89m9QZtPS/Ilco9T8POZdqHRtebmdOpBmxlV2n95Zp99LsP+/LCh9ma7qHfPD0wOeW5Sm3FYfY+iWB+xXZOxsw6lDqbpHCmCccD4UK285HhyVT6uUNOiGiI1Uncu13XwKuc+vhRBrZBU24oE8snThEPR3Jcz+tuOaFQdktoic2p3npYTbYia3QHGb0tWFp5k3TNH8HAAD//5ocqteaCAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: orb-coredns
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2024-12-22T01:30:13Z"
    generation: 1
    labels:
      k8s-app: kube-dns
      kubernetes.io/name: CoreDNS
      objectset.rio.cattle.io/hash: 90462e4597bdcacaa7038840f786450ac707bfa2
    name: coredns
    namespace: kube-system
    resourceVersion: "7642948"
    uid: a4c9c536-4215-4aca-bbd3-621b9edc121b
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 0
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: rancher/mirrored-coredns-coredns:1.10.1
          imagePullPolicy: IfNotPresent
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          resources:
            limits:
              memory: 340Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
          - mountPath: /etc/coredns/custom
            name: custom-config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              k8s-app: kube-dns
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            - key: NodeHosts
              path: NodeHosts
            name: coredns
          name: config-volume
        - configMap:
            defaultMode: 420
            name: coredns-custom
            optional: true
          name: custom-config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-12-22T01:30:24Z"
      lastUpdateTime: "2024-12-22T01:30:24Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-12-22T01:30:24Z"
      lastUpdateTime: "2024-12-22T01:30:32Z"
      message: ReplicaSet "coredns-66cc6945cb" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "4"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xUUW/iOBD+K6d5TgIcLUKR7gG1Pd1pW4padV8qtBqcCbg4tmUP2UYo/301BFqqLe2utE+J7ZnP38z3jbeAXn+lELWzkAN6H3v1ABJYa1tADpfkjWsqsgwJVMRYICPkW0BrHSNrZ6Ms3eKJFEfiLGiXKWQ2lGnX0wICyclz991SSJf1GnJYD+PRST1I/vqibfHPpCic/RTCYkWQg3EKTRrZBVzSLyVFj0oy15sFpbGJTBW0CRhckPmwtBXGFeQwGA/L4bkanZflQg37o7NRf1ieDcvB+bhfjNVojH8XuCjOBPQNSY+8Sn1wtZbmU4Du/ASf6EkJm0Bd/H9aimyudaUZ8n4CkQwpdkGCKmS1un6pAL0/fWsr4ByQadnsLnDGaLt88AUydWDPDxZr1AYXhiAftAlw44Xj3ZtY2afKm0PekVvMb3DZF6qcZdSWQoT8UZZVhWLJx9Pti4xBfJqmytlSLyGBHrHqdav9J3uKzsI8AbL1Dnkvyuz28tt0cnN1P5tcXEECNZoN/RtcJWRKTaa4o/Llf4Ys4h9qzF6Va9t2noCuxH85BLRqRaH3Pue87mf9bCjztkuYbYyZOaNVAzn8X04dzwLFbvg+807tzKaiG7ex3HWskt89z+M2vGJ1G2mXCe1ciFtX0P2RlcSGwRJT3I1NFArabp5FbR+0C5qbC4MxTjvMzrGpwKQqaNYKjUhDodaKJkoJq+lHtaT72BS7YEiAnaFweGwet7AmadDFHn73QMRbaxoZeC+Rwh2unnXkCG2yBSpLUgw5TN29WlGxMfI4dDA7qsEZyt7WKgYMzqTeoKU/ilxh5J1m70DOD0oebC8S3aAXLX62wN7n7WlJ27b9EQAA//8eX0Hn5AUAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: local-storage
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2024-12-22T01:30:13Z"
    generation: 4
    labels:
      objectset.rio.cattle.io/hash: 183f35c65ffbc3064603f43f1580d8c68a2dabd4
    name: local-path-provisioner
    namespace: kube-system
    resourceVersion: "7642964"
    uid: 259c9164-6e13-4270-b02a-993040ddbb3e
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 0
    selector:
      matchLabels:
        app: local-path-provisioner
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
      spec:
        containers:
        - command:
          - local-path-provisioner
          - start
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/local-path-provisioner:v0.0.31
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-12-22T01:30:24Z"
      lastUpdateTime: "2024-12-22T01:30:24Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-12-22T01:30:24Z"
      lastUpdateTime: "2025-11-10T04:43:57Z"
      message: ReplicaSet "local-path-provisioner-777cb94f89" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 4
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"metrics-server"},"name":"metrics-server","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"metrics-server"}},"strategy":{"rollingUpdate":{"maxUnavailable":0}},"template":{"metadata":{"labels":{"k8s-app":"metrics-server"}},"spec":{"containers":[{"args":["--cert-dir=/tmp","--secure-port=10250","--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname","--kubelet-use-node-status-port","--metric-resolution=15s"],"image":"registry.k8s.io/metrics-server/metrics-server:v0.8.0","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/livez","port":"https","scheme":"HTTPS"},"periodSeconds":10},"name":"metrics-server","ports":[{"containerPort":10250,"name":"https","protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/readyz","port":"https","scheme":"HTTPS"},"initialDelaySeconds":20,"periodSeconds":10},"resources":{"requests":{"cpu":"100m","memory":"200Mi"}},"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"drop":["ALL"]},"readOnlyRootFilesystem":true,"runAsNonRoot":true,"runAsUser":1000,"seccompProfile":{"type":"RuntimeDefault"}},"volumeMounts":[{"mountPath":"/tmp","name":"tmp-dir"}]}],"nodeSelector":{"kubernetes.io/os":"linux"},"priorityClassName":"system-cluster-critical","serviceAccountName":"metrics-server","volumes":[{"emptyDir":{},"name":"tmp-dir"}]}}}}
    creationTimestamp: "2025-09-07T02:48:49Z"
    generation: 1
    labels:
      k8s-app: metrics-server
    name: metrics-server
    namespace: kube-system
    resourceVersion: "7643370"
    uid: 90af4325-5cd1-4c14-aaeb-1501abcb87ec
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: metrics-server
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
      spec:
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=10250
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --kubelet-use-node-status-port
          - --metric-resolution=15s
          image: registry.k8s.io/metrics-server/metrics-server:v0.8.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: metrics-server
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-09-07T02:48:49Z"
      lastUpdateTime: "2025-09-07T02:49:25Z"
      message: ReplicaSet "metrics-server-867d48dc9c" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-18T01:37:43Z"
      lastUpdateTime: "2025-11-18T01:37:43Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"clickhouse","component":"langfuse"},"name":"langfuse-clickhouse","namespace":"langfuse"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"clickhouse","component":"langfuse"}},"template":{"metadata":{"labels":{"app":"clickhouse","component":"langfuse"}},"spec":{"containers":[{"env":[{"name":"CLICKHOUSE_DB","value":"langfuse"},{"name":"CLICKHOUSE_USER","value":"langfuse"},{"name":"CLICKHOUSE_PASSWORD","valueFrom":{"secretKeyRef":{"key":"CLICKHOUSE_PASSWORD","name":"langfuse-secrets","optional":true}}},{"name":"CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT","value":"1"}],"image":"clickhouse/clickhouse-server:latest","livenessProbe":{"httpGet":{"path":"/ping","port":8123},"initialDelaySeconds":30,"periodSeconds":10},"name":"clickhouse","ports":[{"containerPort":8123,"name":"http"},{"containerPort":9000,"name":"native"}],"readinessProbe":{"httpGet":{"path":"/ping","port":8123},"initialDelaySeconds":10,"periodSeconds":5},"resources":{"limits":{"cpu":"1000m","memory":"2Gi"},"requests":{"cpu":"100m","memory":"512Mi"}},"volumeMounts":[{"mountPath":"/var/lib/clickhouse","name":"clickhouse-data"}]}],"volumes":[{"emptyDir":{},"name":"clickhouse-data"}]}}}}
    creationTimestamp: "2025-11-08T03:25:56Z"
    generation: 2
    labels:
      app: clickhouse
      component: langfuse
    name: langfuse-clickhouse
    namespace: langfuse
    resourceVersion: "6763243"
    uid: 41fe86c9-9bf2-4769-b21d-0552638a3e85
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: clickhouse
        component: langfuse
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: clickhouse
          component: langfuse
      spec:
        containers:
        - env:
          - name: CLICKHOUSE_DB
            value: langfuse
          - name: CLICKHOUSE_USER
            value: langfuse
          - name: CLICKHOUSE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: CLICKHOUSE_PASSWORD
                name: langfuse-secrets
                optional: true
          - name: CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT
            value: "1"
          image: clickhouse/clickhouse-server:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 8123
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: clickhouse
          ports:
          - containerPort: 8123
            name: http
            protocol: TCP
          - containerPort: 9000
            name: native
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 8123
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/clickhouse
            name: clickhouse-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: clickhouse-data
  status:
    conditions:
    - lastTransitionTime: "2025-11-09T17:23:34Z"
      lastUpdateTime: "2025-11-09T17:23:34Z"
      message: ReplicaSet "langfuse-clickhouse-58b6d94547" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-11T17:14:26Z"
      lastUpdateTime: "2025-11-11T17:14:26Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"litellm"},"name":"litellm-proxy","namespace":"llm-inference"},"spec":{"replicas":2,"selector":{"matchLabels":{"app":"litellm"}},"template":{"metadata":{"labels":{"app":"litellm"}},"spec":{"containers":[{"args":["--config","/app/config.yaml","--port","4000","--num_workers","4"],"command":["litellm"],"image":"ghcr.io/berriai/litellm:main-latest","livenessProbe":{"httpGet":{"path":"/health","port":4000},"initialDelaySeconds":30,"periodSeconds":10},"name":"litellm","ports":[{"containerPort":4000,"name":"http"}],"readinessProbe":{"httpGet":{"path":"/health","port":4000},"initialDelaySeconds":10,"periodSeconds":5},"resources":{"limits":{"cpu":"2000m","memory":"2Gi"},"requests":{"cpu":"500m","memory":"512Mi"}},"volumeMounts":[{"mountPath":"/app","name":"config"}]}],"volumes":[{"configMap":{"name":"litellm-config"},"name":"config"}]}}}}
    creationTimestamp: "2025-11-17T19:58:34Z"
    generation: 2
    labels:
      app: litellm
    name: litellm-proxy
    namespace: llm-inference
    resourceVersion: "7627033"
    uid: 48e2133f-521e-4608-a84a-48747914a30d
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: litellm
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: litellm
      spec:
        containers:
        - args:
          - --config
          - /app/config.yaml
          - --port
          - "4000"
          - --num_workers
          - "4"
          command:
          - litellm
          image: ghcr.io/berriai/litellm:main-latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 4000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: litellm
          ports:
          - containerPort: 4000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 4000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app
            name: config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: litellm-config
          name: config
  status:
    conditions:
    - lastTransitionTime: "2025-11-17T19:58:34Z"
      lastUpdateTime: "2025-11-17T19:58:34Z"
      message: Deployment does not have minimum availability.
      reason: MinimumReplicasUnavailable
      status: "False"
      type: Available
    - lastTransitionTime: "2025-11-17T20:08:35Z"
      lastUpdateTime: "2025-11-17T20:08:35Z"
      message: ReplicaSet "litellm-proxy-7f46445f6c" has timed out progressing.
      reason: ProgressDeadlineExceeded
      status: "False"
      type: Progressing
    observedGeneration: 2
    replicas: 1
    unavailableReplicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"ollama","component":"model-server"},"name":"ollama","namespace":"llm-inference"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"ollama"}},"template":{"metadata":{"labels":{"app":"ollama","component":"model-server"}},"spec":{"containers":[{"env":[{"name":"OLLAMA_HOST","value":"0.0.0.0:11434"},{"name":"OLLAMA_MODELS","value":"/root/.ollama/models"},{"name":"OLLAMA_KEEP_ALIVE","value":"24h"},{"name":"OLLAMA_NUM_PARALLEL","value":"4"},{"name":"OLLAMA_MAX_LOADED_MODELS","value":"3"}],"image":"ollama/ollama:latest","livenessProbe":{"httpGet":{"path":"/","port":11434},"initialDelaySeconds":30,"periodSeconds":10},"name":"ollama","ports":[{"containerPort":11434,"name":"http","protocol":"TCP"}],"readinessProbe":{"httpGet":{"path":"/","port":11434},"initialDelaySeconds":10,"periodSeconds":5},"resources":{"limits":{"cpu":"4000m","memory":"8Gi"},"requests":{"cpu":"2000m","memory":"4Gi"}},"volumeMounts":[{"mountPath":"/root/.ollama","name":"ollama-data"}]}],"volumes":[{"name":"ollama-data","persistentVolumeClaim":{"claimName":"ollama-data"}}]}}}}
    creationTimestamp: "2025-11-17T19:58:34Z"
    generation: 2
    labels:
      app: ollama
      component: model-server
    name: ollama
    namespace: llm-inference
    resourceVersion: "7627155"
    uid: 5af01452-18e8-459e-bf22-96000eaca1c9
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: ollama
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: ollama
          component: model-server
      spec:
        containers:
        - env:
          - name: OLLAMA_HOST
            value: 0.0.0.0:11434
          - name: OLLAMA_MODELS
            value: /root/.ollama/models
          - name: OLLAMA_KEEP_ALIVE
            value: 24h
          - name: OLLAMA_NUM_PARALLEL
            value: "4"
          - name: OLLAMA_MAX_LOADED_MODELS
            value: "3"
          image: ollama/ollama:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 11434
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: ollama
          ports:
          - containerPort: 11434
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 11434
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "4"
              memory: 8Gi
            requests:
              cpu: "2"
              memory: 4Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /root/.ollama
            name: ollama-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: ollama-data
          persistentVolumeClaim:
            claimName: ollama-data
  status:
    conditions:
    - lastTransitionTime: "2025-11-17T23:38:06Z"
      lastUpdateTime: "2025-11-17T23:38:06Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-17T23:38:06Z"
      lastUpdateTime: "2025-11-17T23:38:06Z"
      message: ReplicaSet "ollama-6b9758f96f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"agent-brain","ossa-agent":"true"},"name":"agent-brain","namespace":"llm-platform"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"agent-brain"}},"template":{"metadata":{"annotations":{"prometheus.io/port":"9090","prometheus.io/scrape":"true"},"labels":{"app":"agent-brain"}},"spec":{"containers":[{"env":[{"name":"PORT","value":"3002"},{"name":"NODE_ENV","value":"production"}],"image":"agent-brain:0.1.0","imagePullPolicy":"Never","livenessProbe":{"httpGet":{"path":"/health","port":3002},"initialDelaySeconds":10,"periodSeconds":30},"name":"agent-brain","ports":[{"containerPort":3002,"name":"http"},{"containerPort":9090,"name":"metrics"}],"readinessProbe":{"httpGet":{"path":"/health","port":3002},"initialDelaySeconds":5,"periodSeconds":10},"resources":{"limits":{"cpu":"500m","memory":"512Mi"},"requests":{"cpu":"100m","memory":"256Mi"}}}]}}}}
    creationTimestamp: "2025-10-22T03:49:10Z"
    generation: 6
    labels:
      app: agent-brain
      ossa-agent: "true"
    name: agent-brain
    namespace: llm-platform
    resourceVersion: "5375550"
    uid: 4f0e0716-3e6c-4643-8396-5ba41722a8e9
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: agent-brain
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: agent-brain
      spec:
        containers:
        - env:
          - name: PORT
            value: "3002"
          - name: NODE_ENV
            value: production
          image: agent-brain:0.1.0
          imagePullPolicy: Never
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3002
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: agent-brain
          ports:
          - containerPort: 3002
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3002
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastTransitionTime: "2025-10-22T03:49:10Z"
      lastUpdateTime: "2025-10-22T03:49:50Z"
      message: ReplicaSet "agent-brain-7bc545fd94" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-10-22T05:37:23Z"
      lastUpdateTime: "2025-10-22T05:37:23Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 6
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "5"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"compliance-engine","component":"governance","ossa-agent":"true"},"name":"compliance-engine","namespace":"llm-platform"},"spec":{"replicas":2,"selector":{"matchLabels":{"app":"compliance-engine"}},"template":{"metadata":{"annotations":{"prometheus.io/port":"9090","prometheus.io/scrape":"true"},"labels":{"app":"compliance-engine"}},"spec":{"containers":[{"env":[{"name":"API_KEY","valueFrom":{"secretKeyRef":{"key":"api-key","name":"compliance-engine-secrets"}}},{"name":"NODE_ENV","value":"production"}],"image":"compliance-engine:0.1.0","livenessProbe":{"httpGet":{"path":"/health","port":3100},"initialDelaySeconds":30,"periodSeconds":30},"name":"compliance-engine","ports":[{"containerPort":3100,"name":"http"},{"containerPort":9090,"name":"metrics"}],"readinessProbe":{"httpGet":{"path":"/health","port":3100},"initialDelaySeconds":10,"periodSeconds":10},"resources":{"limits":{"cpu":"1000m","memory":"2Gi"},"requests":{"cpu":"500m","memory":"1Gi"}}}]}}}}
    creationTimestamp: "2025-10-21T20:24:25Z"
    generation: 8
    labels:
      app: compliance-engine
      component: governance
      ossa-agent: "true"
    name: compliance-engine
    namespace: llm-platform
    resourceVersion: "5375553"
    uid: 8beca30b-799e-4dd4-b8d8-5d2a3c9f09a8
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: compliance-engine
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-21T22:33:02-04:00"
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: compliance-engine
      spec:
        containers:
        - env:
          - name: API_KEY
            valueFrom:
              secretKeyRef:
                key: api-key
                name: compliance-engine-secrets
          - name: NODE_ENV
            value: production
          image: compliance-engine:0.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3100
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: compliance-engine
          ports:
          - containerPort: 3100
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3100
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastTransitionTime: "2025-10-22T02:37:40Z"
      lastUpdateTime: "2025-10-22T02:37:40Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-10-22T02:33:02Z"
      lastUpdateTime: "2025-10-22T02:37:53Z"
      message: ReplicaSet "compliance-engine-c644bdc8c" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 8
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "5"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"workflow-engine","component":"orchestration","ossa-agent":"true"},"name":"workflow-engine","namespace":"llm-platform"},"spec":{"replicas":2,"selector":{"matchLabels":{"app":"workflow-engine"}},"template":{"metadata":{"annotations":{"prometheus.io/port":"9090","prometheus.io/scrape":"true"},"labels":{"app":"workflow-engine"}},"spec":{"containers":[{"env":[{"name":"REDIS_URL","value":"redis://redis:6379"},{"name":"API_KEY","valueFrom":{"secretKeyRef":{"key":"api-key","name":"workflow-engine-secrets"}}},{"name":"NODE_ENV","value":"production"}],"image":"workflow-engine:0.1.0","livenessProbe":{"httpGet":{"path":"/health","port":3001},"initialDelaySeconds":30,"periodSeconds":30},"name":"workflow-engine","ports":[{"containerPort":3001,"name":"http"},{"containerPort":9090,"name":"metrics"}],"readinessProbe":{"httpGet":{"path":"/health","port":3001},"initialDelaySeconds":10,"periodSeconds":10},"resources":{"limits":{"cpu":"1000m","memory":"2Gi"},"requests":{"cpu":"500m","memory":"1Gi"}}}]}}}}
    creationTimestamp: "2025-10-21T20:24:19Z"
    generation: 8
    labels:
      app: workflow-engine
      component: orchestration
      ossa-agent: "true"
    name: workflow-engine
    namespace: llm-platform
    resourceVersion: "5375554"
    uid: d5c35285-b0c6-4b6c-ba5b-bc9b5d666f84
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: workflow-engine
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-21T22:33:02-04:00"
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: workflow-engine
      spec:
        containers:
        - env:
          - name: REDIS_URL
            value: redis://redis:6379
          - name: API_KEY
            valueFrom:
              secretKeyRef:
                key: api-key
                name: workflow-engine-secrets
          - name: NODE_ENV
            value: production
          image: workflow-engine:0.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3001
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: workflow-engine
          ports:
          - containerPort: 3001
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3001
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastTransitionTime: "2025-10-22T02:33:02Z"
      lastUpdateTime: "2025-10-22T02:36:39Z"
      message: ReplicaSet "workflow-engine-6854ffbfd8" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-10-22T02:37:38Z"
      lastUpdateTime: "2025-10-22T02:37:38Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 8
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app.kubernetes.io/name":"monitoring-placeholder"},"name":"monitoring-placeholder","namespace":"monitoring"},"spec":{"replicas":1,"selector":{"matchLabels":{"app.kubernetes.io/name":"monitoring-placeholder"}},"template":{"metadata":{"labels":{"app.kubernetes.io/name":"monitoring-placeholder","monitoring":"enabled"}},"spec":{"containers":[{"image":"nginx:alpine","name":"placeholder","ports":[{"containerPort":8080}],"resources":{"limits":{"cpu":"100m","memory":"128Mi"},"requests":{"cpu":"50m","memory":"64Mi"}}}]}}}}
    creationTimestamp: "2025-10-31T19:48:35Z"
    generation: 2
    labels:
      app.kubernetes.io/name: monitoring-placeholder
    name: monitoring-placeholder
    namespace: monitoring
    resourceVersion: "6763267"
    uid: 437d23b7-109b-4cd2-b2c4-57ddc120af40
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/name: monitoring-placeholder
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/name: monitoring-placeholder
          monitoring: enabled
      spec:
        containers:
        - image: nginx:alpine
          imagePullPolicy: IfNotPresent
          name: placeholder
          ports:
          - containerPort: 8080
            protocol: TCP
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastTransitionTime: "2025-10-31T19:48:35Z"
      lastUpdateTime: "2025-10-31T19:48:40Z"
      message: ReplicaSet "monitoring-placeholder-7b7f448b76" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-11T16:57:34Z"
      lastUpdateTime: "2025-11-11T16:57:34Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"neo4j","component":"knowledge-graph"},"name":"neo4j","namespace":"observability"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"neo4j"}},"template":{"metadata":{"labels":{"app":"neo4j","component":"knowledge-graph"}},"spec":{"containers":[{"envFrom":[{"configMapRef":{"name":"neo4j-config"}}],"image":"neo4j:5.24-community","livenessProbe":{"httpGet":{"path":"/","port":7474},"initialDelaySeconds":60,"periodSeconds":20},"name":"neo4j","ports":[{"containerPort":7474,"name":"http"},{"containerPort":7687,"name":"bolt"}],"readinessProbe":{"httpGet":{"path":"/","port":7474},"initialDelaySeconds":30,"periodSeconds":10},"resources":{"limits":{"cpu":"2000m","memory":"4Gi"},"requests":{"cpu":"500m","memory":"2Gi"}},"volumeMounts":[{"mountPath":"/data","name":"neo4j-data"}]}],"volumes":[{"name":"neo4j-data","persistentVolumeClaim":{"claimName":"neo4j-data"}}]}}}}
    creationTimestamp: "2025-10-20T00:31:29Z"
    generation: 6
    labels:
      app: neo4j
      component: knowledge-graph
    name: neo4j
    namespace: observability
    resourceVersion: "6762110"
    uid: ff7f013d-2bdf-4af7-989d-348bd37ed774
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: neo4j
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-30T08:52:46-04:00"
        creationTimestamp: null
        labels:
          app: neo4j
          component: knowledge-graph
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: neo4j-config
          image: neo4j:5.24-community
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 7474
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 1
          name: neo4j
          ports:
          - containerPort: 7474
            name: http
            protocol: TCP
          - containerPort: 7687
            name: bolt
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 7474
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: neo4j-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: neo4j-data
          persistentVolumeClaim:
            claimName: neo4j-data
  status:
    conditions:
    - lastTransitionTime: "2025-10-30T13:03:26Z"
      lastUpdateTime: "2025-10-30T13:07:05Z"
      message: ReplicaSet "neo4j-666b4c8f49" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-11T17:30:19Z"
      lastUpdateTime: "2025-11-11T17:30:19Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 6
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"phoenix"},"name":"phoenix","namespace":"observability"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"phoenix"}},"template":{"metadata":{"labels":{"app":"phoenix"}},"spec":{"containers":[{"env":[{"name":"PHOENIX_WORKING_DIR","value":"/phoenix-data"},{"name":"PHOENIX_PORT","value":"6006"},{"name":"PHOENIX_GRPC_PORT","value":"4317"},{"name":"PHOENIX_SQL_DATABASE_URL","value":"sqlite:////phoenix-data/phoenix.db"}],"image":"arizephoenix/phoenix:latest","livenessProbe":{"httpGet":{"path":"/","port":6006},"initialDelaySeconds":30,"periodSeconds":10},"name":"phoenix","ports":[{"containerPort":6006,"name":"http","protocol":"TCP"},{"containerPort":4317,"name":"otlp-grpc","protocol":"TCP"},{"containerPort":4318,"name":"otlp-http","protocol":"TCP"}],"readinessProbe":{"httpGet":{"path":"/","port":6006},"initialDelaySeconds":10,"periodSeconds":5},"resources":{"limits":{"cpu":"1000m","memory":"2Gi"},"requests":{"cpu":"250m","memory":"512Mi"}},"volumeMounts":[{"mountPath":"/phoenix-data","name":"phoenix-data"}]}],"volumes":[{"name":"phoenix-data","persistentVolumeClaim":{"claimName":"phoenix-data"}}]}}}}
    creationTimestamp: "2025-10-28T14:28:29Z"
    generation: 2
    labels:
      app: phoenix
    name: phoenix
    namespace: observability
    resourceVersion: "6763255"
    uid: 194a71b0-1b18-46a4-906a-0c7a6a0bc835
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: phoenix
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: phoenix
      spec:
        containers:
        - env:
          - name: PHOENIX_WORKING_DIR
            value: /phoenix-data
          - name: PHOENIX_PORT
            value: "6006"
          - name: PHOENIX_GRPC_PORT
            value: "4317"
          - name: PHOENIX_SQL_DATABASE_URL
            value: sqlite:////phoenix-data/phoenix.db
          image: arizephoenix/phoenix:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 6006
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: phoenix
          ports:
          - containerPort: 6006
            name: http
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 6006
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /phoenix-data
            name: phoenix-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: phoenix-data
          persistentVolumeClaim:
            claimName: phoenix-data
  status:
    conditions:
    - lastTransitionTime: "2025-10-28T14:28:29Z"
      lastUpdateTime: "2025-10-28T14:29:46Z"
      message: ReplicaSet "phoenix-7c7c99f584" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-11T17:14:17Z"
      lastUpdateTime: "2025-11-11T17:14:17Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "12"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"agent-router","component":"gateway","tier":"api","version":"v0.1.0-gitlab"},"name":"agent-router","namespace":"ossa-agents"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"agent-router"}},"strategy":{"rollingUpdate":{"maxSurge":1,"maxUnavailable":0},"type":"RollingUpdate"},"template":{"metadata":{"annotations":{"kubectl.kubernetes.io/restartedAt":"2025-10-10T15:00:00Z","prometheus.io/path":"/metrics","prometheus.io/port":"3000","prometheus.io/scrape":"true"},"labels":{"app":"agent-router","component":"gateway","tier":"api","version":"v0.1.0-gitlab"}},"spec":{"containers":[{"env":[{"name":"NODE_ENV","value":"production"},{"name":"PORT","value":"3000"},{"name":"LOG_LEVEL","value":"info"},{"name":"METRICS_ENABLED","value":"true"},{"name":"CORS_ORIGIN","value":"*"},{"name":"PHOENIX_ENABLED","value":"true"},{"name":"PHOENIX_ENDPOINT","value":"http://192.168.139.2:6006"},{"name":"PHOENIX_OTLP_ENDPOINT","value":"http://192.168.139.2:6006/v1/traces"},{"name":"OTEL_EXPORTER_OTLP_ENDPOINT","value":"http://192.168.139.2:6006/v1/traces"},{"name":"OTEL_SERVICE_NAME","value":"agent-router"},{"name":"PHOENIX_PROJECT_NAME","value":"agent-router"},{"name":"PHOENIX_TELEMETRY","value":"true"},{"name":"PHOENIX_OTLP_TIMEOUT","value":"30000"},{"name":"GITLAB_URL","valueFrom":{"configMapKeyRef":{"key":"GITLAB_URL","name":"agent-router-gitlab-config"}}},{"name":"GITLAB_DEFAULT_PROJECT_ID","valueFrom":{"configMapKeyRef":{"key":"GITLAB_DEFAULT_PROJECT_ID","name":"agent-router-gitlab-config"}}},{"name":"GITLAB_DEFAULT_LABELS","valueFrom":{"configMapKeyRef":{"key":"GITLAB_DEFAULT_LABELS","name":"agent-router-gitlab-config"}}},{"name":"GITLAB_AUTO_TRIAGE","valueFrom":{"configMapKeyRef":{"key":"GITLAB_AUTO_TRIAGE","name":"agent-router-gitlab-config"}}},{"name":"GITLAB_AUTO_ASSIGN","valueFrom":{"configMapKeyRef":{"key":"GITLAB_AUTO_ASSIGN","name":"agent-router-gitlab-config"}}},{"name":"GITLAB_TOKEN","valueFrom":{"secretKeyRef":{"key":"GITLAB_TOKEN","name":"agent-router-gitlab-secret"}}}],"image":"agent-router:latest","imagePullPolicy":"Never","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/health","port":3000},"initialDelaySeconds":30,"periodSeconds":10,"timeoutSeconds":3},"name":"agent-router","ports":[{"containerPort":3000,"name":"http","protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/health","port":3000},"initialDelaySeconds":10,"periodSeconds":5,"timeoutSeconds":3},"resources":{"limits":{"cpu":"500m","memory":"512Mi"},"requests":{"cpu":"250m","memory":"256Mi"}}}],"securityContext":{"fsGroup":1000,"runAsNonRoot":false},"terminationGracePeriodSeconds":30}}}}
    creationTimestamp: "2025-10-10T15:47:59Z"
    generation: 13
    labels:
      app: agent-router
      component: gateway
      tier: api
      version: v0.1.0-gitlab
    name: agent-router
    namespace: ossa-agents
    resourceVersion: "6175927"
    uid: c13c9431-a120-4e14-9aad-13b8513a3ecf
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: agent-router
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-10T13:53:42-04:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "3000"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: agent-router
          component: gateway
          tier: api
          version: v0.1.0-gitlab
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: development
          - name: PORT
            value: "3000"
          - name: LOG_LEVEL
            value: info
          - name: METRICS_ENABLED
            value: "true"
          - name: CORS_ORIGIN
            value: '*'
          - name: PHOENIX_ENABLED
            value: "true"
          - name: PHOENIX_ENDPOINT
            value: http://192.168.139.2:6006
          - name: PHOENIX_OTLP_ENDPOINT
            value: http://192.168.139.2:6006/v1/traces
          - name: OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://192.168.139.2:6006/v1/traces
          - name: OTEL_SERVICE_NAME
            value: agent-router
          - name: PHOENIX_PROJECT_NAME
            value: agent-router
          - name: PHOENIX_TELEMETRY
            value: "true"
          - name: PHOENIX_OTLP_TIMEOUT
            value: "30000"
          - name: GITLAB_URL
            valueFrom:
              configMapKeyRef:
                key: GITLAB_URL
                name: agent-router-gitlab-config
          - name: GITLAB_DEFAULT_PROJECT_ID
            valueFrom:
              configMapKeyRef:
                key: GITLAB_DEFAULT_PROJECT_ID
                name: agent-router-gitlab-config
          - name: GITLAB_DEFAULT_LABELS
            valueFrom:
              configMapKeyRef:
                key: GITLAB_DEFAULT_LABELS
                name: agent-router-gitlab-config
          - name: GITLAB_AUTO_TRIAGE
            valueFrom:
              configMapKeyRef:
                key: GITLAB_AUTO_TRIAGE
                name: agent-router-gitlab-config
          - name: GITLAB_AUTO_ASSIGN
            valueFrom:
              configMapKeyRef:
                key: GITLAB_AUTO_ASSIGN
                name: agent-router-gitlab-config
          - name: GITLAB_TOKEN
            valueFrom:
              secretKeyRef:
                key: GITLAB_TOKEN
                name: agent-router-gitlab-secret
          - name: PHOENIX_COLLECTOR_ENDPOINT
            value: http://phoenix.observability.svc.cluster.local:4318/v1/traces
          image: agent-router:latest
          imagePullPolicy: Never
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: agent-router
          ports:
          - containerPort: 3000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 250m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: false
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastTransitionTime: "2025-10-10T15:47:59Z"
      lastUpdateTime: "2025-10-28T14:44:29Z"
      message: ReplicaSet "agent-router-9d69cdc6c" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-04T03:28:47Z"
      lastUpdateTime: "2025-11-04T03:28:47Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 13
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"grafana"},"name":"grafana","namespace":"ossa-agents"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"grafana"}},"template":{"metadata":{"labels":{"app":"grafana"}},"spec":{"containers":[{"env":[{"name":"GF_SECURITY_ADMIN_PASSWORD","value":"ossa-admin"},{"name":"GF_SECURITY_ADMIN_USER","value":"admin"},{"name":"GF_USERS_ALLOW_SIGN_UP","value":"false"},{"name":"GF_AUTH_ANONYMOUS_ENABLED","value":"false"}],"image":"grafana/grafana:latest","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":5,"httpGet":{"path":"/api/health","port":3000},"initialDelaySeconds":60,"periodSeconds":10,"timeoutSeconds":5},"name":"grafana","ports":[{"containerPort":3000,"name":"http","protocol":"TCP"}],"readinessProbe":{"failureThreshold":5,"httpGet":{"path":"/api/health","port":3000},"initialDelaySeconds":30,"periodSeconds":5,"timeoutSeconds":5},"resources":{"limits":{"cpu":"100m","memory":"256Mi"},"requests":{"cpu":"50m","memory":"128Mi"}},"volumeMounts":[{"mountPath":"/var/lib/grafana","name":"grafana-storage"}]}],"restartPolicy":"Always","volumes":[{"emptyDir":{},"name":"grafana-storage"}]}}}}
    creationTimestamp: "2025-11-04T01:15:32Z"
    generation: 3
    labels:
      app: grafana
    name: grafana
    namespace: ossa-agents
    resourceVersion: "7558537"
    uid: 20448d4a-9ba8-4f45-9d7f-ba13bf0865f2
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: grafana
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-11-16T18:27:35-05:00"
        creationTimestamp: null
        labels:
          app: grafana
      spec:
        containers:
        - env:
          - name: GF_SECURITY_ADMIN_PASSWORD
            value: ossa-admin
          - name: GF_SECURITY_ADMIN_USER
            value: admin
          - name: GF_USERS_ALLOW_SIGN_UP
            value: "false"
          - name: GF_AUTH_ANONYMOUS_ENABLED
            value: "false"
          image: grafana/grafana:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: grafana
          ports:
          - containerPort: 3000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 100m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: grafana-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: grafana-storage
  status:
    conditions:
    - lastTransitionTime: "2025-11-04T01:15:32Z"
      lastUpdateTime: "2025-11-16T23:28:18Z"
      message: ReplicaSet "grafana-855c5b68c4" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-17T04:44:27Z"
      lastUpdateTime: "2025-11-17T04:44:27Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "22"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"prometheus"},"name":"prometheus","namespace":"ossa-agents"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"prometheus"}},"template":{"metadata":{"labels":{"app":"prometheus"}},"spec":{"containers":[{"args":["--config.file=/etc/prometheus/prometheus.yml","--storage.tsdb.path=/prometheus","--web.console.libraries=/etc/prometheus/console_libraries","--web.console.templates=/etc/prometheus/consoles","--storage.tsdb.retention.time=15d","--web.enable-lifecycle"],"image":"prom/prometheus:latest","livenessProbe":{"httpGet":{"path":"/-/healthy","port":9090},"initialDelaySeconds":30,"periodSeconds":10},"name":"prometheus","ports":[{"containerPort":9090}],"readinessProbe":{"httpGet":{"path":"/-/ready","port":9090},"initialDelaySeconds":5,"periodSeconds":5},"resources":{"limits":{"cpu":"1000m","memory":"2Gi"},"requests":{"cpu":"100m","memory":"256Mi"}},"volumeMounts":[{"mountPath":"/etc/prometheus","name":"prometheus-config","readOnly":true},{"mountPath":"/prometheus","name":"prometheus-data"}]}],"volumes":[{"configMap":{"name":"prometheus-config"},"name":"prometheus-config"},{"name":"prometheus-data","persistentVolumeClaim":{"claimName":"prometheus-pvc"}}]}}}}
    creationTimestamp: "2025-09-09T21:04:07Z"
    generation: 27
    labels:
      app: prometheus
    name: prometheus
    namespace: ossa-agents
    resourceVersion: "7558536"
    uid: 84f8f118-41c5-4fb8-a158-fb38c3e3c8b2
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: prometheus
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-30T18:07:04-04:00"
        creationTimestamp: null
        labels:
          app: prometheus
      spec:
        containers:
        - args:
          - --config.file=/etc/prometheus/prometheus.yml
          - --storage.tsdb.path=/prometheus
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --storage.tsdb.retention.time=15d
          - --web.enable-lifecycle
          image: prom/prometheus:v2.45.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 10m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/prometheus
            name: prometheus-config
            readOnly: true
          - mountPath: /prometheus
            name: prometheus-data
          - mountPath: /etc/prometheus/rules
            name: prometheus-alerts
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-config
          name: prometheus-config
        - name: prometheus-data
          persistentVolumeClaim:
            claimName: prometheus-pvc
        - configMap:
            defaultMode: 420
            name: prometheus-alerts
          name: prometheus-alerts
  status:
    conditions:
    - lastTransitionTime: "2025-10-31T13:41:16Z"
      lastUpdateTime: "2025-11-04T03:05:30Z"
      message: ReplicaSet "prometheus-7666cd7768" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-17T04:43:59Z"
      lastUpdateTime: "2025-11-17T04:43:59Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 27
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"qdrant"},"name":"qdrant","namespace":"ossa-agents"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"qdrant"}},"template":{"metadata":{"labels":{"app":"qdrant"}},"spec":{"containers":[{"env":[{"name":"QDRANT__SERVICE__HTTP_PORT","value":"6333"},{"name":"QDRANT__SERVICE__GRPC_PORT","value":"6334"}],"image":"qdrant/qdrant:latest","livenessProbe":{"httpGet":{"path":"/health","port":6333},"initialDelaySeconds":30,"periodSeconds":10},"name":"qdrant","ports":[{"containerPort":6333},{"containerPort":6334}],"readinessProbe":{"httpGet":{"path":"/health","port":6333},"initialDelaySeconds":5,"periodSeconds":5},"resources":{"limits":{"cpu":"500m","memory":"512Mi"},"requests":{"cpu":"100m","memory":"256Mi"}}}]}}}}
    creationTimestamp: "2025-10-06T23:37:30Z"
    generation: 4
    labels:
      app: qdrant
    name: qdrant
    namespace: ossa-agents
    resourceVersion: "6762124"
    uid: 89374767-7169-43cf-98f9-c4f093750220
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: qdrant
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: qdrant
      spec:
        containers:
        - env:
          - name: QDRANT__SERVICE__HTTP_PORT
            value: "6333"
          - name: QDRANT__SERVICE__GRPC_PORT
            value: "6334"
          image: qdrant/qdrant:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 6333
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: qdrant
          ports:
          - containerPort: 6333
            protocol: TCP
          - containerPort: 6334
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 6333
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastTransitionTime: "2025-10-06T23:37:30Z"
      lastUpdateTime: "2025-10-06T23:41:53Z"
      message: ReplicaSet "qdrant-7ff67cbff5" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-11T17:30:19Z"
      lastUpdateTime: "2025-11-11T17:30:19Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 4
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"deployment.kubernetes.io/revision":"1"},"creationTimestamp":"2025-10-07T14:51:23Z","generation":1,"labels":{"app":"agent-gateway","version":"0.2.0"},"name":"agent-gateway","namespace":"ossa-prod","resourceVersion":"5067078","uid":"d6ecba74-58ca-4521-9ac6-75eccd815f4b"},"spec":{"progressDeadlineSeconds":600,"replicas":1,"revisionHistoryLimit":10,"selector":{"matchLabels":{"app":"agent-gateway"}},"strategy":{"rollingUpdate":{"maxSurge":"25%","maxUnavailable":"25%"},"type":"RollingUpdate"},"template":{"metadata":{"creationTimestamp":null,"labels":{"app":"agent-gateway","version":"0.2.0"}},"spec":{"containers":[{"command":["node","-e","require('http').createServer((req,res)=\u003eres.end(JSON.stringify({status:'healthy',service:'agent-gateway',agents:129}))).listen(3000)"],"image":"node:20-alpine","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/","port":3000,"scheme":"HTTP"},"initialDelaySeconds":5,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":1},"name":"agent-gateway","ports":[{"containerPort":3000,"protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/","port":3000,"scheme":"HTTP"},"initialDelaySeconds":3,"periodSeconds":5,"successThreshold":1,"timeoutSeconds":1},"resources":{},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File"}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"terminationGracePeriodSeconds":30}}},"status":{"availableReplicas":1,"conditions":[{"lastTransitionTime":"2025-10-07T14:51:23Z","lastUpdateTime":"2025-10-07T14:51:40Z","message":"ReplicaSet \"agent-gateway-85bd86db59\" has successfully progressed.","reason":"NewReplicaSetAvailable","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-10-13T11:29:32Z","lastUpdateTime":"2025-10-13T11:29:32Z","message":"Deployment has minimum availability.","reason":"MinimumReplicasAvailable","status":"True","type":"Available"}],"observedGeneration":1,"readyReplicas":1,"replicas":1,"updatedReplicas":1}}
    creationTimestamp: "2025-10-07T14:51:23Z"
    generation: 4
    labels:
      app: agent-gateway
      version: 0.2.0
    name: agent-gateway
    namespace: ossa-prod
    resourceVersion: "7643029"
    uid: d6ecba74-58ca-4521-9ac6-75eccd815f4b
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: agent-gateway
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: agent-gateway
          version: 0.2.0
      spec:
        containers:
        - command:
          - node
          - -e
          - require('http').createServer((req,res)=>res.end(JSON.stringify({status:'healthy',service:'agent-gateway',agents:129}))).listen(3000)
          env:
          - name: PHOENIX_COLLECTOR_ENDPOINT
            value: http://phoenix.observability.svc.cluster.local:4318/v1/traces
          - name: NODE_ENV
            value: development
          image: node:20-alpine
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: agent-gateway
          ports:
          - containerPort: 3000
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-11T17:14:17Z"
      lastUpdateTime: "2025-11-11T17:14:17Z"
      message: ReplicaSet "agent-gateway-64d665d798" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-18T01:37:24Z"
      lastUpdateTime: "2025-11-18T01:37:24Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 4
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"phoenix","namespace":"phoenix"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"phoenix"}},"template":{"metadata":{"labels":{"app":"phoenix"}},"spec":{"containers":[{"env":[{"name":"PHOENIX_WORKING_DIR","value":"/data"},{"name":"PHOENIX_PORT","value":"6006"},{"name":"PHOENIX_GRPC_PORT","value":"4317"},{"name":"PHOENIX_HOST","value":"0.0.0.0"},{"name":"PHOENIX_COLLECTOR_ENDPOINT","value":"https://api.arize.com/v1"},{"name":"ARIZE_API_KEY","valueFrom":{"secretKeyRef":{"key":"api-key","name":"arize-credentials","optional":true}}}],"image":"arizephoenix/phoenix:latest","livenessProbe":{"httpGet":{"path":"/","port":6006},"initialDelaySeconds":30,"periodSeconds":10},"name":"phoenix","ports":[{"containerPort":6006,"name":"http"},{"containerPort":4317,"name":"otlp-grpc"},{"containerPort":4318,"name":"otlp-http"}],"readinessProbe":{"httpGet":{"path":"/","port":6006},"initialDelaySeconds":10,"periodSeconds":5},"resources":{"limits":{"cpu":"2000m","memory":"4Gi"},"requests":{"cpu":"500m","memory":"1Gi"}},"volumeMounts":[{"mountPath":"/data","name":"data"}]}],"volumes":[{"name":"data","persistentVolumeClaim":{"claimName":"phoenix-data"}}]}}}}
    creationTimestamp: "2025-10-09T20:34:12Z"
    generation: 3
    name: phoenix
    namespace: phoenix
    resourceVersion: "6762135"
    uid: 8df943f2-88c9-461b-b721-bfcc0073ee23
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: phoenix
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: phoenix
      spec:
        containers:
        - env:
          - name: PHOENIX_WORKING_DIR
            value: /data
          - name: PHOENIX_PORT
            value: "6006"
          - name: PHOENIX_GRPC_PORT
            value: "4317"
          - name: PHOENIX_HOST
            value: 0.0.0.0
          - name: ARIZE_API_KEY
            valueFrom:
              secretKeyRef:
                key: api-key
                name: arize-credentials
                optional: true
          image: arizephoenix/phoenix:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 6006
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: phoenix
          ports:
          - containerPort: 6006
            name: http
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 6006
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: data
          persistentVolumeClaim:
            claimName: phoenix-data
  status:
    conditions:
    - lastTransitionTime: "2025-10-09T20:34:12Z"
      lastUpdateTime: "2025-10-10T14:38:55Z"
      message: ReplicaSet "phoenix-6fb5f4d55" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-11T17:30:20Z"
      lastUpdateTime: "2025-11-11T17:30:20Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"aiflow-agent","managed-by":"agent-buildkit","ossa.role":"chat","ossa.version":"1.0"},"name":"aiflow-agent","namespace":"social-agents"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"aiflow-agent"}},"template":{"metadata":{"annotations":{"prometheus.io/path":"/metrics","prometheus.io/port":"9090","prometheus.io/scrape":"true"},"labels":{"app":"aiflow-agent"}},"spec":{"containers":[{"env":[{"name":"CHARACTER_FILE","value":"/app/characters/AIFlow.json"},{"name":"DB_PATH","valueFrom":{"secretKeyRef":{"key":"db-connection-string","name":"aiflow-secrets"}}},{"name":"AIFLOW_API_KEY","valueFrom":{"secretKeyRef":{"key":"api-key","name":"aiflow-secrets"}}},{"name":"TW_API_KEY","valueFrom":{"secretKeyRef":{"key":"twitter-api-key","name":"aiflow-secrets","optional":true}}},{"name":"TW_API_KEY_SECRET","valueFrom":{"secretKeyRef":{"key":"twitter-api-key-secret","name":"aiflow-secrets","optional":true}}},{"name":"TW_ACCESS_TOKEN","valueFrom":{"secretKeyRef":{"key":"twitter-access-token","name":"aiflow-secrets","optional":true}}},{"name":"TW_ACCESS_TOKEN_SECRET","valueFrom":{"secretKeyRef":{"key":"twitter-access-token-secret","name":"aiflow-secrets","optional":true}}},{"name":"TW_BEARER_TOKEN","valueFrom":{"secretKeyRef":{"key":"twitter-bearer-token","name":"aiflow-secrets","optional":true}}},{"name":"TG_BOT_TOKEN","valueFrom":{"secretKeyRef":{"key":"telegram-bot-token","name":"aiflow-secrets","optional":true}}},{"name":"ANTHROPIC_API_KEY","valueFrom":{"secretKeyRef":{"key":"anthropic-api-key","name":"aiflow-secrets","optional":true}}},{"name":"OPENAI_API_KEY","valueFrom":{"secretKeyRef":{"key":"openai-api-key","name":"aiflow-secrets","optional":true}}}],"image":"aiflow/agent:1.0.0","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/health","port":8000},"initialDelaySeconds":30,"periodSeconds":30,"timeoutSeconds":5},"name":"aiflow","ports":[{"containerPort":8000,"name":"http","protocol":"TCP"},{"containerPort":9090,"name":"metrics","protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/health","port":8000},"initialDelaySeconds":10,"periodSeconds":10,"timeoutSeconds":5},"resources":{"limits":{"cpu":"2000m","memory":"4Gi"},"requests":{"cpu":"1000m","memory":"2Gi"}},"volumeMounts":[{"mountPath":"/app/characters","name":"character-config","readOnly":true}]}],"volumes":[{"configMap":{"name":"aiflow-character-config"},"name":"character-config"}]}}}}
    creationTimestamp: "2025-10-21T20:24:39Z"
    generation: 3
    labels:
      app: aiflow-agent
      managed-by: agent-buildkit
      ossa.role: chat
      ossa.version: "1.0"
    name: aiflow-agent
    namespace: social-agents
    resourceVersion: "5375687"
    uid: 3f9219d6-0f03-41c1-b050-a3cd06a55ffe
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: aiflow-agent
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-21T17:47:36-04:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: aiflow-agent
      spec:
        containers:
        - env:
          - name: CHARACTER_FILE
            value: /app/characters/AIFlow.json
          - name: DB_PATH
            valueFrom:
              secretKeyRef:
                key: db-connection-string
                name: aiflow-secrets
          - name: AIFLOW_API_KEY
            valueFrom:
              secretKeyRef:
                key: api-key
                name: aiflow-secrets
          - name: TW_API_KEY
            valueFrom:
              secretKeyRef:
                key: twitter-api-key
                name: aiflow-secrets
                optional: true
          - name: TW_API_KEY_SECRET
            valueFrom:
              secretKeyRef:
                key: twitter-api-key-secret
                name: aiflow-secrets
                optional: true
          - name: TW_ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                key: twitter-access-token
                name: aiflow-secrets
                optional: true
          - name: TW_ACCESS_TOKEN_SECRET
            valueFrom:
              secretKeyRef:
                key: twitter-access-token-secret
                name: aiflow-secrets
                optional: true
          - name: TW_BEARER_TOKEN
            valueFrom:
              secretKeyRef:
                key: twitter-bearer-token
                name: aiflow-secrets
                optional: true
          - name: TG_BOT_TOKEN
            valueFrom:
              secretKeyRef:
                key: telegram-bot-token
                name: aiflow-secrets
                optional: true
          - name: ANTHROPIC_API_KEY
            valueFrom:
              secretKeyRef:
                key: anthropic-api-key
                name: aiflow-secrets
                optional: true
          - name: OPENAI_API_KEY
            valueFrom:
              secretKeyRef:
                key: openai-api-key
                name: aiflow-secrets
                optional: true
          image: aiflow/agent:1.0.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: aiflow
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/characters
            name: character-config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: aiflow-character-config
          name: character-config
  status:
    conditions:
    - lastTransitionTime: "2025-10-22T05:37:28Z"
      lastUpdateTime: "2025-10-22T05:37:28Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-10-22T05:37:28Z"
      lastUpdateTime: "2025-10-22T05:37:28Z"
      message: ReplicaSet "aiflow-agent-7cd976d77b" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 3
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-07T15:48:07Z"
    generation: 1
    labels:
      app: frontend
      pod-template-hash: 7ccd54b7ff
    name: drupal-frontend-7ccd54b7ff
    namespace: agent-chat
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: drupal-frontend
      uid: ba12b426-1941-4a4b-92c9-f6f1217d96c1
    resourceVersion: "7643565"
    uid: 90387c1b-362d-4d74-9184-4fc0947ccb9e
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: frontend
        pod-template-hash: 7ccd54b7ff
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: frontend
          pod-template-hash: 7ccd54b7ff
      spec:
        containers:
        - env:
          - name: DRUPAL_DATABASE_HOST
            value: postgres.agent-chat.local.bluefly.io
          - name: DRUPAL_DATABASE_PORT
            value: "5432"
          - name: DRUPAL_DATABASE_NAME
            value: llm_platform
          - name: DRUPAL_DATABASE_USERNAME
            value: llm_user
          - name: DRUPAL_DATABASE_PASSWORD
            value: llm_password
          image: drupal:10-apache
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 80
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: drupal
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 80
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/www/html/sites/default/files
            name: drupal-files
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: drupal-files
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "6"
    creationTimestamp: "2025-11-10T04:43:52Z"
    generation: 2
    labels:
      app: librechat
      pod-template-hash: 568f776678
    name: librechat-568f776678
    namespace: agent-chat
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: librechat
      uid: 6dc728b1-0d13-45c6-a695-99dfeb74ae2f
    resourceVersion: "7383434"
    uid: 9e18a6d7-ebc1-4b32-8d61-f57b470bee74
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: librechat
        pod-template-hash: 568f776678
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-31T07:14:06-04:00"
        creationTimestamp: null
        labels:
          app: librechat
          pod-template-hash: 568f776678
      spec:
        containers:
        - env:
          - name: PHOENIX_COLLECTOR_ENDPOINT
            value: http://phoenix.observability.svc.cluster.local:4318/v1/traces
          - name: NODE_ENV
            value: development
          envFrom:
          - configMapRef:
              name: librechat-config
          image: ghcr.io/danny-avila/librechat:latest
          imagePullPolicy: Always
          name: librechat
          ports:
          - containerPort: 3080
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/uploads
            name: librechat-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: librechat-storage
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-10-04T16:02:19Z"
    generation: 2
    labels:
      app: librechat
      pod-template-hash: 5697d76f96
    name: librechat-5697d76f96
    namespace: agent-chat
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: librechat
      uid: 6dc728b1-0d13-45c6-a695-99dfeb74ae2f
    resourceVersion: "4628387"
    uid: cb6ae36a-c62a-437f-8fd5-1a4281c7313c
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: librechat
        pod-template-hash: 5697d76f96
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-04T12:02:19-04:00"
        creationTimestamp: null
        labels:
          app: librechat
          pod-template-hash: 5697d76f96
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: librechat-config
          image: ghcr.io/danny-avila/librechat:latest
          imagePullPolicy: Always
          name: librechat
          ports:
          - containerPort: 3080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/uploads
            name: librechat-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: librechat-storage
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "5"
    creationTimestamp: "2025-10-31T11:14:07Z"
    generation: 2
    labels:
      app: librechat
      pod-template-hash: 5ddb54c768
    name: librechat-5ddb54c768
    namespace: agent-chat
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: librechat
      uid: 6dc728b1-0d13-45c6-a695-99dfeb74ae2f
    resourceVersion: "6722682"
    uid: 742c26e6-1760-49b2-92b8-92b7aa0154ad
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: librechat
        pod-template-hash: 5ddb54c768
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-31T07:14:06-04:00"
        creationTimestamp: null
        labels:
          app: librechat
          pod-template-hash: 5ddb54c768
      spec:
        containers:
        - env:
          - name: PHOENIX_COLLECTOR_ENDPOINT
            value: http://phoenix.observability.svc.cluster.local:4318/v1/traces
          - name: NODE_ENV
            value: development
          envFrom:
          - configMapRef:
              name: librechat-config
          image: ghcr.io/danny-avila/librechat:latest
          imagePullPolicy: Always
          name: librechat
          ports:
          - containerPort: 3080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/uploads
            name: librechat-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: librechat-storage
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "7"
    creationTimestamp: "2025-11-16T12:14:30Z"
    generation: 2
    labels:
      app: librechat
      pod-template-hash: 6b6667b7d4
    name: librechat-6b6667b7d4
    namespace: agent-chat
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: librechat
      uid: 6dc728b1-0d13-45c6-a695-99dfeb74ae2f
    resourceVersion: "7560360"
    uid: 47ae9322-30a1-42c9-8e47-56cf0a6ebb6e
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: librechat
        pod-template-hash: 6b6667b7d4
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-11-16T07:14:30-05:00"
        creationTimestamp: null
        labels:
          app: librechat
          pod-template-hash: 6b6667b7d4
      spec:
        containers:
        - env:
          - name: PHOENIX_COLLECTOR_ENDPOINT
            value: http://phoenix.observability.svc.cluster.local:4318/v1/traces
          - name: NODE_ENV
            value: development
          envFrom:
          - configMapRef:
              name: librechat-config
          image: ghcr.io/danny-avila/librechat:latest
          imagePullPolicy: Always
          name: librechat
          ports:
          - containerPort: 3080
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/uploads
            name: librechat-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: librechat-storage
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2025-10-28T14:44:17Z"
    generation: 2
    labels:
      app: librechat
      pod-template-hash: 764bdd6bfd
    name: librechat-764bdd6bfd
    namespace: agent-chat
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: librechat
      uid: 6dc728b1-0d13-45c6-a695-99dfeb74ae2f
    resourceVersion: "5922218"
    uid: 0e72069b-de81-4e93-a333-72ba10bf8eb0
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: librechat
        pod-template-hash: 764bdd6bfd
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-04T12:06:16-04:00"
        creationTimestamp: null
        labels:
          app: librechat
          pod-template-hash: 764bdd6bfd
      spec:
        containers:
        - env:
          - name: PHOENIX_COLLECTOR_ENDPOINT
            value: http://phoenix.observability.svc.cluster.local:4318/v1/traces
          - name: NODE_ENV
            value: development
          envFrom:
          - configMapRef:
              name: librechat-config
          image: ghcr.io/danny-avila/librechat:latest
          imagePullPolicy: Always
          name: librechat
          ports:
          - containerPort: 3080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/uploads
            name: librechat-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: librechat-storage
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2025-10-04T16:06:16Z"
    generation: 2
    labels:
      app: librechat
      pod-template-hash: 766768f689
    name: librechat-766768f689
    namespace: agent-chat
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: librechat
      uid: 6dc728b1-0d13-45c6-a695-99dfeb74ae2f
    resourceVersion: "5744174"
    uid: 8a641cfe-89a4-4594-a5f2-378a5a53eb1a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: librechat
        pod-template-hash: 766768f689
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-04T12:06:16-04:00"
        creationTimestamp: null
        labels:
          app: librechat
          pod-template-hash: 766768f689
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: librechat-config
          image: ghcr.io/danny-avila/librechat:latest
          imagePullPolicy: Always
          name: librechat
          ports:
          - containerPort: 3080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/uploads
            name: librechat-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: librechat-storage
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-04T15:48:21Z"
    generation: 2
    labels:
      app: librechat
      pod-template-hash: 856b56d57f
    name: librechat-856b56d57f
    namespace: agent-chat
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: librechat
      uid: 6dc728b1-0d13-45c6-a695-99dfeb74ae2f
    resourceVersion: "4628172"
    uid: 72ba9b9c-deba-4fa3-88eb-f0bc5bfb300e
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: librechat
        pod-template-hash: 856b56d57f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: librechat
          pod-template-hash: 856b56d57f
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: librechat-config
          image: ghcr.io/danny-avila/librechat:latest
          imagePullPolicy: Always
          name: librechat
          ports:
          - containerPort: 3080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/uploads
            name: librechat-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: librechat-storage
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-11-10T04:43:52Z"
    generation: 2
    labels:
      app: mongo
      pod-template-hash: 74c6b77b75
    name: mongo-74c6b77b75
    namespace: agent-chat
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: mongo
      uid: bcc612de-e55b-4894-a512-8b8a09596de7
    resourceVersion: "7560351"
    uid: f1dc70fc-fbbf-4a9d-bba8-a9e310ba62a6
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: mongo
        pod-template-hash: 74c6b77b75
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: mongo
          pod-template-hash: 74c6b77b75
      spec:
        containers:
        - image: mongo:6
          imagePullPolicy: IfNotPresent
          name: mongo
          ports:
          - containerPort: 27017
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data/db
            name: mongo-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: mongo-storage
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-04T16:01:43Z"
    generation: 2
    labels:
      app: mongo
      pod-template-hash: 84c58f889
    name: mongo-84c58f889
    namespace: agent-chat
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: mongo
      uid: bcc612de-e55b-4894-a512-8b8a09596de7
    resourceVersion: "6722370"
    uid: 2730e717-0cab-4db8-b215-d3ff5357ef07
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: mongo
        pod-template-hash: 84c58f889
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: mongo
          pod-template-hash: 84c58f889
      spec:
        containers:
        - image: mongo:6
          imagePullPolicy: IfNotPresent
          name: mongo
          ports:
          - containerPort: 27017
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data/db
            name: mongo-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: mongo-storage
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-11-10T04:43:53Z"
    generation: 2
    labels:
      app: postgres
      pod-template-hash: 6dc6d9858f
    name: postgres-6dc6d9858f
    namespace: agent-chat
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: postgres
      uid: 3d5c0ad0-818e-481e-b868-1c32f3769ffd
    resourceVersion: "7560356"
    uid: cdabdc68-b2b5-47ed-ba85-b69dd248ed08
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: postgres
        pod-template-hash: 6dc6d9858f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: postgres
          pod-template-hash: 6dc6d9858f
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: postgres-config
          - secretRef:
              name: postgres-secret
          image: postgres:14-alpine
          imagePullPolicy: IfNotPresent
          name: postgres
          ports:
          - containerPort: 5432
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/postgresql/data
            name: postgres-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: postgres-storage
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-04T15:48:20Z"
    generation: 2
    labels:
      app: postgres
      pod-template-hash: 7bdf8c6d7f
    name: postgres-7bdf8c6d7f
    namespace: agent-chat
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: postgres
      uid: 3d5c0ad0-818e-481e-b868-1c32f3769ffd
    resourceVersion: "6722486"
    uid: 54d7a396-c9a1-4bb0-b329-654292b36535
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: postgres
        pod-template-hash: 7bdf8c6d7f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: postgres
          pod-template-hash: 7bdf8c6d7f
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: postgres-config
          - secretRef:
              name: postgres-secret
          image: postgres:14-alpine
          imagePullPolicy: IfNotPresent
          name: postgres
          ports:
          - containerPort: 5432
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/postgresql/data
            name: postgres-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: postgres-storage
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-04T15:48:21Z"
    generation: 2
    labels:
      app: redis
      pod-template-hash: 6f46499764
    name: redis-6f46499764
    namespace: agent-chat
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: redis
      uid: 5a71615b-e260-4a9d-8ab4-9d1ea3a2cbaf
    resourceVersion: "6722415"
    uid: eb81fed4-3071-4869-b816-a2d1b1e2bd70
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: redis
        pod-template-hash: 6f46499764
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: redis
          pod-template-hash: 6f46499764
      spec:
        containers:
        - image: redis:7-alpine
          imagePullPolicy: IfNotPresent
          name: redis
          ports:
          - containerPort: 6379
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: redis-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: redis-storage
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-11-10T04:43:53Z"
    generation: 2
    labels:
      app: redis
      pod-template-hash: 844949cd94
    name: redis-844949cd94
    namespace: agent-chat
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: redis
      uid: 5a71615b-e260-4a9d-8ab4-9d1ea3a2cbaf
    resourceVersion: "7560366"
    uid: 7ee56f28-3366-4ac0-a4ea-96203929671e
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: redis
        pod-template-hash: 844949cd94
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: redis
          pod-template-hash: 844949cd94
      spec:
        containers:
        - image: redis:7-alpine
          imagePullPolicy: IfNotPresent
          name: redis
          ports:
          - containerPort: 6379
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: redis-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: redis-storage
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-10-28T14:44:14Z"
    generation: 2
    labels:
      app: gateway
      pod-template-hash: 5f75787c84
    name: gateway-5f75787c84
    namespace: agent-studio
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gateway
      uid: c0acd696-5826-42e4-9378-44355e550b07
    resourceVersion: "5922037"
    uid: 5d1c1879-a104-4c18-a7ff-ae03eb4ab656
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: gateway
        pod-template-hash: 5f75787c84
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: gateway
          pod-template-hash: 5f75787c84
      spec:
        containers:
        - env:
          - name: PHOENIX_COLLECTOR_ENDPOINT
            value: http://phoenix.observability.svc.cluster.local:4318/v1/traces
          - name: NODE_ENV
            value: development
          image: nginx:alpine
          imagePullPolicy: IfNotPresent
          name: gateway
          ports:
          - containerPort: 80
            protocol: TCP
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-22T05:43:36Z"
    generation: 4
    labels:
      app: gateway
      pod-template-hash: 6b67485d4f
    name: gateway-6b67485d4f
    namespace: agent-studio
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gateway
      uid: c0acd696-5826-42e4-9378-44355e550b07
    resourceVersion: "5744131"
    uid: 2726a7dc-fe50-4db2-85f0-393b274e9dd1
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: gateway
        pod-template-hash: 6b67485d4f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: gateway
          pod-template-hash: 6b67485d4f
      spec:
        containers:
        - image: nginx:alpine
          imagePullPolicy: IfNotPresent
          name: gateway
          ports:
          - containerPort: 80
            protocol: TCP
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2025-10-31T11:14:10Z"
    generation: 1
    labels:
      app: gateway
      pod-template-hash: c7c6db957
    name: gateway-c7c6db957
    namespace: agent-studio
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gateway
      uid: c0acd696-5826-42e4-9378-44355e550b07
    resourceVersion: "7642955"
    uid: 05183faf-db65-43cf-8cea-7dd5146dd814
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: gateway
        pod-template-hash: c7c6db957
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-31T07:14:10-04:00"
        creationTimestamp: null
        labels:
          app: gateway
          pod-template-hash: c7c6db957
      spec:
        containers:
        - env:
          - name: PHOENIX_COLLECTOR_ENDPOINT
            value: http://phoenix.observability.svc.cluster.local:4318/v1/traces
          - name: NODE_ENV
            value: development
          image: nginx:alpine
          imagePullPolicy: IfNotPresent
          name: gateway
          ports:
          - containerPort: 80
            protocol: TCP
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-10-28T14:45:36Z"
    generation: 1
    labels:
      app: aiflow-social-agent
      pod-template-hash: 5fbbb8c656
    name: aiflow-social-agent-5fbbb8c656
    namespace: agents-staging
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: aiflow-social-agent
      uid: e178ddb6-c308-4c1f-97bd-ded31875b7b6
    resourceVersion: "7642929"
    uid: 4636e973-84c9-460f-850f-80f30009a342
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: aiflow-social-agent
        pod-template-hash: 5fbbb8c656
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: aiflow-social-agent
          pod-template-hash: 5fbbb8c656
      spec:
        containers:
        - command:
          - sh
          - -c
          - while true; do echo 'Mock AIFlow agent running'; sleep 30; done
          env:
          - name: AGENT_ID
            value: social-agent-aiflow
          - name: PHOENIX_COLLECTOR_ENDPOINT
            value: http://phoenix.observability.svc.cluster.local:4318/v1/traces
          - name: NODE_ENV
            value: development
          image: python:3.11-slim
          imagePullPolicy: IfNotPresent
          name: aiflow-agent
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-23T11:46:09Z"
    generation: 2
    labels:
      app: aiflow-social-agent
      pod-template-hash: 75dbb877f4
    name: aiflow-social-agent-75dbb877f4
    namespace: agents-staging
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: aiflow-social-agent
      uid: e178ddb6-c308-4c1f-97bd-ded31875b7b6
    resourceVersion: "5744403"
    uid: 1a8e2c85-1291-4e0c-b1c7-8e220e411bfd
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: aiflow-social-agent
        pod-template-hash: 75dbb877f4
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: aiflow-social-agent
          pod-template-hash: 75dbb877f4
      spec:
        containers:
        - command:
          - sh
          - -c
          - while true; do echo 'Mock AIFlow agent running'; sleep 30; done
          env:
          - name: AGENT_ID
            value: social-agent-aiflow
          image: python:3.11-slim
          imagePullPolicy: IfNotPresent
          name: aiflow-agent
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-15T14:26:35Z"
    generation: 2
    labels:
      app: mlflow
      pod-template-hash: 79cbb69488
    name: mlflow-79cbb69488
    namespace: ai-ml
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: mlflow
      uid: 61890152-b062-46a6-9442-3393bf68d672
    resourceVersion: "6175815"
    uid: cde7546a-a011-4a96-853a-c33d7d47d2da
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: mlflow
        pod-template-hash: 79cbb69488
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: mlflow
          pod-template-hash: 79cbb69488
      spec:
        containers:
        - env:
          - name: MLFLOW_BACKEND_STORE_URI
            value: sqlite:///mlflow/mlflow.db
          - name: MLFLOW_DEFAULT_ARTIFACT_ROOT
            value: /mlflow/artifacts
          - name: MLFLOW_HOST
            value: 0.0.0.0
          - name: MLFLOW_PORT
            value: "5000"
          image: ghcr.io/mlflow/mlflow:v2.9.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: mlflow
          ports:
          - containerPort: 5000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /mlflow
            name: mlflow-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: mlflow-data
          persistentVolumeClaim:
            claimName: mlflow-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-21T00:18:41Z"
    generation: 1
    labels:
      app: apidog-runner
      pod-template-hash: 587c6b45b7
    name: apidog-runner-587c6b45b7
    namespace: apidog
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: apidog-runner
      uid: bf46971b-8349-4fab-b23f-3b53edcafecc
    resourceVersion: "7643092"
    uid: 9cd9a45b-fb80-4310-836c-2efcc3a08110
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: apidog-runner
        pod-template-hash: 587c6b45b7
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: apidog-runner
          pod-template-hash: 587c6b45b7
      spec:
        containers:
        - env:
          - name: TZ
            value: America/New_York
          - name: SERVER_APP_BASE_URL
            value: https://api.apidog.com
          - name: TEAM_ID
            valueFrom:
              secretKeyRef:
                key: TEAM_ID
                name: apidog-runner-config
          - name: RUNNER_ID
            valueFrom:
              secretKeyRef:
                key: RUNNER_ID
                name: apidog-runner-config
          - name: ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                key: ACCESS_TOKEN
                name: apidog-runner-config
          image: apidog/self-hosted-general-runner:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 4524
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: runner
          ports:
          - containerPort: 4524
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 4524
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-11-13T23:00:05Z"
    generation: 1
    labels:
      app: argo-server
      pod-template-hash: 54467f6857
    name: argo-server-54467f6857
    namespace: argo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: argo-server
      uid: d3f2e9d3-6a46-45a7-9ae0-836beb3eaaa3
    resourceVersion: "7643411"
    uid: 47e4b6e2-285e-4c0c-ada9-4f0f52748e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: argo-server
        pod-template-hash: 54467f6857
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: argo-server
          pod-template-hash: 54467f6857
      spec:
        containers:
        - args:
          - server
          image: quay.io/argoproj/argocli:v3.5.2
          imagePullPolicy: IfNotPresent
          name: argo-server
          ports:
          - containerPort: 2746
            name: web
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 2746
              scheme: HTTPS
            initialDelaySeconds: 10
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
        serviceAccount: argo-server
        serviceAccountName: argo-server
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-11-13T19:40:07Z"
    generation: 2
    labels:
      app: argo-server
      pod-template-hash: 65b8fb9599
    name: argo-server-65b8fb9599
    namespace: argo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: argo-server
      uid: d3f2e9d3-6a46-45a7-9ae0-836beb3eaaa3
    resourceVersion: "7089869"
    uid: ff2cc466-f886-4c50-b7c9-a933a7fd2cf2
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: argo-server
        pod-template-hash: 65b8fb9599
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: argo-server
          pod-template-hash: 65b8fb9599
      spec:
        containers:
        - args:
          - server
          image: quay.io/argoproj/argocli:v3.5.2
          imagePullPolicy: IfNotPresent
          name: argo-server
          ports:
          - containerPort: 2746
            name: web
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 2746
              scheme: HTTPS
            initialDelaySeconds: 10
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
        serviceAccount: argo-server
        serviceAccountName: argo-server
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-11-13T19:40:07Z"
    generation: 2
    labels:
      app: workflow-controller
      pod-template-hash: 6564d94798
    name: workflow-controller-6564d94798
    namespace: argo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: workflow-controller
      uid: df8dd0f5-dea4-4efa-b3e3-5aec5f9c562a
    resourceVersion: "7028497"
    uid: 935b7ad7-07d0-49ec-beea-9bede44fe9e1
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: workflow-controller
        pod-template-hash: 6564d94798
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: workflow-controller
          pod-template-hash: 6564d94798
      spec:
        containers:
        - command:
          - workflow-controller
          env:
          - name: LEADER_ELECTION_IDENTITY
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: quay.io/argoproj/workflow-controller:v3.5.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6060
              scheme: HTTP
            initialDelaySeconds: 90
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: workflow-controller
          ports:
          - containerPort: 9090
            name: metrics
            protocol: TCP
          - containerPort: 6060
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: workflow-controller
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
        serviceAccount: argo
        serviceAccountName: argo
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-11-13T23:00:05Z"
    generation: 1
    labels:
      app: workflow-controller
      pod-template-hash: 855c5bfc48
    name: workflow-controller-855c5bfc48
    namespace: argo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: workflow-controller
      uid: df8dd0f5-dea4-4efa-b3e3-5aec5f9c562a
    resourceVersion: "7642993"
    uid: 5dce8914-f3d1-4f09-9f49-231b5854f768
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: workflow-controller
        pod-template-hash: 855c5bfc48
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: workflow-controller
          pod-template-hash: 855c5bfc48
      spec:
        containers:
        - command:
          - workflow-controller
          env:
          - name: LEADER_ELECTION_IDENTITY
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: quay.io/argoproj/workflow-controller:v3.5.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6060
              scheme: HTTP
            initialDelaySeconds: 90
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: workflow-controller
          ports:
          - containerPort: 9090
            name: metrics
            protocol: TCP
          - containerPort: 6060
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: workflow-controller
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
        serviceAccount: argo
        serviceAccountName: argo
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-11-03T19:23:57Z"
    generation: 2
    labels:
      app: agent-router
      pod-template-hash: c68d69f7f
      version: v1
    name: agent-router-c68d69f7f
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: agent-router
      uid: 6500c1ca-c99d-4294-87c6-92f1603aef9f
    resourceVersion: "6756368"
    uid: 2acc19c2-bd14-4c6f-b250-3f3519e1a1fb
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: agent-router
        pod-template-hash: c68d69f7f
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: agent-router
          pod-template-hash: c68d69f7f
          version: v1
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - agent-router
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: RUNTIME
            value: kubernetes
          - name: MAX_CONCURRENT_AGENTS
            value: "50"
          - name: LOG_LEVEL
            value: info
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: agent-buildkit/worker:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: router
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 2
            httpGet:
              path: /ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/cache
            name: cache
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: agent-router
        serviceAccountName: agent-router
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: cache
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-06T23:40:48Z"
    generation: 2
    labels:
      app: agent-tracer-api
      pod-template-hash: 754f68bdd4
    name: agent-tracer-api-754f68bdd4
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: agent-tracer-api
      uid: 5173bd3b-aaa0-4560-966d-842f4eb2dfeb
    resourceVersion: "5361724"
    uid: 98384b2f-8d97-41fc-a2a3-bd28fb98da27
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: agent-tracer-api
        pod-template-hash: 754f68bdd4
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: agent-tracer-api
          pod-template-hash: 754f68bdd4
      spec:
        containers:
        - args:
          - -c
          - |
            echo 'server {
              listen 80;
              server_name _;
              location /health {
                add_header Content-Type application/json;
                return 200 "{\"status\":\"healthy\",\"service\":\"agent-tracer-api\"}";
              }
              location /api/v1/traces {
                add_header Content-Type application/json;
                return 200 "{\"traces\":[],\"message\":\"Agent Tracer API\"}";
              }
              location / {
                return 200 "Agent Tracer API - Service Running";
              }
            }' > /etc/nginx/conf.d/default.conf
            nginx -g "daemon off;"
          command:
          - /bin/sh
          image: nginx:alpine
          imagePullPolicy: IfNotPresent
          name: agent-tracer-api
          ports:
          - containerPort: 80
            protocol: TCP
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-06T23:40:48Z"
    generation: 2
    labels:
      app: agent-tracer-dashboard
      pod-template-hash: 94b6c49d6
    name: agent-tracer-dashboard-94b6c49d6
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: agent-tracer-dashboard
      uid: f9b90c64-dd05-4e37-8a1f-342139149858
    resourceVersion: "5361734"
    uid: a04b2fee-b44d-4e04-9df5-baac5811f8f3
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: agent-tracer-dashboard
        pod-template-hash: 94b6c49d6
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: agent-tracer-dashboard
          pod-template-hash: 94b6c49d6
      spec:
        containers:
        - args:
          - -c
          - |
            echo 'server {
              listen 80;
              server_name _;
              location /health {
                add_header Content-Type application/json;
                return 200 "{\"status\":\"healthy\",\"service\":\"agent-tracer-dashboard\"}";
              }
              location / {
                add_header Content-Type text/html;
                return 200 "<html><body><h1>Agent Tracer Dashboard</h1><p>Service is running</p><p>Status: Healthy</p></body></html>";
              }
            }' > /etc/nginx/conf.d/default.conf
            nginx -g "daemon off;"
          command:
          - /bin/sh
          image: nginx:alpine
          imagePullPolicy: IfNotPresent
          name: agent-tracer-dashboard
          ports:
          - containerPort: 80
            protocol: TCP
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      buildkit.ai/auto-deploy: "true"
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
      gitlab.com/project: llm/npm/agent-buildkit
    creationTimestamp: "2025-11-17T16:41:55Z"
    generation: 2
    labels:
      app: ci-fix-agent
      category: automation
      environment: production
      ossa.ai-agent-version: 1.0.0
      ossa.ai-capability: pipeline-repair
      ossa.ai-domain: infrastructure
      ossa.ai-subdomain: ci-cd
      ossa.ai-version: ossa-v0.2.3
      pod-template-hash: 5b6d9b8d5
      team: platform
    name: ci-fix-agent-5b6d9b8d5
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: ci-fix-agent
      uid: 9eb82414-c06b-4915-88db-b93724561a14
    resourceVersion: "7628084"
    uid: f59f1c5c-0442-4034-9bdf-103a6e978d73
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: ci-fix-agent
        pod-template-hash: 5b6d9b8d5
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: ci-fix-agent
          category: automation
          environment: production
          ossa.ai-agent-version: 1.0.0
          ossa.ai-capability: pipeline-repair
          ossa.ai-domain: infrastructure
          ossa.ai-subdomain: ci-cd
          ossa.ai-version: ossa-v0.2.3
          pod-template-hash: 5b6d9b8d5
          team: platform
      spec:
        containers:
        - env:
          - name: AGENT_NAME
            value: ci-fix-agent
          - name: AGENT_VERSION
            value: 1.0.0
          - name: OSSA_VERSION
            value: ossa/v0.2.3
          - name: AGENT_ROLE
            value: You are a CI/CD pipeline expert. You analyze failed GitLab CI pipelines,
              identify root causes, and automatically fix common issues like dependency
              conflicts, test failures, and configuration errors.
          - name: LLM_PROVIDER
            value: anthropic
          - name: LLM_MODEL
            value: claude-3-5-sonnet-20241022
          - name: LLM_TEMPERATURE
            value: "0.1"
          - name: OSSA_TOOLS
            value: '[{"type":"http","name":"gitlab_api"},{"type":"kubernetes"},{"type":"function","name":"ci_analyzer"}]'
          image: node:alpine
          imagePullPolicy: IfNotPresent
          name: agent
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-11-17T16:41:55Z"
    generation: 2
    labels:
      app: git-cleanup-agent
      environment: production
      ossa.ai-agent-version: 1.0.0
      ossa.ai-capability: repository-maintenance
      ossa.ai-domain: infrastructure
      ossa.ai-subdomain: version-control
      ossa.ai-version: ossa-v0.2.3
      pod-template-hash: 56cdf98687
      team: devops
    name: git-cleanup-agent-56cdf98687
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: git-cleanup-agent
      uid: 89865299-0325-4b66-8dfb-764c421c4ff5
    resourceVersion: "7628404"
    uid: 453042ef-32ba-4245-8ea0-281ec1718845
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: git-cleanup-agent
        pod-template-hash: 56cdf98687
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: git-cleanup-agent
          environment: production
          ossa.ai-agent-version: 1.0.0
          ossa.ai-capability: repository-maintenance
          ossa.ai-domain: infrastructure
          ossa.ai-subdomain: version-control
          ossa.ai-version: ossa-v0.2.3
          pod-template-hash: 56cdf98687
          team: devops
      spec:
        containers:
        - env:
          - name: AGENT_NAME
            value: git-cleanup-agent
          - name: AGENT_VERSION
            value: 1.0.0
          - name: OSSA_VERSION
            value: ossa/v0.2.3
          - name: AGENT_ROLE
            value: You are a Git repository cleanup specialist. You analyze repositories,
              identify stale branches, clean up merge conflicts, and maintain repository
              health.
          - name: LLM_PROVIDER
            value: anthropic
          - name: LLM_MODEL
            value: claude-3-5-sonnet-20241022
          - name: LLM_TEMPERATURE
            value: "0.2"
          - name: OSSA_TOOLS
            value: '[{"type":"kubernetes"},{"type":"function","name":"git_operations"}]'
          image: alpine/git:latest
          imagePullPolicy: Always
          name: agent
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-11-10T04:43:53Z"
    generation: 1
    labels:
      app: ossa
      pod-template-hash: 68dc75d46b
    name: ossa-68dc75d46b
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: ossa
      uid: 2242c3f4-b965-4576-923f-1f51c9aa0d1b
    resourceVersion: "6617454"
    uid: b951c9a5-cc23-4808-8424-f5322ad62bc1
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: ossa
        pod-template-hash: 68dc75d46b
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: ossa
          pod-template-hash: 68dc75d46b
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: PORT
            value: "3000"
          image: ossa:latest
          imagePullPolicy: Always
          name: ossa
          ports:
          - containerPort: 3000
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-01T05:31:06Z"
    generation: 2
    labels:
      app: ossa
      pod-template-hash: 74b69b66fb
    name: ossa-74b69b66fb
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: ossa
      uid: 2242c3f4-b965-4576-923f-1f51c9aa0d1b
    resourceVersion: "5362924"
    uid: 87107c09-b86b-4962-bdf1-b15c8b1c7f0d
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: ossa
        pod-template-hash: 74b69b66fb
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: ossa
          pod-template-hash: 74b69b66fb
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: PORT
            value: "3000"
          image: ossa:latest
          imagePullPolicy: Always
          name: ossa
          ports:
          - containerPort: 3000
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-11-17T19:39:59Z"
    generation: 3
    labels:
      app: ossa-website
      component: frontend
      pod-template-hash: 6cb4dd4fbd
    name: ossa-website-6cb4dd4fbd
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: ossa-website
      uid: 501234a6-65f6-488e-9806-5407be1ae79f
    resourceVersion: "7629340"
    uid: c9f4509b-769d-4be8-8c09-808701f7df2c
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: ossa-website
        pod-template-hash: 6cb4dd4fbd
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: ossa-website
          component: frontend
          pod-template-hash: 6cb4dd4fbd
      spec:
        containers:
        - image: ossa-website:latest
          imagePullPolicy: Never
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: website
          ports:
          - containerPort: 3000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 500m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2025-11-18T01:10:03Z"
    generation: 2
    labels:
      app: ossa-website
      component: frontend
      pod-template-hash: 9f7c9b69b
    name: ossa-website-9f7c9b69b
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: ossa-website
      uid: 501234a6-65f6-488e-9806-5407be1ae79f
    resourceVersion: "7640948"
    uid: 29a1ba11-85c8-44c8-9935-f430e266d0d0
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: ossa-website
        pod-template-hash: 9f7c9b69b
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-11-17T20:00:13-05:00"
        creationTimestamp: null
        labels:
          app: ossa-website
          component: frontend
          pod-template-hash: 9f7c9b69b
      spec:
        containers:
        - image: ossa-website:latest
          imagePullPolicy: Never
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 10
          name: website
          ports:
          - containerPort: 3000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-11-18T01:00:13Z"
    generation: 3
    labels:
      app: ossa-website
      component: frontend
      pod-template-hash: f69794dc4
    name: ossa-website-f69794dc4
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: ossa-website
      uid: 501234a6-65f6-488e-9806-5407be1ae79f
    resourceVersion: "7640949"
    uid: 72c13421-130d-443d-933a-209db66fdf64
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: ossa-website
        pod-template-hash: f69794dc4
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-11-17T20:00:13-05:00"
        creationTimestamp: null
        labels:
          app: ossa-website
          component: frontend
          pod-template-hash: f69794dc4
      spec:
        containers:
        - image: ossa-website:latest
          imagePullPolicy: Never
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: website
          ports:
          - containerPort: 3000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 500m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      buildkit.ai/success-rate: 95%
      buildkit.ai/target-errors: "8189"
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
      docs.url: https://gitlab.bluefly.io/llm/npm/agent-buildkit/-/wikis/typescript-fixer
    creationTimestamp: "2025-11-17T16:41:55Z"
    generation: 2
    labels:
      app: typescript-fixer-agent
      category: code-quality
      environment: production
      ossa.ai-agent-version: 1.0.0
      ossa.ai-capability: typescript-fixing
      ossa.ai-domain: infrastructure
      ossa.ai-subdomain: code-quality
      ossa.ai-version: ossa-v0.2.3
      pod-template-hash: 7cc6dbfbd4
      priority: high
      team: platform
    name: typescript-fixer-agent-7cc6dbfbd4
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: typescript-fixer-agent
      uid: d52d80a6-4279-409a-b3ee-b740e6fbd72b
    resourceVersion: "7629624"
    uid: 0ad63e3d-5e3a-4bf9-bf99-caa75ec8f484
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: typescript-fixer-agent
        pod-template-hash: 7cc6dbfbd4
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: typescript-fixer-agent
          category: code-quality
          environment: production
          ossa.ai-agent-version: 1.0.0
          ossa.ai-capability: typescript-fixing
          ossa.ai-domain: infrastructure
          ossa.ai-subdomain: code-quality
          ossa.ai-version: ossa-v0.2.3
          pod-template-hash: 7cc6dbfbd4
          priority: high
          team: platform
      spec:
        containers:
        - env:
          - name: AGENT_NAME
            value: typescript-fixer-agent
          - name: AGENT_VERSION
            value: 1.0.0
          - name: OSSA_VERSION
            value: ossa/v0.2.3
          - name: AGENT_ROLE
            value: You are a TypeScript expert specializing in automated error fixing.
              You analyze TypeScript compilation errors, understand their context,
              and apply intelligent fixes including type annotations, import corrections,
              interface updates, and refactoring. You handle type mismatches, missing
              definitions, strict mode violations, and configuration issues.
          - name: LLM_PROVIDER
            value: anthropic
          - name: LLM_MODEL
            value: claude-3-5-sonnet-20241022
          - name: LLM_TEMPERATURE
            value: "0.3"
          - name: OSSA_TOOLS
            value: '[{"type":"mcp"},{"type":"function","name":"typescript_compiler"},{"type":"function","name":"code_refactor"},{"type":"kubernetes"}]'
          image: node:22-alpine
          imagePullPolicy: IfNotPresent
          name: agent
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 250m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-11-13T19:37:41Z"
    generation: 2
    labels:
      app: dragonfly
      pod-template-hash: 64f74577f
    name: dragonfly-64f74577f
    namespace: dragonfly-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: dragonfly
      uid: 48f3e9f4-a25c-4f52-8599-079990e1ed9c
    resourceVersion: "7089739"
    uid: 105a1605-2de3-4854-b45f-74bdcc90676d
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: dragonfly
        pod-template-hash: 64f74577f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: dragonfly
          pod-template-hash: 64f74577f
      spec:
        containers:
        - args:
          - --requirepass=dragonfly-secret
          - --maxmemory=8gb
          - --cache_mode=true
          - --snapshot_cron="0 */6 * * *"
          - --dbfilename=dump.rdb
          - --logtostderr
          image: docker.dragonflydb.io/dragonflydb/dragonfly:latest
          imagePullPolicy: Always
          livenessProbe:
            exec:
              command:
              - redis-cli
              - -a
              - dragonfly-secret
              - ping
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: dragonfly
          ports:
          - containerPort: 6379
            name: redis
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - redis-cli
              - -a
              - dragonfly-secret
              - ping
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "4"
              memory: 8Gi
            requests:
              cpu: 500m
              memory: 2Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-11-13T19:43:11Z"
    generation: 2
    labels:
      app: dragonfly
      pod-template-hash: 6ff4d57c84
    name: dragonfly-6ff4d57c84
    namespace: dragonfly-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: dragonfly
      uid: 48f3e9f4-a25c-4f52-8599-079990e1ed9c
    resourceVersion: "7089735"
    uid: 6aa85b09-c269-49a5-860c-825c48553c33
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: dragonfly
        pod-template-hash: 6ff4d57c84
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: dragonfly
          pod-template-hash: 6ff4d57c84
      spec:
        containers:
        - args:
          - --requirepass=dragonfly-secret
          - --maxmemory=8gb
          - --cache_mode=true
          - --snapshot_cron="0 */6 * * *"
          - --dbfilename=dump.rdb
          - --logtostderr
          image: docker.dragonflydb.io/dragonflydb/dragonfly:latest
          imagePullPolicy: Always
          livenessProbe:
            exec:
              command:
              - redis-cli
              - -a
              - dragonfly-secret
              - ping
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: dragonfly
          ports:
          - containerPort: 6379
            name: redis
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - redis-cli
              - -a
              - dragonfly-secret
              - ping
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "4"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-11-14T17:11:32Z"
    generation: 1
    labels:
      app: agent-orchestrator
      pod-template-hash: 7847c9cb8c
    name: agent-orchestrator-7847c9cb8c
    namespace: drupal-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: agent-orchestrator
      uid: 6120b5c5-38c1-44da-b720-d691f36c27ac
    resourceVersion: "7642935"
    uid: d09db007-17c8-491b-9307-8fa1705210ff
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agent-orchestrator
        pod-template-hash: 7847c9cb8c
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: agent-orchestrator
          pod-template-hash: 7847c9cb8c
      spec:
        containers:
        - args:
          - -c
          - php -S 0.0.0.0:8080 -t /app
          command:
          - /bin/bash
          env:
          - name: DRUPAL_ROOT
            value: /app
          image: drupal:11-apache
          imagePullPolicy: IfNotPresent
          name: orchestrator
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 250m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app
            name: app-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /Users/flux423/Sites/Cannabis/indoorplantkingdom.com
            type: Directory
          name: app-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: goldilocks
      meta.helm.sh/release-namespace: goldilocks
    creationTimestamp: "2025-11-10T04:40:38Z"
    generation: 2
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: goldilocks
      app.kubernetes.io/name: goldilocks
      pod-template-hash: 55c758c6dd
    name: goldilocks-controller-55c758c6dd
    namespace: goldilocks
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: goldilocks-controller
      uid: c15e39bc-c71a-4614-9332-680703b66939
    resourceVersion: "6763291"
    uid: 63cc9cb6-b17a-4fab-a9dd-6f1699882c43
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: goldilocks
        app.kubernetes.io/name: goldilocks
        pod-template-hash: 55c758c6dd
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: goldilocks
          app.kubernetes.io/name: goldilocks
          pod-template-hash: 55c758c6dd
      spec:
        containers:
        - command:
          - /goldilocks
          - controller
          - -v2
          image: us-docker.pkg.dev/fairwinds-ops/oss/goldilocks:v4.14.1
          imagePullPolicy: Always
          name: goldilocks
          resources:
            requests:
              cpu: 25m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 10324
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: goldilocks-controller
        serviceAccountName: goldilocks-controller
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: goldilocks
      meta.helm.sh/release-namespace: goldilocks
    creationTimestamp: "2025-11-10T04:40:38Z"
    generation: 2
    labels:
      app.kubernetes.io/component: dashboard
      app.kubernetes.io/instance: goldilocks
      app.kubernetes.io/name: goldilocks
      pod-template-hash: 84f44f47f4
    name: goldilocks-dashboard-84f44f47f4
    namespace: goldilocks
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: goldilocks-dashboard
      uid: 1b86e4f5-985d-4f2b-9fb7-751bd61bce94
    resourceVersion: "6763303"
    uid: b4209bf1-26a6-47a4-9ddf-2184337bc1d9
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: dashboard
        app.kubernetes.io/instance: goldilocks
        app.kubernetes.io/name: goldilocks
        pod-template-hash: 84f44f47f4
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: dashboard
          app.kubernetes.io/instance: goldilocks
          app.kubernetes.io/name: goldilocks
          pod-template-hash: 84f44f47f4
      spec:
        containers:
        - command:
          - /goldilocks
          - dashboard
          - --exclude-containers=linkerd-proxy,istio-proxy
          - -v2
          image: us-docker.pkg.dev/fairwinds-ops/oss/goldilocks:v4.14.1
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: goldilocks
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 25m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 10324
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: goldilocks-dashboard
        serviceAccountName: goldilocks-dashboard
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: goldilocks
      meta.helm.sh/release-namespace: goldilocks
    creationTimestamp: "2025-11-10T04:40:38Z"
    generation: 2
    labels:
      app.kubernetes.io/component: admission-controller
      app.kubernetes.io/instance: goldilocks
      app.kubernetes.io/name: vpa
      pod-template-hash: 7b66c67dd6
    name: goldilocks-vpa-admission-controller-7b66c67dd6
    namespace: goldilocks
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: goldilocks-vpa-admission-controller
      uid: 8561f404-3a9e-4bc9-98cf-8400fe627719
    resourceVersion: "6763305"
    uid: 38bde725-be67-431f-b25b-bf4a20afe018
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: admission-controller
        app.kubernetes.io/instance: goldilocks
        app.kubernetes.io/name: vpa
        pod-template-hash: 7b66c67dd6
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: admission-controller
          app.kubernetes.io/instance: goldilocks
          app.kubernetes.io/name: vpa
          pod-template-hash: 7b66c67dd6
      spec:
        containers:
        - args:
          - --register-webhook=false
          - --webhook-service=goldilocks-vpa-webhook
          - --client-ca-file=/etc/tls-certs/ca
          - --tls-cert-file=/etc/tls-certs/cert
          - --tls-private-key=/etc/tls-certs/key
          env:
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: registry.k8s.io/autoscaling/vpa-admission-controller:1.4.1
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /health-check
              port: metrics
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          name: vpa
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8944
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 120
            httpGet:
              path: /health-check
              port: metrics
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            requests:
              cpu: 50m
              memory: 200Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/tls-certs
            name: tls-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: goldilocks-vpa-admission-controller
        serviceAccountName: goldilocks-vpa-admission-controller
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-certs
          secret:
            defaultMode: 420
            secretName: goldilocks-vpa-tls-secret
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: goldilocks
      meta.helm.sh/release-namespace: goldilocks
    creationTimestamp: "2025-11-10T04:40:38Z"
    generation: 2
    labels:
      app.kubernetes.io/component: recommender
      app.kubernetes.io/instance: goldilocks
      app.kubernetes.io/name: vpa
      pod-template-hash: bcc897f7d
    name: goldilocks-vpa-recommender-bcc897f7d
    namespace: goldilocks
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: goldilocks-vpa-recommender
      uid: c8a075f0-071f-4c18-99b3-29aaaa231e43
    resourceVersion: "6763309"
    uid: df077b28-b496-4593-af27-38f4fde2c280
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: recommender
        app.kubernetes.io/instance: goldilocks
        app.kubernetes.io/name: vpa
        pod-template-hash: bcc897f7d
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: recommender
          app.kubernetes.io/instance: goldilocks
          app.kubernetes.io/name: vpa
          pod-template-hash: bcc897f7d
      spec:
        containers:
        - args:
          - --pod-recommendation-min-cpu-millicores=15
          - --pod-recommendation-min-memory-mb=100
          - --v=4
          image: registry.k8s.io/autoscaling/vpa-recommender:1.4.1
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /health-check
              port: metrics
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          name: vpa
          ports:
          - containerPort: 8942
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 120
            httpGet:
              path: /health-check
              port: metrics
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            requests:
              cpu: 50m
              memory: 500Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: goldilocks-vpa-recommender
        serviceAccountName: goldilocks-vpa-recommender
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2025-09-07T03:15:30Z"
    generation: 1
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.13.2
      helm.sh/chart: ingress-nginx-4.13.2
      pod-template-hash: 5b8bf5478
    name: ingress-nginx-controller-5b8bf5478
    namespace: ingress-nginx
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: ingress-nginx-controller
      uid: 5cccec23-6da0-4af6-b96b-e779aece2a9a
    resourceVersion: "7643144"
    uid: b4d730a9-604f-48d0-a1e2-1daf5fbefe21
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
        pod-template-hash: 5b8bf5478
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: ingress-nginx
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: ingress-nginx
          app.kubernetes.io/part-of: ingress-nginx
          app.kubernetes.io/version: 1.13.2
          helm.sh/chart: ingress-nginx-4.13.2
          pod-template-hash: 5b8bf5478
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - /nginx-ingress-controller
          - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
          - --election-id=ingress-nginx-leader
          - --controller-class=k8s.io/ingress-nginx
          - --ingress-class=nginx
          - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
          - --validating-webhook=:8443
          - --validating-webhook-certificate=/usr/local/certificates/cert
          - --validating-webhook-key=/usr/local/certificates/key
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: LD_PRELOAD
            value: /usr/local/lib/libmimalloc.so
          image: registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /wait-shutdown
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: controller
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            name: https
            protocol: TCP
          - containerPort: 8443
            name: webhook
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 90Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsGroup: 82
            runAsNonRoot: true
            runAsUser: 101
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /usr/local/certificates/
            name: webhook-cert
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: ingress-nginx
        serviceAccountName: ingress-nginx
        terminationGracePeriodSeconds: 300
        volumes:
        - name: webhook-cert
          secret:
            defaultMode: 420
            secretName: ingress-nginx-admission
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: keda
      meta.helm.sh/release-namespace: keda
    creationTimestamp: "2025-10-21T15:54:48Z"
    generation: 1
    labels:
      app: keda-admission-webhooks
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: keda
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: keda-admission-webhooks
      app.kubernetes.io/part-of: keda-operator
      app.kubernetes.io/version: 2.18.0
      helm.sh/chart: keda-2.18.0
      name: keda-admission-webhooks
      pod-template-hash: 6656fbbb89
    name: keda-admission-webhooks-6656fbbb89
    namespace: keda
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: keda-admission-webhooks
      uid: 4135a2e3-ea8b-418b-9af3-cf2b56b90ccb
    resourceVersion: "7643403"
    uid: 8986819c-ec54-4ec4-bdcf-fc0d00076c7f
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: keda-admission-webhooks
        pod-template-hash: 6656fbbb89
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: keda-admission-webhooks
          app.kubernetes.io/component: operator
          app.kubernetes.io/instance: keda
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: keda-admission-webhooks
          app.kubernetes.io/part-of: keda-operator
          app.kubernetes.io/version: 2.18.0
          helm.sh/chart: keda-2.18.0
          name: keda-admission-webhooks
          pod-template-hash: 6656fbbb89
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --zap-log-level=info
          - --zap-encoder=console
          - --zap-time-encoding=rfc3339
          - --cert-dir=/certs
          - --health-probe-bind-address=:8081
          - --metrics-bind-address=:8080
          command:
          - /keda-admission-webhooks
          env:
          - name: WATCH_NAMESPACE
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: ghcr.io/kedacore/keda-admission-webhooks:2.18.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 25
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: keda-admission-webhooks
          ports:
          - containerPort: 9443
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 3
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1000Mi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /certs
            name: certificates
            readOnly: true
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
        serviceAccount: keda-webhook
        serviceAccountName: keda-webhook
        terminationGracePeriodSeconds: 30
        volumes:
        - name: certificates
          secret:
            defaultMode: 420
            secretName: kedaorg-certs
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: keda
      meta.helm.sh/release-namespace: keda
    creationTimestamp: "2025-10-21T15:54:48Z"
    generation: 1
    labels:
      app: keda-operator
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: keda
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: keda-operator
      app.kubernetes.io/part-of: keda-operator
      app.kubernetes.io/version: 2.18.0
      helm.sh/chart: keda-2.18.0
      name: keda-operator
      pod-template-hash: 6687649c57
    name: keda-operator-6687649c57
    namespace: keda
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: keda-operator
      uid: 2f68bc20-fd19-41cf-b1fe-4637309eabb0
    resourceVersion: "7643340"
    uid: c1ae0db0-cf9d-4272-b509-b2f52b7af8ef
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: keda-operator
        pod-template-hash: 6687649c57
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: keda-operator
          app.kubernetes.io/component: operator
          app.kubernetes.io/instance: keda
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: keda-operator
          app.kubernetes.io/part-of: keda-operator
          app.kubernetes.io/version: 2.18.0
          helm.sh/chart: keda-2.18.0
          name: keda-operator
          pod-template-hash: 6687649c57
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --leader-elect
          - --disable-compression=true
          - --zap-log-level=info
          - --zap-encoder=console
          - --zap-time-encoding=rfc3339
          - --enable-webhook-patching=true
          - --cert-dir=/certs
          - --enable-cert-rotation=true
          - --cert-secret-name=kedaorg-certs
          - --operator-service-name=keda-operator
          - --metrics-server-service-name=keda-operator-metrics-apiserver
          - --webhooks-service-name=keda-admission-webhooks
          - --k8s-cluster-name=kubernetes-default
          - --k8s-cluster-domain=cluster.local
          - --enable-prometheus-metrics=false
          command:
          - /keda
          env:
          - name: WATCH_NAMESPACE
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: OPERATOR_NAME
            value: keda-operator
          - name: KEDA_HTTP_DEFAULT_TIMEOUT
            value: "3000"
          - name: KEDA_HTTP_MIN_TLS_VERSION
            value: TLS12
          image: ghcr.io/kedacore/keda:2.18.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 25
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: keda-operator
          ports:
          - containerPort: 9666
            name: metricsservice
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 3
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1000Mi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /certs
            name: certificates
            readOnly: true
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
        serviceAccount: keda-operator
        serviceAccountName: keda-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: certificates
          secret:
            defaultMode: 420
            optional: true
            secretName: kedaorg-certs
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: keda
      meta.helm.sh/release-namespace: keda
    creationTimestamp: "2025-10-21T15:54:48Z"
    generation: 1
    labels:
      app: keda-operator-metrics-apiserver
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: keda
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: keda-operator-metrics-apiserver
      app.kubernetes.io/part-of: keda-operator
      app.kubernetes.io/version: 2.18.0
      helm.sh/chart: keda-2.18.0
      pod-template-hash: 74bcc6f665
    name: keda-operator-metrics-apiserver-74bcc6f665
    namespace: keda
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: keda-operator-metrics-apiserver
      uid: 8fff03a4-21d9-4872-b452-969e56adb6d7
    resourceVersion: "7643077"
    uid: bb11dbb5-7f85-4a1d-a846-0cb86905c955
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: keda-operator-metrics-apiserver
        pod-template-hash: 74bcc6f665
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: keda-operator-metrics-apiserver
          app.kubernetes.io/component: operator
          app.kubernetes.io/instance: keda
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: keda-operator-metrics-apiserver
          app.kubernetes.io/part-of: keda-operator
          app.kubernetes.io/version: 2.18.0
          helm.sh/chart: keda-2.18.0
          pod-template-hash: 74bcc6f665
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --secure-port=6443
          - --logtostderr=true
          - --stderrthreshold=ERROR
          - --disable-compression=true
          - --metrics-service-address=keda-operator.keda.svc.cluster.local:9666
          - --client-ca-file=/certs/ca.crt
          - --tls-cert-file=/certs/tls.crt
          - --tls-private-key-file=/certs/tls.key
          - --cert-dir=/certs
          - --v=0
          command:
          - /keda-adapter
          env:
          - name: WATCH_NAMESPACE
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: KEDA_HTTP_DEFAULT_TIMEOUT
            value: "3000"
          - name: KEDA_HTTP_MIN_TLS_VERSION
            value: TLS12
          image: ghcr.io/kedacore/keda-metrics-apiserver:2.18.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6443
              scheme: HTTPS
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: keda-operator-metrics-apiserver
          ports:
          - containerPort: 6443
            name: https
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 6443
              scheme: HTTPS
            initialDelaySeconds: 5
            periodSeconds: 3
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1000Mi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /certs
            name: certificates
            readOnly: true
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
        serviceAccount: keda-metrics-server
        serviceAccountName: keda-metrics-server
        terminationGracePeriodSeconds: 30
        volumes:
        - name: certificates
          secret:
            defaultMode: 420
            secretName: kedaorg-certs
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment-wave: "3"
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      generated-by: agent-swarm
      issues-closed: "79"
    creationTimestamp: "2025-11-13T01:12:13Z"
    generation: 11
    labels:
      app: khook
      managed-by: agent-buildkit
      ossa-version: v0.2.3
      platform: kagent
      pod-template-hash: 674bc6d8c9
    name: khook-674bc6d8c9
    namespace: khook-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: khook
      uid: a9094e0f-ea00-422e-ae6b-2dda10c088ba
    resourceVersion: "7511811"
    uid: d9927898-7742-4784-bae4-80a4d05c8568
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: khook
        managed-by: agent-buildkit
        ossa-version: v0.2.3
        platform: kagent
        pod-template-hash: 674bc6d8c9
    template:
      metadata:
        annotations:
          deployment-wave: "3"
          generated-by: agent-swarm
          issues-closed: "79"
        creationTimestamp: null
        labels:
          app: khook
          managed-by: agent-buildkit
          ossa-version: v0.2.3
          platform: kagent
          pod-template-hash: 674bc6d8c9
      spec:
        containers:
        - env:
          - name: KUBERNETES_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: WEBHOOK_PORT
            value: "8443"
          - name: KAGENT_ENABLED
            value: "true"
          - name: NATS_URL
            value: nats://nats.kagent-system.svc.cluster.local:4222
          image: ghcr.io/kagent/khook:latest
          imagePullPolicy: Always
          name: khook
          ports:
          - containerPort: 8443
            name: webhook
            protocol: TCP
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/webhook/certs
            name: webhook-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: khook
        serviceAccountName: khook
        terminationGracePeriodSeconds: 30
        volumes:
        - name: webhook-certs
          secret:
            defaultMode: 420
            secretName: khook-webhook-certs
  status:
    fullyLabeledReplicas: 1
    observedGeneration: 11
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xU328aRxD+V6p5voMjdoJ9Uh9ScBUrMUUmzkuEomFvDrbs7Wx397CRdf97NXeAIfWPVuoTx+7Mt9/M9808Ajr9jXzQbCEHdC70NwNIYK1tATmMyRneVmQjJFBRxAIjQv4IaC1HjJptkL+8+JNUDBR7XnNPYYyGepr7WkAgefGe7y35dLlZQw7rs3B0sxkkv3zWtvj1Y1GwfRPCYkWQA/tFqthTYcO/SgkOleSt6wWlYRsiVdAkYHBBpi1sfRFSdG4f0uHKp7cUKQja7ukRexpPZq88u8Kwghwus/MP7+j8/eVwUShUiMPs7OLiPCuHFx/O32eohtlwUeI7IbLDfirpFdLBkRLKnjZa9PykQ2S//aIrHSHPEghkSEX2ElRhVKsvr5XZCGT0GGm5bWHZGG2Xd67ASB3Ew53FDWqDC0OQD5oE4tYJs9uTWDmnypl93pGNXm10c1SUYhtRW/IB8u+PgH4pH5AqtiUk0Keo+rsu9UWJUhuCeQK6wqUw8mjViny/0t5L2N4l+9980BtkPXF+mzGtjZmy0WoLOVyXE45TT6Ebg39o4tjHjtaB5ZR9hPz92SF6F+k5smIDOdyNp9Akb6SkUbnTtK+jZ9MuB0eJFUWvVXgmcZ6Ap8C1V9Q23Yg3QidKxV6KPTvPbrRI5umvmkJ3q1wNOQyyrGrXwC502EaKSKRqr+N2xDbSQ2z3gzF8P/V6ow0t6SooNO22gLxEEygBhQ4X2uioOypYFCLo5Orrj9+uJ+Mfs6vbb9ejK9Gw8OzkDo2BecsMiz+s2d4yx9+1od0I5NHX1CSwYVNXdMO13alSyecUowzfsVGOtbSlXqZdJjy9sMd8GaOv6hC5OoJq/6dvIM5FisKGg8fGVGJtWntxQbOjST1dNhwgB6Nt/SAaOa+5bbzBECYdga4bqTJ1iORT5XXUCg2ITH6jFX1USoqZ/GzjyIb8fqV/f4Q1CbHRLr9dw6EtIQF2Ein84OpBi0mkR1SWpCLkMOGZWlFRG6m8g5GqUs+Geqf1iI89m9QZtPS/Ilco9T8POZdqHRtebmdOpBmxlV2n95Zp99LsP+/LCh9ma7qHfPD0wOeW5Sm3FYfY+iWB+xXZOxsw6lDqbpHCmCccD4UK285HhyVT6uUNOiGiI1Uncu13XwKuc+vhRBrZBU24oE8snThEPR3Jcz+tuOaFQdktoic2p3npYTbYia3QHGb0tWFp5k3TNH8HAAD//5ocqteaCAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: orb-coredns
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2024-12-22T01:30:24Z"
    generation: 1
    labels:
      k8s-app: kube-dns
      pod-template-hash: 66cc6945cb
    name: coredns-66cc6945cb
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: coredns
      uid: a4c9c536-4215-4aca-bbd3-621b9edc121b
    resourceVersion: "7642946"
    uid: 5fa6d5db-e4b7-48d1-a432-a852c39f2db0
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: 66cc6945cb
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: 66cc6945cb
      spec:
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: rancher/mirrored-coredns-coredns:1.10.1
          imagePullPolicy: IfNotPresent
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          resources:
            limits:
              memory: 340Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
          - mountPath: /etc/coredns/custom
            name: custom-config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              k8s-app: kube-dns
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            - key: NodeHosts
              path: NodeHosts
            name: coredns
          name: config-volume
        - configMap:
            defaultMode: 420
            name: coredns-custom
            optional: true
          name: custom-config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xUUW/iOBD+K6d5TgIcLUKR7gG1Pd1pW4padV8qtBqcCbg4tmUP2UYo/301BFqqLe2utE+J7ZnP38z3jbeAXn+lELWzkAN6H3v1ABJYa1tADpfkjWsqsgwJVMRYICPkW0BrHSNrZ6Ms3eKJFEfiLGiXKWQ2lGnX0wICyclz991SSJf1GnJYD+PRST1I/vqibfHPpCic/RTCYkWQg3EKTRrZBVzSLyVFj0oy15sFpbGJTBW0CRhckPmwtBXGFeQwGA/L4bkanZflQg37o7NRf1ieDcvB+bhfjNVojH8XuCjOBPQNSY+8Sn1wtZbmU4Du/ASf6EkJm0Bd/H9aimyudaUZ8n4CkQwpdkGCKmS1un6pAL0/fWsr4ByQadnsLnDGaLt88AUydWDPDxZr1AYXhiAftAlw44Xj3ZtY2afKm0PekVvMb3DZF6qcZdSWQoT8UZZVhWLJx9Pti4xBfJqmytlSLyGBHrHqdav9J3uKzsI8AbL1Dnkvyuz28tt0cnN1P5tcXEECNZoN/RtcJWRKTaa4o/Llf4Ys4h9qzF6Va9t2noCuxH85BLRqRaH3Pue87mf9bCjztkuYbYyZOaNVAzn8X04dzwLFbvg+807tzKaiG7ex3HWskt89z+M2vGJ1G2mXCe1ciFtX0P2RlcSGwRJT3I1NFArabp5FbR+0C5qbC4MxTjvMzrGpwKQqaNYKjUhDodaKJkoJq+lHtaT72BS7YEiAnaFweGwet7AmadDFHn73QMRbaxoZeC+Rwh2unnXkCG2yBSpLUgw5TN29WlGxMfI4dDA7qsEZyt7WKgYMzqTeoKU/ilxh5J1m70DOD0oebC8S3aAXLX62wN7n7WlJ27b9EQAA//8eX0Hn5AUAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: local-storage
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-11-10T04:43:54Z"
    generation: 1
    labels:
      app: local-path-provisioner
      pod-template-hash: 777cb94f89
    name: local-path-provisioner-777cb94f89
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: local-path-provisioner
      uid: 259c9164-6e13-4270-b02a-993040ddbb3e
    resourceVersion: "7642963"
    uid: 3ea7ba0a-7d65-4ea9-a33f-4819c9bef530
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: local-path-provisioner
        pod-template-hash: 777cb94f89
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
          pod-template-hash: 777cb94f89
      spec:
        containers:
        - command:
          - local-path-provisioner
          - start
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/local-path-provisioner:v0.0.31
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-09-07T02:48:49Z"
    generation: 1
    labels:
      k8s-app: metrics-server
      pod-template-hash: 867d48dc9c
    name: metrics-server-867d48dc9c
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: metrics-server
      uid: 90af4325-5cd1-4c14-aaeb-1501abcb87ec
    resourceVersion: "7643367"
    uid: ff1b092e-aac8-4849-acea-29fa44749738
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: metrics-server
        pod-template-hash: 867d48dc9c
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
          pod-template-hash: 867d48dc9c
      spec:
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=10250
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --kubelet-use-node-status-port
          - --metric-resolution=15s
          image: registry.k8s.io/metrics-server/metrics-server:v0.8.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: metrics-server
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-11-08T03:25:56Z"
    generation: 2
    labels:
      app: clickhouse
      component: langfuse
      pod-template-hash: 58b6d94547
    name: langfuse-clickhouse-58b6d94547
    namespace: langfuse
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: langfuse-clickhouse
      uid: 41fe86c9-9bf2-4769-b21d-0552638a3e85
    resourceVersion: "6763242"
    uid: 8fdaa831-96a7-42c8-a686-f270dde77160
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: clickhouse
        component: langfuse
        pod-template-hash: 58b6d94547
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: clickhouse
          component: langfuse
          pod-template-hash: 58b6d94547
      spec:
        containers:
        - env:
          - name: CLICKHOUSE_DB
            value: langfuse
          - name: CLICKHOUSE_USER
            value: langfuse
          - name: CLICKHOUSE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: CLICKHOUSE_PASSWORD
                name: langfuse-secrets
                optional: true
          - name: CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT
            value: "1"
          image: clickhouse/clickhouse-server:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 8123
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: clickhouse
          ports:
          - containerPort: 8123
            name: http
            protocol: TCP
          - containerPort: 9000
            name: native
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 8123
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/clickhouse
            name: clickhouse-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: clickhouse-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-11-17T19:58:34Z"
    generation: 2
    labels:
      app: litellm
      pod-template-hash: 7f46445f6c
    name: litellm-proxy-7f46445f6c
    namespace: llm-inference
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: litellm-proxy
      uid: 48e2133f-521e-4608-a84a-48747914a30d
    resourceVersion: "7627032"
    uid: b88e92da-3b07-4687-8426-dc6dc2bc0d5c
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: litellm
        pod-template-hash: 7f46445f6c
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: litellm
          pod-template-hash: 7f46445f6c
      spec:
        containers:
        - args:
          - --config
          - /app/config.yaml
          - --port
          - "4000"
          - --num_workers
          - "4"
          command:
          - litellm
          image: ghcr.io/berriai/litellm:main-latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 4000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: litellm
          ports:
          - containerPort: 4000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 4000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app
            name: config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: litellm-config
          name: config
  status:
    fullyLabeledReplicas: 1
    observedGeneration: 2
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-11-17T19:58:34Z"
    generation: 2
    labels:
      app: ollama
      component: model-server
      pod-template-hash: 6b9758f96f
    name: ollama-6b9758f96f
    namespace: llm-inference
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: ollama
      uid: 5af01452-18e8-459e-bf22-96000eaca1c9
    resourceVersion: "7627154"
    uid: 9ddcfd7a-6453-4f80-bcd4-cb6fca81f9b6
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: ollama
        pod-template-hash: 6b9758f96f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: ollama
          component: model-server
          pod-template-hash: 6b9758f96f
      spec:
        containers:
        - env:
          - name: OLLAMA_HOST
            value: 0.0.0.0:11434
          - name: OLLAMA_MODELS
            value: /root/.ollama/models
          - name: OLLAMA_KEEP_ALIVE
            value: 24h
          - name: OLLAMA_NUM_PARALLEL
            value: "4"
          - name: OLLAMA_MAX_LOADED_MODELS
            value: "3"
          image: ollama/ollama:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 11434
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: ollama
          ports:
          - containerPort: 11434
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 11434
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "4"
              memory: 8Gi
            requests:
              cpu: "2"
              memory: 4Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /root/.ollama
            name: ollama-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: ollama-data
          persistentVolumeClaim:
            claimName: ollama-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-22T03:49:10Z"
    generation: 6
    labels:
      app: agent-brain
      pod-template-hash: 7bc545fd94
    name: agent-brain-7bc545fd94
    namespace: llm-platform
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: agent-brain
      uid: 4f0e0716-3e6c-4643-8396-5ba41722a8e9
    resourceVersion: "5375521"
    uid: bdbeef0c-4312-4714-8bd8-edd4679672e0
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: agent-brain
        pod-template-hash: 7bc545fd94
    template:
      metadata:
        annotations:
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: agent-brain
          pod-template-hash: 7bc545fd94
      spec:
        containers:
        - env:
          - name: PORT
            value: "3002"
          - name: NODE_ENV
            value: production
          image: agent-brain:0.1.0
          imagePullPolicy: Never
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3002
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: agent-brain
          ports:
          - containerPort: 3002
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3002
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 6
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-10-21T21:47:36Z"
    generation: 2
    labels:
      app: compliance-engine
      pod-template-hash: 5dbfc9d6fc
    name: compliance-engine-5dbfc9d6fc
    namespace: llm-platform
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: compliance-engine
      uid: 8beca30b-799e-4dd4-b8d8-5d2a3c9f09a8
    resourceVersion: "5349498"
    uid: 9336ea77-8acf-41bb-8db3-30cae3c9427c
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: compliance-engine
        pod-template-hash: 5dbfc9d6fc
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-21T17:47:36-04:00"
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: compliance-engine
          pod-template-hash: 5dbfc9d6fc
      spec:
        containers:
        - env:
          - name: API_KEY
            valueFrom:
              secretKeyRef:
                key: api-key
                name: compliance-engine-secrets
          - name: NODE_ENV
            value: production
          image: compliance-engine:0.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3100
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: compliance-engine
          ports:
          - containerPort: 3100
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3100
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2025-10-21T21:53:59Z"
    generation: 3
    labels:
      app: compliance-engine
      pod-template-hash: 6888d67b89
    name: compliance-engine-6888d67b89
    namespace: llm-platform
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: compliance-engine
      uid: 8beca30b-799e-4dd4-b8d8-5d2a3c9f09a8
    resourceVersion: "5352895"
    uid: f9588d3c-f8fc-47c4-8301-4cac6516a7f5
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: compliance-engine
        pod-template-hash: 6888d67b89
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-21T17:53:59-04:00"
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: compliance-engine
          pod-template-hash: 6888d67b89
      spec:
        containers:
        - env:
          - name: API_KEY
            valueFrom:
              secretKeyRef:
                key: api-key
                name: compliance-engine-secrets
          - name: NODE_ENV
            value: production
          image: compliance-engine:0.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3100
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: compliance-engine
          ports:
          - containerPort: 3100
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3100
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-21T20:24:25Z"
    generation: 3
    labels:
      app: compliance-engine
      pod-template-hash: 7fd5495c69
    name: compliance-engine-7fd5495c69
    namespace: llm-platform
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: compliance-engine
      uid: 8beca30b-799e-4dd4-b8d8-5d2a3c9f09a8
    resourceVersion: "5348826"
    uid: f8476270-31ae-4ff5-8c48-9af8bc49038e
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: compliance-engine
        pod-template-hash: 7fd5495c69
    template:
      metadata:
        annotations:
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: compliance-engine
          pod-template-hash: 7fd5495c69
      spec:
        containers:
        - env:
          - name: API_KEY
            valueFrom:
              secretKeyRef:
                key: api-key
                name: compliance-engine-secrets
          - name: NODE_ENV
            value: production
          image: compliance-engine:0.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3100
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: compliance-engine
          ports:
          - containerPort: 3100
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3100
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "5"
    creationTimestamp: "2025-10-22T02:33:02Z"
    generation: 4
    labels:
      app: compliance-engine
      pod-template-hash: c644bdc8c
    name: compliance-engine-c644bdc8c
    namespace: llm-platform
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: compliance-engine
      uid: 8beca30b-799e-4dd4-b8d8-5d2a3c9f09a8
    resourceVersion: "5375548"
    uid: 3739eb8f-1a0b-4395-b32b-8f0d493fa865
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: compliance-engine
        pod-template-hash: c644bdc8c
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-21T22:33:02-04:00"
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: compliance-engine
          pod-template-hash: c644bdc8c
      spec:
        containers:
        - env:
          - name: API_KEY
            valueFrom:
              secretKeyRef:
                key: api-key
                name: compliance-engine-secrets
          - name: NODE_ENV
            value: production
          image: compliance-engine:0.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3100
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: compliance-engine
          ports:
          - containerPort: 3100
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3100
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2025-10-21T21:49:18Z"
    generation: 5
    labels:
      app: compliance-engine
      pod-template-hash: df84c67cc
    name: compliance-engine-df84c67cc
    namespace: llm-platform
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: compliance-engine
      uid: 8beca30b-799e-4dd4-b8d8-5d2a3c9f09a8
    resourceVersion: "5352835"
    uid: ee795f24-b3f2-4643-a59d-c5c5f0d863f7
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: compliance-engine
        pod-template-hash: df84c67cc
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-21T17:47:36-04:00"
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: compliance-engine
          pod-template-hash: df84c67cc
      spec:
        containers:
        - env:
          - name: API_KEY
            valueFrom:
              secretKeyRef:
                key: api-key
                name: compliance-engine-secrets
          - name: NODE_ENV
            value: production
          image: compliance-engine:0.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3100
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: compliance-engine
          ports:
          - containerPort: 3100
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3100
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 5
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2025-10-21T21:49:18Z"
    generation: 5
    labels:
      app: workflow-engine
      pod-template-hash: 59b6c7cb4f
    name: workflow-engine-59b6c7cb4f
    namespace: llm-platform
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: workflow-engine
      uid: d5c35285-b0c6-4b6c-ba5b-bc9b5d666f84
    resourceVersion: "5352460"
    uid: a4cb3f54-b574-41ce-bc03-473d3e17545f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: workflow-engine
        pod-template-hash: 59b6c7cb4f
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-21T17:47:35-04:00"
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: workflow-engine
          pod-template-hash: 59b6c7cb4f
      spec:
        containers:
        - env:
          - name: REDIS_URL
            value: redis://redis:6379
          - name: API_KEY
            valueFrom:
              secretKeyRef:
                key: api-key
                name: workflow-engine-secrets
          - name: NODE_ENV
            value: production
          image: workflow-engine:0.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3001
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: workflow-engine
          ports:
          - containerPort: 3001
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3001
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 5
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2025-10-21T21:53:59Z"
    generation: 3
    labels:
      app: workflow-engine
      pod-template-hash: 5bd9c75dfd
    name: workflow-engine-5bd9c75dfd
    namespace: llm-platform
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: workflow-engine
      uid: d5c35285-b0c6-4b6c-ba5b-bc9b5d666f84
    resourceVersion: "5352526"
    uid: 888b1d16-66b0-48be-a3f5-cb4d15fbb27a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: workflow-engine
        pod-template-hash: 5bd9c75dfd
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-21T17:53:59-04:00"
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: workflow-engine
          pod-template-hash: 5bd9c75dfd
      spec:
        containers:
        - env:
          - name: REDIS_URL
            value: redis://redis:6379
          - name: API_KEY
            valueFrom:
              secretKeyRef:
                key: api-key
                name: workflow-engine-secrets
          - name: NODE_ENV
            value: production
          image: workflow-engine:0.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3001
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: workflow-engine
          ports:
          - containerPort: 3001
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3001
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "5"
    creationTimestamp: "2025-10-22T02:33:02Z"
    generation: 4
    labels:
      app: workflow-engine
      pod-template-hash: 6854ffbfd8
    name: workflow-engine-6854ffbfd8
    namespace: llm-platform
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: workflow-engine
      uid: d5c35285-b0c6-4b6c-ba5b-bc9b5d666f84
    resourceVersion: "5375544"
    uid: ee3908ac-ae02-4bb3-890d-9c596e482a43
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: workflow-engine
        pod-template-hash: 6854ffbfd8
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-21T22:33:02-04:00"
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: workflow-engine
          pod-template-hash: 6854ffbfd8
      spec:
        containers:
        - env:
          - name: REDIS_URL
            value: redis://redis:6379
          - name: API_KEY
            valueFrom:
              secretKeyRef:
                key: api-key
                name: workflow-engine-secrets
          - name: NODE_ENV
            value: production
          image: workflow-engine:0.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3001
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: workflow-engine
          ports:
          - containerPort: 3001
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3001
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-21T20:24:19Z"
    generation: 3
    labels:
      app: workflow-engine
      pod-template-hash: 6dd9b66846
    name: workflow-engine-6dd9b66846
    namespace: llm-platform
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: workflow-engine
      uid: d5c35285-b0c6-4b6c-ba5b-bc9b5d666f84
    resourceVersion: "5348812"
    uid: 2b6584a0-3d74-4dba-a3cf-5591cc8bbe4f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: workflow-engine
        pod-template-hash: 6dd9b66846
    template:
      metadata:
        annotations:
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: workflow-engine
          pod-template-hash: 6dd9b66846
      spec:
        containers:
        - env:
          - name: REDIS_URL
            value: redis://redis:6379
          - name: API_KEY
            valueFrom:
              secretKeyRef:
                key: api-key
                name: workflow-engine-secrets
          - name: NODE_ENV
            value: production
          image: workflow-engine:0.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3001
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: workflow-engine
          ports:
          - containerPort: 3001
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3001
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-10-21T21:47:36Z"
    generation: 2
    labels:
      app: workflow-engine
      pod-template-hash: 7fdf495557
    name: workflow-engine-7fdf495557
    namespace: llm-platform
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: workflow-engine
      uid: d5c35285-b0c6-4b6c-ba5b-bc9b5d666f84
    resourceVersion: "5349480"
    uid: d6e45e9d-ad76-465a-a7e0-995081e0eb71
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: workflow-engine
        pod-template-hash: 7fdf495557
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-21T17:47:35-04:00"
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: workflow-engine
          pod-template-hash: 7fdf495557
      spec:
        containers:
        - env:
          - name: REDIS_URL
            value: redis://redis:6379
          - name: API_KEY
            valueFrom:
              secretKeyRef:
                key: api-key
                name: workflow-engine-secrets
          - name: NODE_ENV
            value: production
          image: workflow-engine:0.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3001
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: workflow-engine
          ports:
          - containerPort: 3001
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3001
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-31T19:48:35Z"
    generation: 2
    labels:
      app.kubernetes.io/name: monitoring-placeholder
      monitoring: enabled
      pod-template-hash: 7b7f448b76
    name: monitoring-placeholder-7b7f448b76
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: monitoring-placeholder
      uid: 437d23b7-109b-4cd2-b2c4-57ddc120af40
    resourceVersion: "6763266"
    uid: a82e1bf0-630f-4288-b199-229ace9c07ad
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/name: monitoring-placeholder
        pod-template-hash: 7b7f448b76
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/name: monitoring-placeholder
          monitoring: enabled
          pod-template-hash: 7b7f448b76
      spec:
        containers:
        - image: nginx:alpine
          imagePullPolicy: IfNotPresent
          name: placeholder
          ports:
          - containerPort: 8080
            protocol: TCP
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-10-20T00:35:53Z"
    generation: 2
    labels:
      app: neo4j
      component: knowledge-graph
      pod-template-hash: 5f976f547f
    name: neo4j-5f976f547f
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: neo4j
      uid: ff7f013d-2bdf-4af7-989d-348bd37ed774
    resourceVersion: "5885550"
    uid: 5f154296-4109-42ac-b12e-44a616f8691d
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: neo4j
        pod-template-hash: 5f976f547f
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-19T20:35:53-04:00"
        creationTimestamp: null
        labels:
          app: neo4j
          component: knowledge-graph
          pod-template-hash: 5f976f547f
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: neo4j-config
          image: neo4j:5.24-community
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 7474
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 1
          name: neo4j
          ports:
          - containerPort: 7474
            name: http
            protocol: TCP
          - containerPort: 7687
            name: bolt
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 7474
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: neo4j-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: neo4j-data
          persistentVolumeClaim:
            claimName: neo4j-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2025-10-30T12:52:46Z"
    generation: 4
    labels:
      app: neo4j
      component: knowledge-graph
      pod-template-hash: 666b4c8f49
    name: neo4j-666b4c8f49
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: neo4j
      uid: ff7f013d-2bdf-4af7-989d-348bd37ed774
    resourceVersion: "6762109"
    uid: c2c3264d-814d-4671-b663-7ef050d8a9c5
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: neo4j
        pod-template-hash: 666b4c8f49
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-30T08:52:46-04:00"
        creationTimestamp: null
        labels:
          app: neo4j
          component: knowledge-graph
          pod-template-hash: 666b4c8f49
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: neo4j-config
          image: neo4j:5.24-community
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 7474
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 1
          name: neo4j
          ports:
          - containerPort: 7474
            name: http
            protocol: TCP
          - containerPort: 7687
            name: bolt
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 7474
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: neo4j-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: neo4j-data
          persistentVolumeClaim:
            claimName: neo4j-data
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-20T00:31:29Z"
    generation: 2
    labels:
      app: neo4j
      component: knowledge-graph
      pod-template-hash: 69f68cc575
    name: neo4j-69f68cc575
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: neo4j
      uid: ff7f013d-2bdf-4af7-989d-348bd37ed774
    resourceVersion: "5241086"
    uid: 735c0bee-8505-4e67-834b-35c9c93a0861
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: neo4j
        pod-template-hash: 69f68cc575
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: neo4j
          component: knowledge-graph
          pod-template-hash: 69f68cc575
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: neo4j-config
          image: neo4j:5.24-community
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 7474
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 1
          name: neo4j
          ports:
          - containerPort: 7474
            name: http
            protocol: TCP
          - containerPort: 7687
            name: bolt
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 7474
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: neo4j-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: neo4j-data
          persistentVolumeClaim:
            claimName: neo4j-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-28T14:28:29Z"
    generation: 2
    labels:
      app: phoenix
      pod-template-hash: 7c7c99f584
    name: phoenix-7c7c99f584
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: phoenix
      uid: 194a71b0-1b18-46a4-906a-0c7a6a0bc835
    resourceVersion: "6763254"
    uid: c55ca05f-0afd-486b-ac92-900ce0d288fb
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: phoenix
        pod-template-hash: 7c7c99f584
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: phoenix
          pod-template-hash: 7c7c99f584
      spec:
        containers:
        - env:
          - name: PHOENIX_WORKING_DIR
            value: /phoenix-data
          - name: PHOENIX_PORT
            value: "6006"
          - name: PHOENIX_GRPC_PORT
            value: "4317"
          - name: PHOENIX_SQL_DATABASE_URL
            value: sqlite:////phoenix-data/phoenix.db
          image: arizephoenix/phoenix:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 6006
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: phoenix
          ports:
          - containerPort: 6006
            name: http
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 6006
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /phoenix-data
            name: phoenix-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: phoenix-data
          persistentVolumeClaim:
            claimName: phoenix-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "11"
    creationTimestamp: "2025-10-10T17:53:42Z"
    generation: 2
    labels:
      app: agent-router
      component: gateway
      pod-template-hash: 5b94c4cddf
      tier: api
      version: v0.1.0-gitlab
    name: agent-router-5b94c4cddf
    namespace: ossa-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: agent-router
      uid: c13c9431-a120-4e14-9aad-13b8513a3ecf
    resourceVersion: "5744210"
    uid: e0681a74-d8e4-4c01-bea1-0d9a20c86e9c
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: agent-router
        pod-template-hash: 5b94c4cddf
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-10T13:53:42-04:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "3000"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: agent-router
          component: gateway
          pod-template-hash: 5b94c4cddf
          tier: api
          version: v0.1.0-gitlab
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: PORT
            value: "3000"
          - name: LOG_LEVEL
            value: info
          - name: METRICS_ENABLED
            value: "true"
          - name: CORS_ORIGIN
            value: '*'
          - name: PHOENIX_ENABLED
            value: "true"
          - name: PHOENIX_ENDPOINT
            value: http://192.168.139.2:6006
          - name: PHOENIX_OTLP_ENDPOINT
            value: http://192.168.139.2:6006/v1/traces
          - name: OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://192.168.139.2:6006/v1/traces
          - name: OTEL_SERVICE_NAME
            value: agent-router
          - name: PHOENIX_PROJECT_NAME
            value: agent-router
          - name: PHOENIX_TELEMETRY
            value: "true"
          - name: PHOENIX_OTLP_TIMEOUT
            value: "30000"
          - name: GITLAB_URL
            valueFrom:
              configMapKeyRef:
                key: GITLAB_URL
                name: agent-router-gitlab-config
          - name: GITLAB_DEFAULT_PROJECT_ID
            valueFrom:
              configMapKeyRef:
                key: GITLAB_DEFAULT_PROJECT_ID
                name: agent-router-gitlab-config
          - name: GITLAB_DEFAULT_LABELS
            valueFrom:
              configMapKeyRef:
                key: GITLAB_DEFAULT_LABELS
                name: agent-router-gitlab-config
          - name: GITLAB_AUTO_TRIAGE
            valueFrom:
              configMapKeyRef:
                key: GITLAB_AUTO_TRIAGE
                name: agent-router-gitlab-config
          - name: GITLAB_AUTO_ASSIGN
            valueFrom:
              configMapKeyRef:
                key: GITLAB_AUTO_ASSIGN
                name: agent-router-gitlab-config
          - name: GITLAB_TOKEN
            valueFrom:
              secretKeyRef:
                key: GITLAB_TOKEN
                name: agent-router-gitlab-secret
          image: agent-router:latest
          imagePullPolicy: Never
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: agent-router
          ports:
          - containerPort: 3000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 250m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: false
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "10"
    creationTimestamp: "2025-10-10T17:25:53Z"
    generation: 2
    labels:
      app: agent-router
      component: gateway
      pod-template-hash: 6f689678d
      tier: api
      version: v0.1.0-gitlab
    name: agent-router-6f689678d
    namespace: ossa-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: agent-router
      uid: c13c9431-a120-4e14-9aad-13b8513a3ecf
    resourceVersion: "4948294"
    uid: e3ba952e-afa3-4d02-b56e-0c90f6d157c2
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: agent-router
        pod-template-hash: 6f689678d
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-10T13:25:53-04:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "3000"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: agent-router
          component: gateway
          pod-template-hash: 6f689678d
          tier: api
          version: v0.1.0-gitlab
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: PORT
            value: "3000"
          - name: LOG_LEVEL
            value: info
          - name: METRICS_ENABLED
            value: "true"
          - name: CORS_ORIGIN
            value: '*'
          - name: PHOENIX_ENABLED
            value: "true"
          - name: PHOENIX_ENDPOINT
            value: http://192.168.139.2:6006
          - name: PHOENIX_OTLP_ENDPOINT
            value: http://192.168.139.2:6006/v1/traces
          - name: OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://192.168.139.2:6006/v1/traces
          - name: OTEL_SERVICE_NAME
            value: agent-router
          - name: PHOENIX_PROJECT_NAME
            value: agent-router
          - name: PHOENIX_TELEMETRY
            value: "true"
          - name: PHOENIX_OTLP_TIMEOUT
            value: "30000"
          - name: GITLAB_URL
            valueFrom:
              configMapKeyRef:
                key: GITLAB_URL
                name: agent-router-gitlab-config
          - name: GITLAB_DEFAULT_PROJECT_ID
            valueFrom:
              configMapKeyRef:
                key: GITLAB_DEFAULT_PROJECT_ID
                name: agent-router-gitlab-config
          - name: GITLAB_DEFAULT_LABELS
            valueFrom:
              configMapKeyRef:
                key: GITLAB_DEFAULT_LABELS
                name: agent-router-gitlab-config
          - name: GITLAB_AUTO_TRIAGE
            valueFrom:
              configMapKeyRef:
                key: GITLAB_AUTO_TRIAGE
                name: agent-router-gitlab-config
          - name: GITLAB_AUTO_ASSIGN
            valueFrom:
              configMapKeyRef:
                key: GITLAB_AUTO_ASSIGN
                name: agent-router-gitlab-config
          - name: GITLAB_TOKEN
            valueFrom:
              secretKeyRef:
                key: GITLAB_TOKEN
                name: agent-router-gitlab-secret
          image: agent-router:latest
          imagePullPolicy: Never
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: agent-router
          ports:
          - containerPort: 3000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 250m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: false
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "9"
    creationTimestamp: "2025-10-10T17:22:06Z"
    generation: 2
    labels:
      app: agent-router
      component: gateway
      pod-template-hash: 857958ccd9
      tier: api
      version: v0.1.0-gitlab
    name: agent-router-857958ccd9
    namespace: ossa-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: agent-router
      uid: c13c9431-a120-4e14-9aad-13b8513a3ecf
    resourceVersion: "4947077"
    uid: 545fa545-878d-45ea-8ba9-34e44807c89a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: agent-router
        pod-template-hash: 857958ccd9
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-10T13:22:06-04:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "3000"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: agent-router
          component: gateway
          pod-template-hash: 857958ccd9
          tier: api
          version: v0.1.0-gitlab
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: PORT
            value: "3000"
          - name: LOG_LEVEL
            value: info
          - name: METRICS_ENABLED
            value: "true"
          - name: CORS_ORIGIN
            value: '*'
          - name: PHOENIX_ENABLED
            value: "true"
          - name: PHOENIX_ENDPOINT
            value: http://192.168.139.2:6006
          - name: PHOENIX_OTLP_ENDPOINT
            value: http://192.168.139.2:6006/v1/traces
          - name: OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://192.168.139.2:6006/v1/traces
          - name: OTEL_SERVICE_NAME
            value: agent-router
          - name: PHOENIX_PROJECT_NAME
            value: agent-router
          - name: PHOENIX_TELEMETRY
            value: "true"
          - name: PHOENIX_OTLP_TIMEOUT
            value: "30000"
          - name: GITLAB_URL
            valueFrom:
              configMapKeyRef:
                key: GITLAB_URL
                name: agent-router-gitlab-config
          - name: GITLAB_DEFAULT_PROJECT_ID
            valueFrom:
              configMapKeyRef:
                key: GITLAB_DEFAULT_PROJECT_ID
                name: agent-router-gitlab-config
          - name: GITLAB_DEFAULT_LABELS
            valueFrom:
              configMapKeyRef:
                key: GITLAB_DEFAULT_LABELS
                name: agent-router-gitlab-config
          - name: GITLAB_AUTO_TRIAGE
            valueFrom:
              configMapKeyRef:
                key: GITLAB_AUTO_TRIAGE
                name: agent-router-gitlab-config
          - name: GITLAB_AUTO_ASSIGN
            valueFrom:
              configMapKeyRef:
                key: GITLAB_AUTO_ASSIGN
                name: agent-router-gitlab-config
          - name: GITLAB_TOKEN
            valueFrom:
              secretKeyRef:
                key: GITLAB_TOKEN
                name: agent-router-gitlab-secret
          image: agent-router:latest
          imagePullPolicy: Never
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: agent-router
          ports:
          - containerPort: 3000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 250m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: false
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "12"
    creationTimestamp: "2025-10-28T14:44:10Z"
    generation: 2
    labels:
      app: agent-router
      component: gateway
      pod-template-hash: 9d69cdc6c
      tier: api
      version: v0.1.0-gitlab
    name: agent-router-9d69cdc6c
    namespace: ossa-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: agent-router
      uid: c13c9431-a120-4e14-9aad-13b8513a3ecf
    resourceVersion: "6175926"
    uid: 302fdebd-9dfa-4eee-bbdf-89fcbd4ac371
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: agent-router
        pod-template-hash: 9d69cdc6c
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-10T13:53:42-04:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "3000"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: agent-router
          component: gateway
          pod-template-hash: 9d69cdc6c
          tier: api
          version: v0.1.0-gitlab
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: development
          - name: PORT
            value: "3000"
          - name: LOG_LEVEL
            value: info
          - name: METRICS_ENABLED
            value: "true"
          - name: CORS_ORIGIN
            value: '*'
          - name: PHOENIX_ENABLED
            value: "true"
          - name: PHOENIX_ENDPOINT
            value: http://192.168.139.2:6006
          - name: PHOENIX_OTLP_ENDPOINT
            value: http://192.168.139.2:6006/v1/traces
          - name: OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://192.168.139.2:6006/v1/traces
          - name: OTEL_SERVICE_NAME
            value: agent-router
          - name: PHOENIX_PROJECT_NAME
            value: agent-router
          - name: PHOENIX_TELEMETRY
            value: "true"
          - name: PHOENIX_OTLP_TIMEOUT
            value: "30000"
          - name: GITLAB_URL
            valueFrom:
              configMapKeyRef:
                key: GITLAB_URL
                name: agent-router-gitlab-config
          - name: GITLAB_DEFAULT_PROJECT_ID
            valueFrom:
              configMapKeyRef:
                key: GITLAB_DEFAULT_PROJECT_ID
                name: agent-router-gitlab-config
          - name: GITLAB_DEFAULT_LABELS
            valueFrom:
              configMapKeyRef:
                key: GITLAB_DEFAULT_LABELS
                name: agent-router-gitlab-config
          - name: GITLAB_AUTO_TRIAGE
            valueFrom:
              configMapKeyRef:
                key: GITLAB_AUTO_TRIAGE
                name: agent-router-gitlab-config
          - name: GITLAB_AUTO_ASSIGN
            valueFrom:
              configMapKeyRef:
                key: GITLAB_AUTO_ASSIGN
                name: agent-router-gitlab-config
          - name: GITLAB_TOKEN
            valueFrom:
              secretKeyRef:
                key: GITLAB_TOKEN
                name: agent-router-gitlab-secret
          - name: PHOENIX_COLLECTOR_ENDPOINT
            value: http://phoenix.observability.svc.cluster.local:4318/v1/traces
          image: agent-router:latest
          imagePullPolicy: Never
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: agent-router
          ports:
          - containerPort: 3000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 250m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: false
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-11-04T01:15:32Z"
    generation: 2
    labels:
      app: grafana
      pod-template-hash: 6ff7698ff5
    name: grafana-6ff7698ff5
    namespace: ossa-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: grafana
      uid: 20448d4a-9ba8-4f45-9d7f-ba13bf0865f2
    resourceVersion: "7453333"
    uid: ffedbe31-1368-47ce-9cb8-fdeb52cdccd9
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: grafana
        pod-template-hash: 6ff7698ff5
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: grafana
          pod-template-hash: 6ff7698ff5
      spec:
        containers:
        - env:
          - name: GF_SECURITY_ADMIN_PASSWORD
            value: ossa-admin
          - name: GF_SECURITY_ADMIN_USER
            value: admin
          - name: GF_USERS_ALLOW_SIGN_UP
            value: "false"
          - name: GF_AUTH_ANONYMOUS_ENABLED
            value: "false"
          image: grafana/grafana:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: grafana
          ports:
          - containerPort: 3000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 100m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: grafana-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: grafana-storage
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-11-16T23:27:35Z"
    generation: 2
    labels:
      app: grafana
      pod-template-hash: 855c5b68c4
    name: grafana-855c5b68c4
    namespace: ossa-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: grafana
      uid: 20448d4a-9ba8-4f45-9d7f-ba13bf0865f2
    resourceVersion: "7558535"
    uid: 73c3c9b3-f10b-4489-ba8e-5b136c742cbe
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: grafana
        pod-template-hash: 855c5b68c4
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-11-16T18:27:35-05:00"
        creationTimestamp: null
        labels:
          app: grafana
          pod-template-hash: 855c5b68c4
      spec:
        containers:
        - env:
          - name: GF_SECURITY_ADMIN_PASSWORD
            value: ossa-admin
          - name: GF_SECURITY_ADMIN_USER
            value: admin
          - name: GF_USERS_ALLOW_SIGN_UP
            value: "false"
          - name: GF_AUTH_ANONYMOUS_ENABLED
            value: "false"
          image: grafana/grafana:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: grafana
          ports:
          - containerPort: 3000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 100m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: grafana-storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: grafana-storage
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "19"
    creationTimestamp: "2025-10-30T22:07:04Z"
    generation: 2
    labels:
      app: prometheus
      pod-template-hash: 58d78b4dfc
    name: prometheus-58d78b4dfc
    namespace: ossa-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus
      uid: 84f8f118-41c5-4fb8-a158-fb38c3e3c8b2
    resourceVersion: "5934823"
    uid: 578a26c5-fdc1-4c26-83a3-b84f4209364b
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: prometheus
        pod-template-hash: 58d78b4dfc
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-30T18:07:04-04:00"
        creationTimestamp: null
        labels:
          app: prometheus
          pod-template-hash: 58d78b4dfc
      spec:
        containers:
        - args:
          - --config.file=/etc/prometheus/prometheus.yml
          - --storage.tsdb.path=/prometheus
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --storage.tsdb.retention.time=15d
          - --web.enable-lifecycle
          image: prom/prometheus:v2.45.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/prometheus
            name: prometheus-config
            readOnly: true
          - mountPath: /prometheus
            name: prometheus-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-config
          name: prometheus-config
        - name: prometheus-data
          persistentVolumeClaim:
            claimName: prometheus-pvc
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "20"
    creationTimestamp: "2025-10-31T10:46:12Z"
    generation: 2
    labels:
      app: prometheus
      pod-template-hash: 5c4bc9cfbb
    name: prometheus-5c4bc9cfbb
    namespace: ossa-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus
      uid: 84f8f118-41c5-4fb8-a158-fb38c3e3c8b2
    resourceVersion: "6172487"
    uid: 6c9f877d-1b78-49ae-b2ea-067473b64054
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: prometheus
        pod-template-hash: 5c4bc9cfbb
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-30T18:07:04-04:00"
        creationTimestamp: null
        labels:
          app: prometheus
          pod-template-hash: 5c4bc9cfbb
      spec:
        containers:
        - args:
          - --config.file=/etc/prometheus/prometheus.yml
          - --storage.tsdb.path=/prometheus
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --storage.tsdb.retention.time=15d
          - --web.enable-lifecycle
          image: prom/prometheus:v2.45.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/prometheus
            name: prometheus-config
            readOnly: true
          - mountPath: /prometheus
            name: prometheus-data
          - mountPath: /etc/prometheus/rules
            name: prometheus-alerts
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-config
          name: prometheus-config
        - name: prometheus-data
          persistentVolumeClaim:
            claimName: prometheus-pvc
        - configMap:
            defaultMode: 420
            name: prometheus-alerts
          name: prometheus-alerts
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "15"
    creationTimestamp: "2025-10-10T17:49:33Z"
    generation: 2
    labels:
      app: prometheus
      pod-template-hash: 6495ffc654
    name: prometheus-6495ffc654
    namespace: ossa-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus
      uid: 84f8f118-41c5-4fb8-a158-fb38c3e3c8b2
    resourceVersion: "4948142"
    uid: 6289889c-12b6-4424-8274-56ffa8514d2c
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: prometheus
        pod-template-hash: 6495ffc654
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-10T13:49:33-04:00"
        creationTimestamp: null
        labels:
          app: prometheus
          pod-template-hash: 6495ffc654
      spec:
        containers:
        - args:
          - --config.file=/etc/prometheus/prometheus.yml
          - --storage.tsdb.path=/prometheus
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --storage.tsdb.retention.time=15d
          - --web.enable-lifecycle
          image: prom/prometheus:v2.45.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/prometheus
            name: prometheus-config
            readOnly: true
          - mountPath: /prometheus
            name: prometheus-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-config
          name: prometheus-config
        - name: prometheus-data
          persistentVolumeClaim:
            claimName: prometheus-pvc
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "21"
    creationTimestamp: "2025-11-04T02:13:31Z"
    generation: 4
    labels:
      app: prometheus
      pod-template-hash: 68cfbbc496
    name: prometheus-68cfbbc496
    namespace: ossa-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus
      uid: 84f8f118-41c5-4fb8-a158-fb38c3e3c8b2
    resourceVersion: "6174975"
    uid: 6cf41cb4-2a0b-407d-8c83-1e2001f1d7f1
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: prometheus
        pod-template-hash: 68cfbbc496
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-30T18:07:04-04:00"
        creationTimestamp: null
        labels:
          app: prometheus
          pod-template-hash: 68cfbbc496
      spec:
        containers:
        - args:
          - --config.file=/etc/prometheus/prometheus.yml
          - --storage.tsdb.path=/prometheus
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --storage.tsdb.retention.time=15d
          - --web.enable-lifecycle
          image: prom/prometheus:v2.45.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 50m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/prometheus
            name: prometheus-config
            readOnly: true
          - mountPath: /prometheus
            name: prometheus-data
          - mountPath: /etc/prometheus/rules
            name: prometheus-alerts
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-config
          name: prometheus-config
        - name: prometheus-data
          persistentVolumeClaim:
            claimName: prometheus-pvc
        - configMap:
            defaultMode: 420
            name: prometheus-alerts
          name: prometheus-alerts
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "22"
    creationTimestamp: "2025-11-04T03:05:30Z"
    generation: 3
    labels:
      app: prometheus
      pod-template-hash: 7666cd7768
    name: prometheus-7666cd7768
    namespace: ossa-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus
      uid: 84f8f118-41c5-4fb8-a158-fb38c3e3c8b2
    resourceVersion: "7558532"
    uid: a0d99ca1-bb80-4b61-874d-5a20cf57ecab
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: prometheus
        pod-template-hash: 7666cd7768
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-30T18:07:04-04:00"
        creationTimestamp: null
        labels:
          app: prometheus
          pod-template-hash: 7666cd7768
      spec:
        containers:
        - args:
          - --config.file=/etc/prometheus/prometheus.yml
          - --storage.tsdb.path=/prometheus
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --storage.tsdb.retention.time=15d
          - --web.enable-lifecycle
          image: prom/prometheus:v2.45.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 10m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/prometheus
            name: prometheus-config
            readOnly: true
          - mountPath: /prometheus
            name: prometheus-data
          - mountPath: /etc/prometheus/rules
            name: prometheus-alerts
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-config
          name: prometheus-config
        - name: prometheus-data
          persistentVolumeClaim:
            claimName: prometheus-pvc
        - configMap:
            defaultMode: 420
            name: prometheus-alerts
          name: prometheus-alerts
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "18"
      deployment.kubernetes.io/revision-history: 14,16
    creationTimestamp: "2025-10-10T16:46:44Z"
    generation: 2
    labels:
      app: prometheus
      pod-template-hash: b5887f696
    name: prometheus-b5887f696
    namespace: ossa-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus
      uid: 84f8f118-41c5-4fb8-a158-fb38c3e3c8b2
    resourceVersion: "5910085"
    uid: cf3d2883-66e8-4042-b903-ba02e2b1f643
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: prometheus
        pod-template-hash: b5887f696
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-10T12:46:44-04:00"
        creationTimestamp: null
        labels:
          app: prometheus
          pod-template-hash: b5887f696
      spec:
        containers:
        - args:
          - --config.file=/etc/prometheus/prometheus.yml
          - --storage.tsdb.path=/prometheus
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --storage.tsdb.retention.time=15d
          - --web.enable-lifecycle
          image: prom/prometheus:v2.45.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/prometheus
            name: prometheus-config
            readOnly: true
          - mountPath: /prometheus
            name: prometheus-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-config
          name: prometheus-config
        - name: prometheus-data
          persistentVolumeClaim:
            claimName: prometheus-pvc
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "13"
    creationTimestamp: "2025-10-10T14:01:51Z"
    generation: 4
    labels:
      app: prometheus
      pod-template-hash: db75fcc87
    name: prometheus-db75fcc87
    namespace: ossa-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus
      uid: 84f8f118-41c5-4fb8-a158-fb38c3e3c8b2
    resourceVersion: "4945275"
    uid: d99808c6-8ce5-44c6-ac21-bcef65327bdc
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: prometheus
        pod-template-hash: db75fcc87
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-10T10:01:51-04:00"
        creationTimestamp: null
        labels:
          app: prometheus
          pod-template-hash: db75fcc87
      spec:
        containers:
        - args:
          - --config.file=/etc/prometheus/prometheus.yml
          - --storage.tsdb.path=/prometheus
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --storage.tsdb.retention.time=15d
          - --web.enable-lifecycle
          image: prom/prometheus:v2.45.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/prometheus
            name: prometheus-config
            readOnly: true
          - mountPath: /prometheus
            name: prometheus-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-config
          name: prometheus-config
        - name: prometheus-data
          persistentVolumeClaim:
            claimName: prometheus-pvc
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "17"
    creationTimestamp: "2025-10-30T17:24:16Z"
    generation: 2
    labels:
      app: prometheus
      pod-template-hash: fdf76895
    name: prometheus-fdf76895
    namespace: ossa-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus
      uid: 84f8f118-41c5-4fb8-a158-fb38c3e3c8b2
    resourceVersion: "5906088"
    uid: bcec06a3-ad13-4802-9266-9bc88898fe55
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: prometheus
        pod-template-hash: fdf76895
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-30T13:24:15-04:00"
        creationTimestamp: null
        labels:
          app: prometheus
          pod-template-hash: fdf76895
      spec:
        containers:
        - args:
          - --config.file=/etc/prometheus/prometheus.yml
          - --storage.tsdb.path=/prometheus
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --storage.tsdb.retention.time=15d
          - --web.enable-lifecycle
          image: prom/prometheus:v2.45.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/prometheus
            name: prometheus-config
            readOnly: true
          - mountPath: /prometheus
            name: prometheus-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-config
          name: prometheus-config
        - name: prometheus-data
          persistentVolumeClaim:
            claimName: prometheus-pvc
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-06T23:37:30Z"
    generation: 2
    labels:
      app: qdrant
      pod-template-hash: 5486f49dcf
    name: qdrant-5486f49dcf
    namespace: ossa-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: qdrant
      uid: 89374767-7169-43cf-98f9-c4f093750220
    resourceVersion: "4674237"
    uid: f1636cdc-652f-40b3-b426-272cc265864b
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: qdrant
        pod-template-hash: 5486f49dcf
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: qdrant
          pod-template-hash: 5486f49dcf
      spec:
        containers:
        - env:
          - name: QDRANT__SERVICE__HTTP_PORT
            value: "6333"
          - name: QDRANT__SERVICE__GRPC_PORT
            value: "6334"
          image: qdrant/qdrant:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 6333
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: qdrant
          ports:
          - containerPort: 6333
            protocol: TCP
          - containerPort: 6334
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 6333
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-10-06T23:41:34Z"
    generation: 2
    labels:
      app: qdrant
      pod-template-hash: 57d7446f9c
    name: qdrant-57d7446f9c
    namespace: ossa-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: qdrant
      uid: 89374767-7169-43cf-98f9-c4f093750220
    resourceVersion: "4674292"
    uid: f033aa7b-5adb-4d51-974e-7abf5c82cd22
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: qdrant
        pod-template-hash: 57d7446f9c
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: qdrant
          pod-template-hash: 57d7446f9c
      spec:
        containers:
        - env:
          - name: QDRANT__SERVICE__HTTP_PORT
            value: "6333"
          - name: QDRANT__SERVICE__GRPC_PORT
            value: "6334"
          image: qdrant/qdrant:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 6333
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: qdrant
          ports:
          - containerPort: 6333
            protocol: TCP
          - containerPort: 6334
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 6333
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2025-10-06T23:41:38Z"
    generation: 3
    labels:
      app: qdrant
      pod-template-hash: 7ff67cbff5
    name: qdrant-7ff67cbff5
    namespace: ossa-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: qdrant
      uid: 89374767-7169-43cf-98f9-c4f093750220
    resourceVersion: "6762123"
    uid: e138e0f0-ef54-4c0b-a5eb-ea76492ff731
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: qdrant
        pod-template-hash: 7ff67cbff5
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: qdrant
          pod-template-hash: 7ff67cbff5
      spec:
        containers:
        - env:
          - name: QDRANT__SERVICE__HTTP_PORT
            value: "6333"
          - name: QDRANT__SERVICE__GRPC_PORT
            value: "6334"
          image: qdrant/qdrant:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 6333
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: qdrant
          ports:
          - containerPort: 6333
            protocol: TCP
          - containerPort: 6334
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 6333
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2025-11-10T04:43:55Z"
    generation: 1
    labels:
      app: agent-gateway
      pod-template-hash: 64d665d798
      version: 0.2.0
    name: agent-gateway-64d665d798
    namespace: ossa-prod
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: agent-gateway
      uid: d6ecba74-58ca-4521-9ac6-75eccd815f4b
    resourceVersion: "7643026"
    uid: e9a989e6-fd08-4379-b9db-24577d1e03b7
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agent-gateway
        pod-template-hash: 64d665d798
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: agent-gateway
          pod-template-hash: 64d665d798
          version: 0.2.0
      spec:
        containers:
        - command:
          - node
          - -e
          - require('http').createServer((req,res)=>res.end(JSON.stringify({status:'healthy',service:'agent-gateway',agents:129}))).listen(3000)
          env:
          - name: PHOENIX_COLLECTOR_ENDPOINT
            value: http://phoenix.observability.svc.cluster.local:4318/v1/traces
          - name: NODE_ENV
            value: development
          image: node:20-alpine
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: agent-gateway
          ports:
          - containerPort: 3000
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-10-28T14:45:36Z"
    generation: 2
    labels:
      app: agent-gateway
      pod-template-hash: 7775d648d4
      version: 0.2.0
    name: agent-gateway-7775d648d4
    namespace: ossa-prod
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: agent-gateway
      uid: d6ecba74-58ca-4521-9ac6-75eccd815f4b
    resourceVersion: "6759085"
    uid: b8792fb1-e936-4998-9e9f-e6862b59cb86
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: agent-gateway
        pod-template-hash: 7775d648d4
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: agent-gateway
          pod-template-hash: 7775d648d4
          version: 0.2.0
      spec:
        containers:
        - command:
          - node
          - -e
          - require('http').createServer((req,res)=>res.end(JSON.stringify({status:'healthy',service:'agent-gateway',agents:129}))).listen(3000)
          env:
          - name: PHOENIX_COLLECTOR_ENDPOINT
            value: http://phoenix.observability.svc.cluster.local:4318/v1/traces
          - name: NODE_ENV
            value: development
          image: node:20-alpine
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: agent-gateway
          ports:
          - containerPort: 3000
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-07T14:51:23Z"
    generation: 2
    labels:
      app: agent-gateway
      pod-template-hash: 85bd86db59
      version: 0.2.0
    name: agent-gateway-85bd86db59
    namespace: ossa-prod
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: agent-gateway
      uid: d6ecba74-58ca-4521-9ac6-75eccd815f4b
    resourceVersion: "5744429"
    uid: 10c48266-d86d-46a8-9440-936c85526826
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: agent-gateway
        pod-template-hash: 85bd86db59
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: agent-gateway
          pod-template-hash: 85bd86db59
          version: 0.2.0
      spec:
        containers:
        - command:
          - node
          - -e
          - require('http').createServer((req,res)=>res.end(JSON.stringify({status:'healthy',service:'agent-gateway',agents:129}))).listen(3000)
          image: node:20-alpine
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: agent-gateway
          ports:
          - containerPort: 3000
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-09T20:34:12Z"
    generation: 2
    labels:
      app: phoenix
      pod-template-hash: 5d56cbb768
    name: phoenix-5d56cbb768
    namespace: phoenix
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: phoenix
      uid: 8df943f2-88c9-461b-b721-bfcc0073ee23
    resourceVersion: "4939071"
    uid: 55522f36-f2b8-4828-b115-fff77caabe52
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: phoenix
        pod-template-hash: 5d56cbb768
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: phoenix
          pod-template-hash: 5d56cbb768
      spec:
        containers:
        - env:
          - name: PHOENIX_WORKING_DIR
            value: /data
          - name: PHOENIX_PORT
            value: "6006"
          - name: PHOENIX_GRPC_PORT
            value: "4317"
          - name: PHOENIX_HOST
            value: 0.0.0.0
          - name: PHOENIX_COLLECTOR_ENDPOINT
            value: https://api.arize.com/v1
          - name: ARIZE_API_KEY
            valueFrom:
              secretKeyRef:
                key: api-key
                name: arize-credentials
                optional: true
          image: arizephoenix/phoenix:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 6006
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: phoenix
          ports:
          - containerPort: 6006
            name: http
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 6006
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: data
          persistentVolumeClaim:
            claimName: phoenix-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-10-10T14:38:38Z"
    generation: 2
    labels:
      app: phoenix
      pod-template-hash: 6fb5f4d55
    name: phoenix-6fb5f4d55
    namespace: phoenix
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: phoenix
      uid: 8df943f2-88c9-461b-b721-bfcc0073ee23
    resourceVersion: "6762134"
    uid: a898fa89-a4ee-429c-b0ce-1dd39c2f8d6d
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: phoenix
        pod-template-hash: 6fb5f4d55
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: phoenix
          pod-template-hash: 6fb5f4d55
      spec:
        containers:
        - env:
          - name: PHOENIX_WORKING_DIR
            value: /data
          - name: PHOENIX_PORT
            value: "6006"
          - name: PHOENIX_GRPC_PORT
            value: "4317"
          - name: PHOENIX_HOST
            value: 0.0.0.0
          - name: ARIZE_API_KEY
            valueFrom:
              secretKeyRef:
                key: api-key
                name: arize-credentials
                optional: true
          image: arizephoenix/phoenix:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 6006
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: phoenix
          ports:
          - containerPort: 6006
            name: http
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 6006
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: data
          persistentVolumeClaim:
            claimName: phoenix-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-10-21T21:47:36Z"
    generation: 2
    labels:
      app: aiflow-agent
      pod-template-hash: 7cd976d77b
    name: aiflow-agent-7cd976d77b
    namespace: social-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: aiflow-agent
      uid: 3f9219d6-0f03-41c1-b050-a3cd06a55ffe
    resourceVersion: "5375682"
    uid: fcb7fa59-dbd7-4a69-aaff-8597d130059d
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: aiflow-agent
        pod-template-hash: 7cd976d77b
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-21T17:47:36-04:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: aiflow-agent
          pod-template-hash: 7cd976d77b
      spec:
        containers:
        - env:
          - name: CHARACTER_FILE
            value: /app/characters/AIFlow.json
          - name: DB_PATH
            valueFrom:
              secretKeyRef:
                key: db-connection-string
                name: aiflow-secrets
          - name: AIFLOW_API_KEY
            valueFrom:
              secretKeyRef:
                key: api-key
                name: aiflow-secrets
          - name: TW_API_KEY
            valueFrom:
              secretKeyRef:
                key: twitter-api-key
                name: aiflow-secrets
                optional: true
          - name: TW_API_KEY_SECRET
            valueFrom:
              secretKeyRef:
                key: twitter-api-key-secret
                name: aiflow-secrets
                optional: true
          - name: TW_ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                key: twitter-access-token
                name: aiflow-secrets
                optional: true
          - name: TW_ACCESS_TOKEN_SECRET
            valueFrom:
              secretKeyRef:
                key: twitter-access-token-secret
                name: aiflow-secrets
                optional: true
          - name: TW_BEARER_TOKEN
            valueFrom:
              secretKeyRef:
                key: twitter-bearer-token
                name: aiflow-secrets
                optional: true
          - name: TG_BOT_TOKEN
            valueFrom:
              secretKeyRef:
                key: telegram-bot-token
                name: aiflow-secrets
                optional: true
          - name: ANTHROPIC_API_KEY
            valueFrom:
              secretKeyRef:
                key: anthropic-api-key
                name: aiflow-secrets
                optional: true
          - name: OPENAI_API_KEY
            valueFrom:
              secretKeyRef:
                key: openai-api-key
                name: aiflow-secrets
                optional: true
          image: aiflow/agent:1.0.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: aiflow
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/characters
            name: character-config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: aiflow-character-config
          name: character-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-21T20:24:39Z"
    generation: 2
    labels:
      app: aiflow-agent
      pod-template-hash: b9d8b7b6d
    name: aiflow-agent-b9d8b7b6d
    namespace: social-agents
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: aiflow-agent
      uid: 3f9219d6-0f03-41c1-b050-a3cd06a55ffe
    resourceVersion: "5375686"
    uid: e1c33ded-453f-4a33-a730-dc4f9791bd8e
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: aiflow-agent
        pod-template-hash: b9d8b7b6d
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: aiflow-agent
          pod-template-hash: b9d8b7b6d
      spec:
        containers:
        - env:
          - name: CHARACTER_FILE
            value: /app/characters/AIFlow.json
          - name: DB_PATH
            valueFrom:
              secretKeyRef:
                key: db-connection-string
                name: aiflow-secrets
          - name: AIFLOW_API_KEY
            valueFrom:
              secretKeyRef:
                key: api-key
                name: aiflow-secrets
          - name: TW_API_KEY
            valueFrom:
              secretKeyRef:
                key: twitter-api-key
                name: aiflow-secrets
                optional: true
          - name: TW_API_KEY_SECRET
            valueFrom:
              secretKeyRef:
                key: twitter-api-key-secret
                name: aiflow-secrets
                optional: true
          - name: TW_ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                key: twitter-access-token
                name: aiflow-secrets
                optional: true
          - name: TW_ACCESS_TOKEN_SECRET
            valueFrom:
              secretKeyRef:
                key: twitter-access-token-secret
                name: aiflow-secrets
                optional: true
          - name: TW_BEARER_TOKEN
            valueFrom:
              secretKeyRef:
                key: twitter-bearer-token
                name: aiflow-secrets
                optional: true
          - name: TG_BOT_TOKEN
            valueFrom:
              secretKeyRef:
                key: telegram-bot-token
                name: aiflow-secrets
                optional: true
          - name: ANTHROPIC_API_KEY
            valueFrom:
              secretKeyRef:
                key: anthropic-api-key
                name: aiflow-secrets
                optional: true
          - name: OPENAI_API_KEY
            valueFrom:
              secretKeyRef:
                key: openai-api-key
                name: aiflow-secrets
                optional: true
          image: aiflow/agent:1.0.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: aiflow
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/characters
            name: character-config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: aiflow-character-config
          name: character-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: redis
      meta.helm.sh/release-namespace: llm-platform
    creationTimestamp: "2025-10-16T03:33:33Z"
    generation: 1
    labels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: redis
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 8.2.2
      helm.sh/chart: redis-23.1.3
    name: redis-master
    namespace: llm-platform
    resourceVersion: "7643378"
    uid: 2703949c-5ceb-4c09-8470-ad67dcf601d8
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: master
        app.kubernetes.io/instance: redis
        app.kubernetes.io/name: redis
    serviceName: redis-headless
    template:
      metadata:
        annotations:
          checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580
          checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
          checksum/scripts: a9a88a19d473fec18995a3083aa78788c24d98ad662f8eb697f9426b962bbd61
          checksum/secret: f3b30bdb3e109ce14d4714de475ecb9f805ec15fa55fdf70e527ef53c0ddd1e6
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: master
          app.kubernetes.io/instance: redis
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: redis
          app.kubernetes.io/version: 8.2.2
          helm.sh/chart: redis-23.1.3
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: master
                    app.kubernetes.io/instance: redis
                    app.kubernetes.io/name: redis
                topologyKey: kubernetes.io/hostname
              weight: 1
        automountServiceAccountToken: false
        containers:
        - args:
          - -ec
          - /opt/bitnami/scripts/start-scripts/start-master.sh
          command:
          - /bin/bash
          env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: REDIS_REPLICATION_MODE
            value: master
          - name: ALLOW_EMPTY_PASSWORD
            value: "no"
          - name: REDIS_PASSWORD_FILE
            value: /opt/bitnami/redis/secrets/redis-password
          - name: REDIS_TLS_ENABLED
            value: "no"
          - name: REDIS_PORT
            value: "6379"
          image: registry-1.docker.io/bitnami/redis:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/bash
              - -ec
              - /health/ping_liveness_local.sh 5
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 6
          name: redis
          ports:
          - containerPort: 6379
            name: redis
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/bash
              - -ec
              - /health/ping_readiness_local.sh 1
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/bitnami/scripts/start-scripts
            name: start-scripts
          - mountPath: /health
            name: health
          - mountPath: /opt/bitnami/redis/secrets/
            name: redis-password
          - mountPath: /data
            name: redis-data
          - mountPath: /opt/bitnami/redis/mounted-etc
            name: config
          - mountPath: /opt/bitnami/redis/etc/
            name: empty-dir
            subPath: app-conf-dir
          - mountPath: /tmp
            name: empty-dir
            subPath: tmp-dir
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          fsGroupChangePolicy: Always
        serviceAccount: redis-master
        serviceAccountName: redis-master
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 493
            name: redis-scripts
          name: start-scripts
        - configMap:
            defaultMode: 493
            name: redis-health
          name: health
        - name: redis-password
          secret:
            defaultMode: 420
            items:
            - key: redis-password
              path: redis-password
            secretName: redis
        - configMap:
            defaultMode: 420
            name: redis-configuration
          name: config
        - emptyDir: {}
          name: empty-dir
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: master
          app.kubernetes.io/instance: redis
          app.kubernetes.io/name: redis
        name: redis-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: redis-master-cc984cc8c
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: redis-master-cc984cc8c
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: redis
      meta.helm.sh/release-namespace: llm-platform
    creationTimestamp: "2025-10-16T03:33:33Z"
    generation: 1
    labels:
      app.kubernetes.io/component: replica
      app.kubernetes.io/instance: redis
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 8.2.2
      helm.sh/chart: redis-23.1.3
    name: redis-replicas
    namespace: llm-platform
    resourceVersion: "7643483"
    uid: d0e4f22e-e47d-415c-9fca-5a7f228cc8b1
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: replica
        app.kubernetes.io/instance: redis
        app.kubernetes.io/name: redis
    serviceName: redis-headless
    template:
      metadata:
        annotations:
          checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580
          checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
          checksum/scripts: a9a88a19d473fec18995a3083aa78788c24d98ad662f8eb697f9426b962bbd61
          checksum/secret: f3b30bdb3e109ce14d4714de475ecb9f805ec15fa55fdf70e527ef53c0ddd1e6
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: replica
          app.kubernetes.io/instance: redis
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: redis
          app.kubernetes.io/version: 8.2.2
          helm.sh/chart: redis-23.1.3
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: replica
                    app.kubernetes.io/instance: redis
                    app.kubernetes.io/name: redis
                topologyKey: kubernetes.io/hostname
              weight: 1
        automountServiceAccountToken: false
        containers:
        - args:
          - -ec
          - /opt/bitnami/scripts/start-scripts/start-replica.sh
          command:
          - /bin/bash
          env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: REDIS_REPLICATION_MODE
            value: replica
          - name: REDIS_MASTER_HOST
            value: redis-master-0.redis-headless.llm-platform.svc.cluster.local
          - name: REDIS_MASTER_PORT_NUMBER
            value: "6379"
          - name: ALLOW_EMPTY_PASSWORD
            value: "no"
          - name: REDIS_PASSWORD_FILE
            value: /opt/bitnami/redis/secrets/redis-password
          - name: REDIS_MASTER_PASSWORD_FILE
            value: /opt/bitnami/redis/secrets/redis-password
          - name: REDIS_TLS_ENABLED
            value: "no"
          - name: REDIS_PORT
            value: "6379"
          image: registry-1.docker.io/bitnami/redis:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/bash
              - -ec
              - /health/ping_liveness_local_and_master.sh 5
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 6
          name: redis
          ports:
          - containerPort: 6379
            name: redis
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/bash
              - -ec
              - /health/ping_readiness_local_and_master.sh 1
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          startupProbe:
            failureThreshold: 22
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: redis
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/bitnami/scripts/start-scripts
            name: start-scripts
          - mountPath: /health
            name: health
          - mountPath: /opt/bitnami/redis/secrets/
            name: redis-password
          - mountPath: /data
            name: redis-data
          - mountPath: /opt/bitnami/redis/mounted-etc
            name: config
          - mountPath: /opt/bitnami/redis/etc
            name: empty-dir
            subPath: app-conf-dir
          - mountPath: /tmp
            name: empty-dir
            subPath: tmp-dir
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          fsGroupChangePolicy: Always
        serviceAccount: redis-replica
        serviceAccountName: redis-replica
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 493
            name: redis-scripts
          name: start-scripts
        - configMap:
            defaultMode: 493
            name: redis-health
          name: health
        - name: redis-password
          secret:
            defaultMode: 420
            items:
            - key: redis-password
              path: redis-password
            secretName: redis
        - configMap:
            defaultMode: 420
            name: redis-configuration
          name: config
        - emptyDir: {}
          name: empty-dir
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: replica
          app.kubernetes.io/instance: redis
          app.kubernetes.io/name: redis
        name: redis-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: redis-replicas-747ddc9f46
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: redis-replicas-747ddc9f46
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"StatefulSet","metadata":{"annotations":{},"name":"kafka","namespace":"ossa-agents"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"kafka"}},"serviceName":"kafka","template":{"metadata":{"labels":{"app":"kafka"}},"spec":{"containers":[{"env":[{"name":"KAFKA_NODE_ID","value":"1"},{"name":"KAFKA_PROCESS_ROLES","value":"broker,controller"},{"name":"KAFKA_LISTENERS","value":"PLAINTEXT://:9092,CONTROLLER://:9093"},{"name":"KAFKA_ADVERTISED_LISTENERS","value":"PLAINTEXT://kafka:9092"},{"name":"KAFKA_CONTROLLER_LISTENER_NAMES","value":"CONTROLLER"},{"name":"KAFKA_LISTENER_SECURITY_PROTOCOL_MAP","value":"CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"},{"name":"KAFKA_CONTROLLER_QUORUM_VOTERS","value":"1@kafka-0.kafka.ossa-agents.svc.cluster.local:9093"},{"name":"KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR","value":"1"},{"name":"KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR","value":"1"},{"name":"KAFKA_TRANSACTION_STATE_LOG_MIN_ISR","value":"1"},{"name":"KAFKA_LOG_DIRS","value":"/tmp/kraft-combined-logs"},{"name":"CLUSTER_ID","value":"MkU3OEVBNTcwNTJENDM2Qk"}],"image":"apache/kafka:latest","name":"kafka","ports":[{"containerPort":9092,"name":"plaintext"},{"containerPort":9093,"name":"controller"}],"resources":{"limits":{"cpu":"500m","memory":"512Mi"},"requests":{"cpu":"100m","memory":"256Mi"}}}]}}}}
    creationTimestamp: "2025-10-10T13:12:40Z"
    generation: 1
    name: kafka
    namespace: ossa-agents
    resourceVersion: "6780522"
    uid: 9217b3d7-8ee9-47a9-ae61-4194abbcae7d
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: kafka
    serviceName: kafka
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: kafka
      spec:
        containers:
        - env:
          - name: KAFKA_NODE_ID
            value: "1"
          - name: KAFKA_PROCESS_ROLES
            value: broker,controller
          - name: KAFKA_LISTENERS
            value: PLAINTEXT://:9092,CONTROLLER://:9093
          - name: KAFKA_ADVERTISED_LISTENERS
            value: PLAINTEXT://kafka:9092
          - name: KAFKA_CONTROLLER_LISTENER_NAMES
            value: CONTROLLER
          - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
            value: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
          - name: KAFKA_CONTROLLER_QUORUM_VOTERS
            value: 1@kafka-0.kafka.ossa-agents.svc.cluster.local:9093
          - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
            value: "1"
          - name: KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
            value: "1"
          - name: KAFKA_TRANSACTION_STATE_LOG_MIN_ISR
            value: "1"
          - name: KAFKA_LOG_DIRS
            value: /tmp/kraft-combined-logs
          - name: CLUSTER_ID
            value: MkU3OEVBNTcwNTJENDM2Qk
          image: apache/kafka:latest
          imagePullPolicy: Always
          name: kafka
          ports:
          - containerPort: 9092
            name: plaintext
            protocol: TCP
          - containerPort: 9093
            name: controller
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
  status:
    availableReplicas: 0
    collisionCount: 0
    currentReplicas: 1
    currentRevision: kafka-c7758466b
    observedGeneration: 1
    replicas: 1
    updateRevision: kafka-c7758466b
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: ossa-agents
    creationTimestamp: "2025-10-30T15:37:08Z"
    generation: 1
    labels:
      app: loki
      app.kubernetes.io/managed-by: Helm
      chart: loki-2.16.0
      heritage: Helm
      release: loki
    name: loki
    namespace: ossa-agents
    resourceVersion: "7643750"
    uid: 9cc7ae8e-58e3-4b69-ac75-6a537ca5c13e
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: loki
        release: loki
    serviceName: loki-headless
    template:
      metadata:
        annotations:
          checksum/config: ef488086761255c99c5bc79ce6c045c5952aaff1c871419df514bc9c4fdf2cca
          prometheus.io/port: http-metrics
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: loki
          name: loki
          release: loki
      spec:
        affinity: {}
        containers:
        - args:
          - -config.file=/etc/loki/loki.yaml
          image: grafana/loki:2.6.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: loki
          ports:
          - containerPort: 3100
            name: http-metrics
            protocol: TCP
          - containerPort: 9095
            name: grpc
            protocol: TCP
          - containerPort: 7946
            name: memberlist-port
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /etc/loki
            name: config
          - mountPath: /data
            name: storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 10001
          runAsGroup: 10001
          runAsNonRoot: true
          runAsUser: 10001
        serviceAccount: loki
        serviceAccountName: loki
        terminationGracePeriodSeconds: 4800
        volumes:
        - emptyDir: {}
          name: tmp
        - name: config
          secret:
            defaultMode: 420
            secretName: loki
        - emptyDir: {}
          name: storage
    updateStrategy:
      type: RollingUpdate
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: loki-774677948b
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: loki-774677948b
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: tempo
      meta.helm.sh/release-namespace: ossa-agents
    creationTimestamp: "2025-10-30T15:36:09Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: tempo
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: tempo
      app.kubernetes.io/version: 2.9.0
      helm.sh/chart: tempo-1.24.0
    name: tempo
    namespace: ossa-agents
    resourceVersion: "7643625"
    uid: d1f893ff-06a4-47dd-8de6-c95e1fbabd3d
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: tempo
        app.kubernetes.io/name: tempo
    serviceName: tempo-headless
    template:
      metadata:
        annotations:
          checksum/config: 641ccac989f9a653a71e14216fd5ac5cef35d2867853bade1cd7aed136ba1eb1
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: tempo
          app.kubernetes.io/name: tempo
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - -config.file=/conf/tempo.yaml
          - -mem-ballast-size-mbs=1024
          image: grafana/tempo:2.9.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 3200
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: tempo
          ports:
          - containerPort: 3200
            name: prom-metrics
            protocol: TCP
          - containerPort: 6831
            name: jaeger-thrift-c
            protocol: UDP
          - containerPort: 6832
            name: jaeger-thrift-b
            protocol: UDP
          - containerPort: 14268
            name: jaeger-thrift-h
            protocol: TCP
          - containerPort: 14250
            name: jaeger-grpc
            protocol: TCP
          - containerPort: 9411
            name: zipkin
            protocol: TCP
          - containerPort: 55680
            name: otlp-legacy
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 55681
            name: otlp-httplegacy
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          - containerPort: 55678
            name: opencensus
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 3200
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /conf
            name: tempo-conf
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 10001
          runAsGroup: 10001
          runAsNonRoot: true
          runAsUser: 10001
        serviceAccount: tempo
        serviceAccountName: tempo
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: tempo
          name: tempo-conf
    updateStrategy:
      type: RollingUpdate
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: tempo-7bc544f9cf
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: tempo-7bc544f9cf
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"StatefulSet","metadata":{"annotations":{},"labels":{"app.kubernetes.io/component":"server","app.kubernetes.io/managed-by":"kustomize","app.kubernetes.io/name":"vault","app.kubernetes.io/part-of":"security-platform"},"name":"vault","namespace":"vault"},"spec":{"replicas":3,"selector":{"matchLabels":{"app.kubernetes.io/component":"server","app.kubernetes.io/name":"vault"}},"serviceName":"vault-internal","template":{"metadata":{"labels":{"app.kubernetes.io/component":"server","app.kubernetes.io/name":"vault"}},"spec":{"affinity":{"podAntiAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":[{"labelSelector":{"matchLabels":{"app.kubernetes.io/name":"vault"}},"topologyKey":"kubernetes.io/hostname"}]}},"containers":[{"args":["server"],"env":[{"name":"VAULT_ADDR","value":"http://vault.local.bluefly.io"},{"name":"VAULT_API_ADDR","value":"http://vault.local.bluefly.io"},{"name":"VAULT_CLUSTER_ADDR","value":"http://$(HOSTNAME).vault-internal:8201"},{"name":"VAULT_RAFT_NODE_ID","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"HOSTNAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"VAULT_K8S_POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"VAULT_K8S_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"SKIP_SETCAP","value":"true"}],"image":"hashicorp/vault:1.20.4","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/v1/sys/health?standbyok=true","port":8200,"scheme":"HTTP"},"initialDelaySeconds":60,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":3},"name":"vault","ports":[{"containerPort":8200,"name":"http","protocol":"TCP"},{"containerPort":8201,"name":"https-internal","protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/v1/sys/health?standbyok=true\u0026sealedcode=204\u0026uninitcode=204","port":8200,"scheme":"HTTP"},"initialDelaySeconds":10,"periodSeconds":5,"successThreshold":1,"timeoutSeconds":3},"resources":{"limits":{"cpu":"500m","memory":"512Mi"},"requests":{"cpu":"250m","memory":"256Mi"}},"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"drop":["ALL"]},"readOnlyRootFilesystem":true},"volumeMounts":[{"mountPath":"/vault/data","name":"data"},{"mountPath":"/vault/config","name":"config"},{"mountPath":"/vault/tls","name":"tls","readOnly":true}]}],"securityContext":{"fsGroup":1000,"runAsNonRoot":true,"runAsUser":100},"terminationGracePeriodSeconds":10,"volumes":[{"configMap":{"name":"vault-config"},"name":"config"},{"name":"tls","secret":{"secretName":"vault-tls"}}]}},"volumeClaimTemplates":[{"metadata":{"name":"data"},"spec":{"accessModes":["ReadWriteOnce"],"resources":{"requests":{"storage":"10Gi"}}}}]}}
    creationTimestamp: "2025-10-08T13:16:41Z"
    generation: 6
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/managed-by: kustomize
      app.kubernetes.io/name: vault
      app.kubernetes.io/part-of: security-platform
    name: vault
    namespace: vault
    resourceVersion: "5520121"
    uid: 237fe2ce-3bf8-4588-833f-1a9ebda77cc9
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: server
        app.kubernetes.io/name: vault
    serviceName: vault-internal
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-10-08T12:36:39-04:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: server
          app.kubernetes.io/name: vault
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/name: vault
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - server
          env:
          - name: VAULT_ADDR
            value: http://vault.local.bluefly.io
          - name: VAULT_API_ADDR
            value: http://vault.local.bluefly.io
          - name: VAULT_CLUSTER_ADDR
            value: http://$(HOSTNAME).vault-internal:8201
          - name: VAULT_RAFT_NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: HOSTNAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: VAULT_K8S_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: VAULT_K8S_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: SKIP_SETCAP
            value: "true"
          image: hashicorp/vault:1.20.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /v1/sys/health?standbyok=true
              port: 8200
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: vault
          ports:
          - containerPort: 8200
            name: http
            protocol: TCP
          - containerPort: 8201
            name: https-internal
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /v1/sys/health?standbyok=true&sealedcode=204&uninitcode=204
              port: 8200
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /vault/data
            name: data
          - mountPath: /vault/config
            name: config
          - mountPath: /vault/tls
            name: tls
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 100
        terminationGracePeriodSeconds: 10
        volumes:
        - configMap:
            defaultMode: 420
            name: vault-config
          name: config
        - name: tls
          secret:
            defaultMode: 420
            secretName: vault-tls
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 0
    collisionCount: 0
    currentRevision: vault-7994d84d79
    observedGeneration: 6
    replicas: 0
    updateRevision: vault-7994d84d79
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"autoscaling/v2","kind":"HorizontalPodAutoscaler","metadata":{"annotations":{},"name":"analytics-agent-hpa","namespace":"default"},"spec":{"maxReplicas":10,"metrics":[{"resource":{"name":"cpu","target":{"averageUtilization":70,"type":"Utilization"}},"type":"Resource"},{"resource":{"name":"memory","target":{"averageUtilization":80,"type":"Utilization"}},"type":"Resource"}],"minReplicas":1,"scaleTargetRef":{"apiVersion":"apps/v1","kind":"Deployment","name":"analytics-agent"}}}
    creationTimestamp: "2025-11-13T17:36:32Z"
    name: analytics-agent-hpa
    namespace: default
    resourceVersion: "7632251"
    uid: 221b33ee-e547-4345-8325-5c90b2f7030f
  spec:
    maxReplicas: 10
    metrics:
    - resource:
        name: cpu
        target:
          averageUtilization: 70
          type: Utilization
      type: Resource
    - resource:
        name: memory
        target:
          averageUtilization: 80
          type: Utilization
      type: Resource
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: analytics-agent
  status:
    conditions:
    - lastTransitionTime: "2025-11-17T23:58:26Z"
      message: 'the HPA controller was unable to get the target''s current scale:
        deployments/scale.apps "analytics-agent" not found'
      reason: FailedGetScale
      status: "False"
      type: AbleToScale
    - lastTransitionTime: "2025-11-13T17:36:47Z"
      message: 'pods by selector app=kagent,kagent=analytics-agent are controlled
        by multiple HPAs: [default/keda-hpa-analytics-agent-scaler default/analytics-agent-hpa]'
      reason: AmbiguousSelector
      status: "False"
      type: ScalingActive
    currentMetrics: null
    currentReplicas: 1
    desiredReplicas: 0
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"keda.sh/v1alpha1","kind":"ScaledObject","metadata":{"annotations":{},"name":"analytics-agent-scaler","namespace":"default"},"spec":{"cooldownPeriod":300,"idleReplicaCount":0,"maxReplicaCount":1,"minReplicaCount":0,"scaleTargetRef":{"name":"analytics-agent"},"triggers":[{"metadata":{"metricName":"http_requests_total","query":"sum(rate(http_requests_total{deployment=\"analytics-agent\"}[1m]))","serverAddress":"http://prometheus.ossa-agents.svc.cluster.local:9090","threshold":"1"},"type":"prometheus"}]}}
    creationTimestamp: "2025-11-11T17:18:48Z"
    labels:
      app.kubernetes.io/managed-by: keda-operator
      app.kubernetes.io/name: keda-hpa-analytics-agent-scaler
      app.kubernetes.io/part-of: analytics-agent-scaler
      app.kubernetes.io/version: 2.18.0
      scaledobject.keda.sh/name: analytics-agent-scaler
    name: keda-hpa-analytics-agent-scaler
    namespace: default
    ownerReferences:
    - apiVersion: keda.sh/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ScaledObject
      name: analytics-agent-scaler
      uid: d08b1a2f-2480-4705-937b-2277c09153e6
    resourceVersion: "7632262"
    uid: 5c971256-067c-407e-82a1-31760f355eb1
  spec:
    maxReplicas: 1
    metrics:
    - external:
        metric:
          name: s0-prometheus
          selector:
            matchLabels:
              scaledobject.keda.sh/name: analytics-agent-scaler
        target:
          averageValue: "1"
          type: AverageValue
      type: External
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: analytics-agent
  status:
    conditions:
    - lastTransitionTime: "2025-11-17T23:58:27Z"
      message: 'the HPA controller was unable to get the target''s current scale:
        deployments/scale.apps "analytics-agent" not found'
      reason: FailedGetScale
      status: "False"
      type: AbleToScale
    - lastTransitionTime: "2025-11-11T17:19:03Z"
      message: 'pods by selector app=kagent,kagent=analytics-agent are controlled
        by multiple HPAs: [default/keda-hpa-analytics-agent-scaler default/analytics-agent-hpa]'
      reason: AmbiguousSelector
      status: "False"
      type: ScalingActive
    currentMetrics: null
    currentReplicas: 1
    desiredReplicas: 0
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"keda.sh/v1alpha1","kind":"ScaledObject","metadata":{"annotations":{},"name":"auth-security-specialist-scaler","namespace":"default"},"spec":{"cooldownPeriod":300,"idleReplicaCount":0,"maxReplicaCount":1,"minReplicaCount":0,"scaleTargetRef":{"name":"auth-security-specialist"},"triggers":[{"metadata":{"metricName":"http_requests_total","query":"sum(rate(http_requests_total{deployment=\"auth-security-specialist\"}[1m]))","serverAddress":"http://prometheus.ossa-agents.svc.cluster.local:9090","threshold":"1"},"type":"prometheus"}]}}
    creationTimestamp: "2025-11-11T17:18:49Z"
    labels:
      app.kubernetes.io/managed-by: keda-operator
      app.kubernetes.io/name: keda-hpa-auth-security-specialist-scaler
      app.kubernetes.io/part-of: auth-security-specialist-scaler
      app.kubernetes.io/version: 2.18.0
      scaledobject.keda.sh/name: auth-security-specialist-scaler
    name: keda-hpa-auth-security-specialist-scaler
    namespace: default
    ownerReferences:
    - apiVersion: keda.sh/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ScaledObject
      name: auth-security-specialist-scaler
      uid: dec98a7b-a2be-49f8-b4f0-935a8dfdec44
    resourceVersion: "7632255"
    uid: f91f0358-b971-4b60-a81d-484eae430889
  spec:
    maxReplicas: 1
    metrics:
    - external:
        metric:
          name: s0-prometheus
          selector:
            matchLabels:
              scaledobject.keda.sh/name: auth-security-specialist-scaler
        target:
          averageValue: "1"
          type: AverageValue
      type: External
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: auth-security-specialist
  status:
    conditions:
    - lastTransitionTime: "2025-11-17T23:58:26Z"
      message: 'the HPA controller was unable to get the target''s current scale:
        deployments/scale.apps "auth-security-specialist" not found'
      reason: FailedGetScale
      status: "False"
      type: AbleToScale
    - lastTransitionTime: "2025-11-11T17:19:04Z"
      message: 'the HPA was unable to compute the replica count: unable to get external
        metric default/s0-prometheus/&LabelSelector{MatchLabels:map[string]string{scaledobject.keda.sh/name:
        auth-security-specialist-scaler,},MatchExpressions:[]LabelSelectorRequirement{},}:
        unable to fetch metrics from external metrics API: the server is currently
        unable to handle the request (get s0-prometheus.external.metrics.k8s.io)'
      reason: FailedGetExternalMetric
      status: "False"
      type: ScalingActive
    currentMetrics:
    - type: ""
    currentReplicas: 1
    desiredReplicas: 0
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"keda.sh/v1alpha1","kind":"ScaledObject","metadata":{"annotations":{},"name":"developer-docs-specialist-scaler","namespace":"default"},"spec":{"cooldownPeriod":300,"idleReplicaCount":0,"maxReplicaCount":1,"minReplicaCount":0,"scaleTargetRef":{"name":"developer-docs-specialist"},"triggers":[{"metadata":{"metricName":"http_requests_total","query":"sum(rate(http_requests_total{deployment=\"developer-docs-specialist\"}[1m]))","serverAddress":"http://prometheus.ossa-agents.svc.cluster.local:9090","threshold":"1"},"type":"prometheus"}]}}
    creationTimestamp: "2025-11-11T17:18:49Z"
    labels:
      app.kubernetes.io/managed-by: keda-operator
      app.kubernetes.io/name: keda-hpa-developer-docs-specialist-scaler
      app.kubernetes.io/part-of: developer-docs-specialist-scaler
      app.kubernetes.io/version: 2.18.0
      scaledobject.keda.sh/name: developer-docs-specialist-scaler
    name: keda-hpa-developer-docs-specialist-scaler
    namespace: default
    ownerReferences:
    - apiVersion: keda.sh/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ScaledObject
      name: developer-docs-specialist-scaler
      uid: fee990f1-fd9f-45fe-b3f3-0ca7ca1b4078
    resourceVersion: "7632254"
    uid: 579a0965-295e-4518-85f3-9c0f3cb92c07
  spec:
    maxReplicas: 1
    metrics:
    - external:
        metric:
          name: s0-prometheus
          selector:
            matchLabels:
              scaledobject.keda.sh/name: developer-docs-specialist-scaler
        target:
          averageValue: "1"
          type: AverageValue
      type: External
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: developer-docs-specialist
  status:
    conditions:
    - lastTransitionTime: "2025-11-17T23:58:26Z"
      message: 'the HPA controller was unable to get the target''s current scale:
        deployments/scale.apps "developer-docs-specialist" not found'
      reason: FailedGetScale
      status: "False"
      type: AbleToScale
    - lastTransitionTime: "2025-11-11T17:19:04Z"
      message: 'the HPA was unable to compute the replica count: unable to get external
        metric default/s0-prometheus/&LabelSelector{MatchLabels:map[string]string{scaledobject.keda.sh/name:
        developer-docs-specialist-scaler,},MatchExpressions:[]LabelSelectorRequirement{},}:
        unable to fetch metrics from external metrics API: the server is currently
        unable to handle the request (get s0-prometheus.external.metrics.k8s.io)'
      reason: FailedGetExternalMetric
      status: "False"
      type: ScalingActive
    currentMetrics:
    - type: ""
    currentReplicas: 1
    desiredReplicas: 0
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"keda.sh/v1alpha1","kind":"ScaledObject","metadata":{"annotations":{},"name":"drupal-migration-specialist-scaler","namespace":"default"},"spec":{"cooldownPeriod":300,"idleReplicaCount":0,"maxReplicaCount":1,"minReplicaCount":0,"scaleTargetRef":{"name":"drupal-migration-specialist"},"triggers":[{"metadata":{"metricName":"http_requests_total","query":"sum(rate(http_requests_total{deployment=\"drupal-migration-specialist\"}[1m]))","serverAddress":"http://prometheus.ossa-agents.svc.cluster.local:9090","threshold":"1"},"type":"prometheus"}]}}
    creationTimestamp: "2025-11-11T17:18:48Z"
    labels:
      app.kubernetes.io/managed-by: keda-operator
      app.kubernetes.io/name: keda-hpa-drupal-migration-specialist-scaler
      app.kubernetes.io/part-of: drupal-migration-specialist-scaler
      app.kubernetes.io/version: 2.18.0
      scaledobject.keda.sh/name: drupal-migration-specialist-scaler
    name: keda-hpa-drupal-migration-specialist-scaler
    namespace: default
    ownerReferences:
    - apiVersion: keda.sh/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ScaledObject
      name: drupal-migration-specialist-scaler
      uid: 4006742d-45b4-4516-a62c-c7c9bb8d7f3f
    resourceVersion: "7632685"
    uid: 3e3fa66d-9af6-4409-a51a-6c9063f7f486
  spec:
    maxReplicas: 1
    metrics:
    - external:
        metric:
          name: s0-prometheus
          selector:
            matchLabels:
              scaledobject.keda.sh/name: drupal-migration-specialist-scaler
        target:
          averageValue: "1"
          type: AverageValue
      type: External
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: drupal-migration-specialist
  status:
    conditions:
    - lastTransitionTime: "2025-11-17T23:58:41Z"
      message: 'the HPA controller was unable to get the target''s current scale:
        deployments/scale.apps "drupal-migration-specialist" not found'
      reason: FailedGetScale
      status: "False"
      type: AbleToScale
    - lastTransitionTime: "2025-11-11T17:19:03Z"
      message: 'the HPA was unable to compute the replica count: unable to get external
        metric default/s0-prometheus/&LabelSelector{MatchLabels:map[string]string{scaledobject.keda.sh/name:
        drupal-migration-specialist-scaler,},MatchExpressions:[]LabelSelectorRequirement{},}:
        unable to fetch metrics from external metrics API: the server is currently
        unable to handle the request (get s0-prometheus.external.metrics.k8s.io)'
      reason: FailedGetExternalMetric
      status: "False"
      type: ScalingActive
    currentMetrics:
    - type: ""
    currentReplicas: 1
    desiredReplicas: 0
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"keda.sh/v1alpha1","kind":"ScaledObject","metadata":{"annotations":{},"name":"ecosystem-cleanup-orchestrator-scaler","namespace":"default"},"spec":{"cooldownPeriod":300,"idleReplicaCount":0,"maxReplicaCount":1,"minReplicaCount":0,"scaleTargetRef":{"name":"ecosystem-cleanup-orchestrator"},"triggers":[{"metadata":{"desiredReplicas":"1","end":"0 17 * * *","start":"0 9 * * *","timezone":"UTC"},"type":"cron"}]}}
    creationTimestamp: "2025-11-11T17:18:49Z"
    labels:
      app.kubernetes.io/managed-by: keda-operator
      app.kubernetes.io/name: keda-hpa-ecosystem-cleanup-orchestrator-scaler
      app.kubernetes.io/part-of: ecosystem-cleanup-orchestrator-scaler
      app.kubernetes.io/version: 2.18.0
      scaledobject.keda.sh/name: ecosystem-cleanup-orchestrator-scaler
    name: keda-hpa-ecosystem-cleanup-orchestrator-scaler
    namespace: default
    ownerReferences:
    - apiVersion: keda.sh/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ScaledObject
      name: ecosystem-cleanup-orchestrator-scaler
      uid: 6b3f5da6-5c2b-452e-84c0-c4b4bb0725f8
    resourceVersion: "7632673"
    uid: bd78cb48-6292-4d4b-af68-bdaedd031ced
  spec:
    maxReplicas: 1
    metrics:
    - external:
        metric:
          name: s0-cron-UTC-09xxx-017xxx
          selector:
            matchLabels:
              scaledobject.keda.sh/name: ecosystem-cleanup-orchestrator-scaler
        target:
          averageValue: "1"
          type: AverageValue
      type: External
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: ecosystem-cleanup-orchestrator
  status:
    conditions:
    - lastTransitionTime: "2025-11-17T23:58:41Z"
      message: 'the HPA controller was unable to get the target''s current scale:
        deployments/scale.apps "ecosystem-cleanup-orchestrator" not found'
      reason: FailedGetScale
      status: "False"
      type: AbleToScale
    - lastTransitionTime: "2025-11-11T17:19:04Z"
      message: scaling is disabled since the replica count of the target is zero
      reason: ScalingDisabled
      status: "False"
      type: ScalingActive
    currentMetrics: null
    desiredReplicas: 0
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"keda.sh/v1alpha1","kind":"ScaledObject","metadata":{"annotations":{},"name":"example-orchestrator-scaler","namespace":"default"},"spec":{"cooldownPeriod":300,"idleReplicaCount":0,"maxReplicaCount":1,"minReplicaCount":0,"scaleTargetRef":{"name":"example-orchestrator"},"triggers":[{"metadata":{"metricName":"http_requests_total","query":"sum(rate(http_requests_total{deployment=\"example-orchestrator\"}[1m]))","serverAddress":"http://prometheus.ossa-agents.svc.cluster.local:9090","threshold":"1"},"type":"prometheus"}]}}
    creationTimestamp: "2025-11-11T17:18:12Z"
    labels:
      app.kubernetes.io/managed-by: keda-operator
      app.kubernetes.io/name: keda-hpa-example-orchestrator-scaler
      app.kubernetes.io/part-of: example-orchestrator-scaler
      app.kubernetes.io/version: 2.18.0
      scaledobject.keda.sh/name: example-orchestrator-scaler
    name: keda-hpa-example-orchestrator-scaler
    namespace: default
    ownerReferences:
    - apiVersion: keda.sh/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ScaledObject
      name: example-orchestrator-scaler
      uid: 40bf35c8-350b-4e3e-bdbb-788e87ca9b3a
    resourceVersion: "7632683"
    uid: d2241bb4-5d87-416f-8e1b-7882b7b513fe
  spec:
    maxReplicas: 1
    metrics:
    - external:
        metric:
          name: s0-prometheus
          selector:
            matchLabels:
              scaledobject.keda.sh/name: example-orchestrator-scaler
        target:
          averageValue: "1"
          type: AverageValue
      type: External
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: example-orchestrator
  status:
    conditions:
    - lastTransitionTime: "2025-11-17T23:58:41Z"
      message: 'the HPA controller was unable to get the target''s current scale:
        deployments/scale.apps "example-orchestrator" not found'
      reason: FailedGetScale
      status: "False"
      type: AbleToScale
    - lastTransitionTime: "2025-11-11T17:18:27Z"
      message: 'the HPA was unable to compute the replica count: unable to get external
        metric default/s0-prometheus/&LabelSelector{MatchLabels:map[string]string{scaledobject.keda.sh/name:
        example-orchestrator-scaler,},MatchExpressions:[]LabelSelectorRequirement{},}:
        unable to fetch metrics from external metrics API: the server is currently
        unable to handle the request (get s0-prometheus.external.metrics.k8s.io)'
      reason: FailedGetExternalMetric
      status: "False"
      type: ScalingActive
    currentMetrics:
    - type: ""
    currentReplicas: 1
    desiredReplicas: 0
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"keda.sh/v1alpha1","kind":"ScaledObject","metadata":{"annotations":{},"name":"kubernetes-orchestrator-scaler","namespace":"default"},"spec":{"cooldownPeriod":300,"idleReplicaCount":0,"maxReplicaCount":1,"minReplicaCount":0,"scaleTargetRef":{"name":"kubernetes-orchestrator"},"triggers":[{"metadata":{"metricName":"http_requests_total","query":"sum(rate(http_requests_total{deployment=\"kubernetes-orchestrator\"}[1m]))","serverAddress":"http://prometheus.ossa-agents.svc.cluster.local:9090","threshold":"1"},"type":"prometheus"}]}}
    creationTimestamp: "2025-11-11T17:18:12Z"
    labels:
      app.kubernetes.io/managed-by: keda-operator
      app.kubernetes.io/name: keda-hpa-kubernetes-orchestrator-scaler
      app.kubernetes.io/part-of: kubernetes-orchestrator-scaler
      app.kubernetes.io/version: 2.18.0
      scaledobject.keda.sh/name: kubernetes-orchestrator-scaler
    name: keda-hpa-kubernetes-orchestrator-scaler
    namespace: default
    ownerReferences:
    - apiVersion: keda.sh/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ScaledObject
      name: kubernetes-orchestrator-scaler
      uid: 87269b0c-3603-4f74-a439-b1bfaf1a1f6f
    resourceVersion: "7632669"
    uid: e4101838-6540-4ee2-9586-cda14126760b
  spec:
    maxReplicas: 1
    metrics:
    - external:
        metric:
          name: s0-prometheus
          selector:
            matchLabels:
              scaledobject.keda.sh/name: kubernetes-orchestrator-scaler
        target:
          averageValue: "1"
          type: AverageValue
      type: External
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: kubernetes-orchestrator
  status:
    conditions:
    - lastTransitionTime: "2025-11-17T23:58:41Z"
      message: 'the HPA controller was unable to get the target''s current scale:
        deployments/scale.apps "kubernetes-orchestrator" not found'
      reason: FailedGetScale
      status: "False"
      type: AbleToScale
    - lastTransitionTime: "2025-11-11T17:18:27Z"
      message: scaling is disabled since the replica count of the target is zero
      reason: ScalingDisabled
      status: "False"
      type: ScalingActive
    currentMetrics: null
    desiredReplicas: 0
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"keda.sh/v1alpha1","kind":"ScaledObject","metadata":{"annotations":{},"name":"ossa-cli-orchestrator-scaler","namespace":"default"},"spec":{"cooldownPeriod":300,"idleReplicaCount":0,"maxReplicaCount":1,"minReplicaCount":0,"scaleTargetRef":{"name":"ossa-cli-orchestrator"},"triggers":[{"metadata":{"metricName":"http_requests_total","query":"sum(rate(http_requests_total{deployment=\"ossa-cli-orchestrator\"}[1m]))","serverAddress":"http://prometheus.ossa-agents.svc.cluster.local:9090","threshold":"1"},"type":"prometheus"}]}}
    creationTimestamp: "2025-11-11T17:18:11Z"
    labels:
      app.kubernetes.io/managed-by: keda-operator
      app.kubernetes.io/name: keda-hpa-ossa-cli-orchestrator-scaler
      app.kubernetes.io/part-of: ossa-cli-orchestrator-scaler
      app.kubernetes.io/version: 2.18.0
      scaledobject.keda.sh/name: ossa-cli-orchestrator-scaler
    name: keda-hpa-ossa-cli-orchestrator-scaler
    namespace: default
    ownerReferences:
    - apiVersion: keda.sh/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ScaledObject
      name: ossa-cli-orchestrator-scaler
      uid: 4f73f977-94b8-46eb-9387-c061fa9ceab4
    resourceVersion: "7632672"
    uid: 26619819-ff87-498d-b1ab-b892a4bf64e3
  spec:
    maxReplicas: 1
    metrics:
    - external:
        metric:
          name: s0-prometheus
          selector:
            matchLabels:
              scaledobject.keda.sh/name: ossa-cli-orchestrator-scaler
        target:
          averageValue: "1"
          type: AverageValue
      type: External
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: ossa-cli-orchestrator
  status:
    conditions:
    - lastTransitionTime: "2025-11-17T23:58:41Z"
      message: 'the HPA controller was unable to get the target''s current scale:
        deployments/scale.apps "ossa-cli-orchestrator" not found'
      reason: FailedGetScale
      status: "False"
      type: AbleToScale
    - lastTransitionTime: "2025-11-11T17:18:26Z"
      message: scaling is disabled since the replica count of the target is zero
      reason: ScalingDisabled
      status: "False"
      type: ScalingActive
    currentMetrics: null
    desiredReplicas: 0
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"keda.sh/v1alpha1","kind":"ScaledObject","metadata":{"annotations":{},"name":"ossa-master-orchestrator-scaler","namespace":"default"},"spec":{"cooldownPeriod":300,"idleReplicaCount":0,"maxReplicaCount":1,"minReplicaCount":0,"scaleTargetRef":{"name":"ossa-master-orchestrator"},"triggers":[{"metadata":{"metricName":"http_requests_total","query":"sum(rate(http_requests_total{deployment=\"ossa-master-orchestrator\"}[1m]))","serverAddress":"http://prometheus.ossa-agents.svc.cluster.local:9090","threshold":"1"},"type":"prometheus"}]}}
    creationTimestamp: "2025-11-11T17:18:11Z"
    labels:
      app.kubernetes.io/managed-by: keda-operator
      app.kubernetes.io/name: keda-hpa-ossa-master-orchestrator-scaler
      app.kubernetes.io/part-of: ossa-master-orchestrator-scaler
      app.kubernetes.io/version: 2.18.0
      scaledobject.keda.sh/name: ossa-master-orchestrator-scaler
    name: keda-hpa-ossa-master-orchestrator-scaler
    namespace: default
    ownerReferences:
    - apiVersion: keda.sh/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ScaledObject
      name: ossa-master-orchestrator-scaler
      uid: 2e2d45ed-5421-4d6e-83d0-540a214fbd0c
    resourceVersion: "7632674"
    uid: 12d2ed7f-9812-4487-ba96-dfeb4423c028
  spec:
    maxReplicas: 1
    metrics:
    - external:
        metric:
          name: s0-prometheus
          selector:
            matchLabels:
              scaledobject.keda.sh/name: ossa-master-orchestrator-scaler
        target:
          averageValue: "1"
          type: AverageValue
      type: External
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: ossa-master-orchestrator
  status:
    conditions:
    - lastTransitionTime: "2025-11-17T23:58:41Z"
      message: 'the HPA controller was unable to get the target''s current scale:
        deployments/scale.apps "ossa-master-orchestrator" not found'
      reason: FailedGetScale
      status: "False"
      type: AbleToScale
    - lastTransitionTime: "2025-11-11T17:18:26Z"
      message: scaling is disabled since the replica count of the target is zero
      reason: ScalingDisabled
      status: "False"
      type: ScalingActive
    currentMetrics: null
    desiredReplicas: 0
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"keda.sh/v1alpha1","kind":"ScaledObject","metadata":{"annotations":{},"name":"roadmap-orchestrator-scaler","namespace":"default"},"spec":{"cooldownPeriod":300,"idleReplicaCount":0,"maxReplicaCount":1,"minReplicaCount":0,"scaleTargetRef":{"name":"roadmap-orchestrator"},"triggers":[{"metadata":{"metricName":"http_requests_total","query":"sum(rate(http_requests_total{deployment=\"roadmap-orchestrator\"}[1m]))","serverAddress":"http://prometheus.ossa-agents.svc.cluster.local:9090","threshold":"1"},"type":"prometheus"}]}}
    creationTimestamp: "2025-11-11T17:18:11Z"
    labels:
      app.kubernetes.io/managed-by: keda-operator
      app.kubernetes.io/name: keda-hpa-roadmap-orchestrator-scaler
      app.kubernetes.io/part-of: roadmap-orchestrator-scaler
      app.kubernetes.io/version: 2.18.0
      scaledobject.keda.sh/name: roadmap-orchestrator-scaler
    name: keda-hpa-roadmap-orchestrator-scaler
    namespace: default
    ownerReferences:
    - apiVersion: keda.sh/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ScaledObject
      name: roadmap-orchestrator-scaler
      uid: e69e0de4-05d5-473c-b75f-9b50754840c3
    resourceVersion: "7633047"
    uid: 53d6ab0b-76d0-44e2-99ca-b04d91380bbf
  spec:
    maxReplicas: 1
    metrics:
    - external:
        metric:
          name: s0-prometheus
          selector:
            matchLabels:
              scaledobject.keda.sh/name: roadmap-orchestrator-scaler
        target:
          averageValue: "1"
          type: AverageValue
      type: External
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: roadmap-orchestrator
  status:
    conditions:
    - lastTransitionTime: "2025-11-17T23:58:56Z"
      message: 'the HPA controller was unable to get the target''s current scale:
        deployments/scale.apps "roadmap-orchestrator" not found'
      reason: FailedGetScale
      status: "False"
      type: AbleToScale
    - lastTransitionTime: "2025-11-11T17:18:26Z"
      message: scaling is disabled since the replica count of the target is zero
      reason: ScalingDisabled
      status: "False"
      type: ScalingActive
    currentMetrics: null
    desiredReplicas: 0
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"autoscaling/v2","kind":"HorizontalPodAutoscaler","metadata":{"annotations":{},"name":"kubernetes-orchestrator-hpa","namespace":"default"},"spec":{"maxReplicas":10,"metrics":[{"resource":{"name":"cpu","target":{"averageUtilization":70,"type":"Utilization"}},"type":"Resource"},{"resource":{"name":"memory","target":{"averageUtilization":80,"type":"Utilization"}},"type":"Resource"}],"minReplicas":1,"scaleTargetRef":{"apiVersion":"apps/v1","kind":"Deployment","name":"kubernetes-orchestrator"}}}
    creationTimestamp: "2025-11-13T17:36:32Z"
    name: kubernetes-orchestrator-hpa
    namespace: default
    resourceVersion: "7632676"
    uid: 4d4892d6-c776-4da6-a903-0eef37a9a536
  spec:
    maxReplicas: 10
    metrics:
    - resource:
        name: cpu
        target:
          averageUtilization: 70
          type: Utilization
      type: Resource
    - resource:
        name: memory
        target:
          averageUtilization: 80
          type: Utilization
      type: Resource
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: kubernetes-orchestrator
  status:
    conditions:
    - lastTransitionTime: "2025-11-17T23:58:41Z"
      message: 'the HPA controller was unable to get the target''s current scale:
        deployments/scale.apps "kubernetes-orchestrator" not found'
      reason: FailedGetScale
      status: "False"
      type: AbleToScale
    - lastTransitionTime: "2025-11-13T17:36:47Z"
      message: scaling is disabled since the replica count of the target is zero
      reason: ScalingDisabled
      status: "False"
      type: ScalingActive
    currentMetrics: null
    desiredReplicas: 0
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"autoscaling/v2","kind":"HorizontalPodAutoscaler","metadata":{"annotations":{},"labels":{"app":"design-system-api","environment":"production","managed-by":"kustomize","version":"1.0.0"},"name":"design-system-api-hpa","namespace":"design-system-api"},"spec":{"behavior":{"scaleDown":{"policies":[{"periodSeconds":60,"type":"Percent","value":50}],"stabilizationWindowSeconds":300},"scaleUp":{"policies":[{"periodSeconds":30,"type":"Percent","value":100},{"periodSeconds":30,"type":"Pods","value":2}],"selectPolicy":"Max","stabilizationWindowSeconds":60}},"maxReplicas":10,"metrics":[{"resource":{"name":"cpu","target":{"averageUtilization":70,"type":"Utilization"}},"type":"Resource"},{"resource":{"name":"memory","target":{"averageUtilization":80,"type":"Utilization"}},"type":"Resource"}],"minReplicas":2,"scaleTargetRef":{"apiVersion":"apps/v1","kind":"Deployment","name":"design-system-api"}}}
    creationTimestamp: "2025-10-07T02:23:21Z"
    labels:
      app: design-system-api
      environment: production
      managed-by: kustomize
      version: 1.0.0
    name: design-system-api-hpa
    namespace: design-system-api
    resourceVersion: "6752699"
    uid: b2c4e293-6f4e-41c1-b0ab-311864a36d9c
  spec:
    behavior:
      scaleDown:
        policies:
        - periodSeconds: 60
          type: Percent
          value: 50
        selectPolicy: Max
        stabilizationWindowSeconds: 300
      scaleUp:
        policies:
        - periodSeconds: 30
          type: Percent
          value: 100
        - periodSeconds: 30
          type: Pods
          value: 2
        selectPolicy: Max
        stabilizationWindowSeconds: 60
    maxReplicas: 10
    metrics:
    - resource:
        name: cpu
        target:
          averageUtilization: 70
          type: Utilization
      type: Resource
    - resource:
        name: memory
        target:
          averageUtilization: 80
          type: Utilization
      type: Resource
    minReplicas: 2
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: design-system-api
  status:
    conditions:
    - lastTransitionTime: "2025-10-22T15:22:33Z"
      message: 'the HPA controller was unable to get the target''s current scale:
        deployments/scale.apps "design-system-api" not found'
      reason: FailedGetScale
      status: "False"
      type: AbleToScale
    - lastTransitionTime: "2025-10-07T02:23:36Z"
      message: 'the HPA was unable to compute the replica count: failed to get cpu
        utilization: unable to get metrics for resource cpu: no metrics returned from
        resource metrics API'
      reason: FailedGetResourceMetric
      status: "False"
      type: ScalingActive
    currentMetrics:
    - type: ""
    - type: ""
    currentReplicas: 2
    desiredReplicas: 2
    lastScaleTime: "2025-10-13T13:23:14Z"
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"autoscaling/v2","kind":"HorizontalPodAutoscaler","metadata":{"annotations":{},"name":"dragonfly-hpa","namespace":"dragonfly-system"},"spec":{"maxReplicas":3,"metrics":[{"resource":{"name":"memory","target":{"averageUtilization":70,"type":"Utilization"}},"type":"Resource"},{"resource":{"name":"cpu","target":{"averageUtilization":80,"type":"Utilization"}},"type":"Resource"}],"minReplicas":1,"scaleTargetRef":{"apiVersion":"apps/v1","kind":"Deployment","name":"dragonfly"}}}
    creationTimestamp: "2025-11-13T19:37:41Z"
    name: dragonfly-hpa
    namespace: dragonfly-system
    resourceVersion: "7089837"
    uid: e01f34d8-332d-46ad-b0f1-5e667e0c938b
  spec:
    maxReplicas: 3
    metrics:
    - resource:
        name: memory
        target:
          averageUtilization: 70
          type: Utilization
      type: Resource
    - resource:
        name: cpu
        target:
          averageUtilization: 80
          type: Utilization
      type: Resource
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: dragonfly
  status:
    conditions:
    - lastTransitionTime: "2025-11-13T19:37:56Z"
      message: the HPA controller was able to get the target's current scale
      reason: SucceededGetScale
      status: "True"
      type: AbleToScale
    - lastTransitionTime: "2025-11-13T19:37:56Z"
      message: scaling is disabled since the replica count of the target is zero
      reason: ScalingDisabled
      status: "False"
      type: ScalingActive
    currentMetrics: null
    desiredReplicas: 0
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      deployment-wave: "3"
      generated-by: agent-swarm
      issues-closed: "79"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"keda.sh/v1alpha1","kind":"ScaledObject","metadata":{"annotations":{"deployment-wave":"3","generated-by":"agent-swarm","issues-closed":"79"},"labels":{"managed-by":"agent-buildkit","ossa-version":"v0.2.3","platform":"kagent"},"name":"khook-scaler","namespace":"khook-system"},"spec":{"maxReplicaCount":5,"minReplicaCount":1,"scaleTargetRef":{"name":"khook"},"triggers":[{"metadata":{"podSelector":"app=khook","value":"3"},"type":"kubernetes-workload"}]}}
    creationTimestamp: "2025-11-13T01:12:13Z"
    labels:
      app.kubernetes.io/managed-by: keda-operator
      app.kubernetes.io/name: keda-hpa-khook-scaler
      app.kubernetes.io/part-of: khook-scaler
      app.kubernetes.io/version: 2.18.0
      managed-by: agent-buildkit
      ossa-version: v0.2.3
      platform: kagent
      scaledobject.keda.sh/name: khook-scaler
    name: keda-hpa-khook-scaler
    namespace: khook-system
    ownerReferences:
    - apiVersion: keda.sh/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ScaledObject
      name: khook-scaler
      uid: 9036bf81-3660-41e1-88ad-96ca229a69ba
    resourceVersion: "7360429"
    uid: 34ef7e71-c3d8-4f1e-b7b3-e119161ec015
  spec:
    maxReplicas: 5
    metrics:
    - external:
        metric:
          name: s0-workload-khook-system
          selector:
            matchLabels:
              scaledobject.keda.sh/name: khook-scaler
        target:
          averageValue: "3"
          type: AverageValue
      type: External
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: khook
  status:
    conditions:
    - lastTransitionTime: "2025-11-13T01:12:28Z"
      message: the HPA controller was able to get the target's current scale
      reason: SucceededGetScale
      status: "True"
      type: AbleToScale
    - lastTransitionTime: "2025-11-13T01:12:28Z"
      message: 'the HPA was unable to compute the replica count: unable to get external
        metric khook-system/s0-workload-khook-system/&LabelSelector{MatchLabels:map[string]string{scaledobject.keda.sh/name:
        khook-scaler,},MatchExpressions:[]LabelSelectorRequirement{},}: unable to
        fetch metrics from external metrics API: the server is currently unable to
        handle the request (get s0-workload-khook-system.external.metrics.k8s.io)'
      reason: FailedGetExternalMetric
      status: "False"
      type: ScalingActive
    currentMetrics:
    - type: ""
    currentReplicas: 1
    desiredReplicas: 0
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"autoscaling/v2","kind":"HorizontalPodAutoscaler","metadata":{"annotations":{},"name":"agent-brain-hpa","namespace":"llm-platform"},"spec":{"maxReplicas":10,"metrics":[{"resource":{"name":"cpu","target":{"averageUtilization":70,"type":"Utilization"}},"type":"Resource"},{"resource":{"name":"memory","target":{"averageUtilization":80,"type":"Utilization"}},"type":"Resource"}],"minReplicas":2,"scaleTargetRef":{"apiVersion":"apps/v1","kind":"Deployment","name":"agent-brain"}}}
    creationTimestamp: "2025-10-21T20:24:31Z"
    name: agent-brain-hpa
    namespace: llm-platform
    resourceVersion: "6752702"
    uid: b705b678-59d6-4f1f-bc1a-0f4d869aec95
  spec:
    maxReplicas: 10
    metrics:
    - resource:
        name: cpu
        target:
          averageUtilization: 70
          type: Utilization
      type: Resource
    - resource:
        name: memory
        target:
          averageUtilization: 80
          type: Utilization
      type: Resource
    minReplicas: 2
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: agent-brain
  status:
    conditions:
    - lastTransitionTime: "2025-11-11T16:06:58Z"
      message: the HPA controller was able to get the target's current scale
      reason: SucceededGetScale
      status: "True"
      type: AbleToScale
    - lastTransitionTime: "2025-10-21T20:24:46Z"
      message: scaling is disabled since the replica count of the target is zero
      reason: ScalingDisabled
      status: "False"
      type: ScalingActive
    currentMetrics: null
    desiredReplicas: 0
    lastScaleTime: "2025-10-22T03:50:04Z"
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"autoscaling/v2","kind":"HorizontalPodAutoscaler","metadata":{"annotations":{},"name":"compliance-engine-hpa","namespace":"llm-platform"},"spec":{"maxReplicas":6,"metrics":[{"resource":{"name":"cpu","target":{"averageUtilization":70,"type":"Utilization"}},"type":"Resource"}],"minReplicas":2,"scaleTargetRef":{"apiVersion":"apps/v1","kind":"Deployment","name":"compliance-engine"}}}
    creationTimestamp: "2025-10-21T20:24:25Z"
    name: compliance-engine-hpa
    namespace: llm-platform
    resourceVersion: "6752697"
    uid: 2ec3a3b8-5786-434d-bf5e-5a75acbbed05
  spec:
    maxReplicas: 6
    metrics:
    - resource:
        name: cpu
        target:
          averageUtilization: 70
          type: Utilization
      type: Resource
    minReplicas: 2
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: compliance-engine
  status:
    conditions:
    - lastTransitionTime: "2025-11-11T16:06:58Z"
      message: the HPA controller was able to get the target's current scale
      reason: SucceededGetScale
      status: "True"
      type: AbleToScale
    - lastTransitionTime: "2025-10-21T20:24:40Z"
      message: scaling is disabled since the replica count of the target is zero
      reason: ScalingDisabled
      status: "False"
      type: ScalingActive
    currentMetrics: null
    desiredReplicas: 0
    lastScaleTime: "2025-10-21T21:49:38Z"
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"autoscaling/v2","kind":"HorizontalPodAutoscaler","metadata":{"annotations":{},"name":"workflow-engine-hpa","namespace":"llm-platform"},"spec":{"maxReplicas":8,"metrics":[{"resource":{"name":"cpu","target":{"averageUtilization":70,"type":"Utilization"}},"type":"Resource"}],"minReplicas":2,"scaleTargetRef":{"apiVersion":"apps/v1","kind":"Deployment","name":"workflow-engine"}}}
    creationTimestamp: "2025-10-21T20:24:19Z"
    name: workflow-engine-hpa
    namespace: llm-platform
    resourceVersion: "6752693"
    uid: 52deca0b-9960-428f-975f-86c0203f1cab
  spec:
    maxReplicas: 8
    metrics:
    - resource:
        name: cpu
        target:
          averageUtilization: 70
          type: Utilization
      type: Resource
    minReplicas: 2
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: workflow-engine
  status:
    conditions:
    - lastTransitionTime: "2025-11-11T16:06:58Z"
      message: the HPA controller was able to get the target's current scale
      reason: SucceededGetScale
      status: "True"
      type: AbleToScale
    - lastTransitionTime: "2025-10-21T20:24:34Z"
      message: scaling is disabled since the replica count of the target is zero
      reason: ScalingDisabled
      status: "False"
      type: ScalingActive
    currentMetrics: null
    desiredReplicas: 0
    lastScaleTime: "2025-10-21T21:49:36Z"
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"autoscaling/v2","kind":"HorizontalPodAutoscaler","metadata":{"annotations":{},"name":"agent-gateway-hpa","namespace":"ossa-prod"},"spec":{"behavior":{"scaleDown":{"policies":[{"periodSeconds":60,"type":"Percent","value":10}],"selectPolicy":"Max","stabilizationWindowSeconds":60},"scaleUp":{"policies":[{"periodSeconds":15,"type":"Percent","value":100},{"periodSeconds":15,"type":"Pods","value":2}],"selectPolicy":"Max","stabilizationWindowSeconds":30}},"maxReplicas":10,"metrics":[{"resource":{"name":"cpu","target":{"averageUtilization":70,"type":"Utilization"}},"type":"Resource"},{"resource":{"name":"memory","target":{"averageUtilization":80,"type":"Utilization"}},"type":"Resource"}],"minReplicas":1,"scaleTargetRef":{"apiVersion":"apps/v1","kind":"Deployment","name":"agent-gateway"}}}
    creationTimestamp: "2025-11-13T17:36:32Z"
    name: agent-gateway-hpa
    namespace: ossa-prod
    resourceVersion: "7643573"
    uid: 317ef13c-17a1-4b86-bd7c-5dc7a186e77a
  spec:
    behavior:
      scaleDown:
        policies:
        - periodSeconds: 60
          type: Percent
          value: 10
        selectPolicy: Max
        stabilizationWindowSeconds: 60
      scaleUp:
        policies:
        - periodSeconds: 15
          type: Percent
          value: 100
        - periodSeconds: 15
          type: Pods
          value: 2
        selectPolicy: Max
        stabilizationWindowSeconds: 30
    maxReplicas: 10
    metrics:
    - resource:
        name: cpu
        target:
          averageUtilization: 70
          type: Utilization
      type: Resource
    - resource:
        name: memory
        target:
          averageUtilization: 80
          type: Utilization
      type: Resource
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: agent-gateway
  status:
    conditions:
    - lastTransitionTime: "2025-11-13T17:36:47Z"
      message: the HPA controller was able to get the target's current scale
      reason: SucceededGetScale
      status: "True"
      type: AbleToScale
    - lastTransitionTime: "2025-11-13T17:36:47Z"
      message: 'the HPA was unable to compute the replica count: failed to get cpu
        utilization: unable to get metrics for resource cpu: no metrics returned from
        resource metrics API'
      reason: FailedGetResourceMetric
      status: "False"
      type: ScalingActive
    currentMetrics:
    - type: ""
    - type: ""
    currentReplicas: 1
    desiredReplicas: 0
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"autoscaling/v2","kind":"HorizontalPodAutoscaler","metadata":{"annotations":{},"name":"aiflow-agent-hpa","namespace":"social-agents"},"spec":{"behavior":{"scaleDown":{"policies":[{"periodSeconds":60,"type":"Percent","value":50}],"stabilizationWindowSeconds":300},"scaleUp":{"policies":[{"periodSeconds":30,"type":"Percent","value":100},{"periodSeconds":30,"type":"Pods","value":2}],"selectPolicy":"Max","stabilizationWindowSeconds":60}},"maxReplicas":5,"metrics":[{"resource":{"name":"cpu","target":{"averageUtilization":70,"type":"Utilization"}},"type":"Resource"},{"resource":{"name":"memory","target":{"averageUtilization":80,"type":"Utilization"}},"type":"Resource"}],"minReplicas":1,"scaleTargetRef":{"apiVersion":"apps/v1","kind":"Deployment","name":"aiflow-agent"}}}
    creationTimestamp: "2025-10-21T20:24:47Z"
    name: aiflow-agent-hpa
    namespace: social-agents
    resourceVersion: "6716401"
    uid: 1d65b4af-c04d-4de1-8d40-9170083d982b
  spec:
    behavior:
      scaleDown:
        policies:
        - periodSeconds: 60
          type: Percent
          value: 50
        selectPolicy: Max
        stabilizationWindowSeconds: 300
      scaleUp:
        policies:
        - periodSeconds: 30
          type: Percent
          value: 100
        - periodSeconds: 30
          type: Pods
          value: 2
        selectPolicy: Max
        stabilizationWindowSeconds: 60
    maxReplicas: 5
    metrics:
    - resource:
        name: cpu
        target:
          averageUtilization: 70
          type: Utilization
      type: Resource
    - resource:
        name: memory
        target:
          averageUtilization: 80
          type: Utilization
      type: Resource
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: aiflow-agent
  status:
    conditions:
    - lastTransitionTime: "2025-11-11T02:35:21Z"
      message: the HPA controller was able to get the target's current scale
      reason: SucceededGetScale
      status: "True"
      type: AbleToScale
    - lastTransitionTime: "2025-10-21T20:25:02Z"
      message: scaling is disabled since the replica count of the target is zero
      reason: ScalingDisabled
      status: "False"
      type: ScalingActive
    currentMetrics: null
    desiredReplicas: 0
- apiVersion: batch/v1
  kind: CronJob
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: "{\"apiVersion\":\"batch/v1\",\"kind\":\"CronJob\",\"metadata\":{\"annotations\":{},\"labels\":{\"app\":\"k8s-auto-cleanup\",\"managed-by\":\"buildkit\"},\"name\":\"k8s-auto-cleanup\",\"namespace\":\"default\"},\"spec\":{\"concurrencyPolicy\":\"Replace\",\"failedJobsHistoryLimit\":1,\"jobTemplate\":{\"spec\":{\"activeDeadlineSeconds\":600,\"backoffLimit\":0,\"template\":{\"metadata\":{\"labels\":{\"app\":\"k8s-auto-cleanup\"}},\"spec\":{\"containers\":[{\"command\":[\"/bin/bash\",\"-c\",\"set
        -e\\n\\necho \\\"\U0001F9F9 Starting K8s Auto-Cleanup at $(date)\\\"\\n\\n#
        Step 1: Delete failed pods\\necho \\\"\U0001F4E6 Step 1: Cleaning failed pods...\\\"\\nFAILED_COUNT=$(kubectl
        get pods -A --field-selector=status.phase=Failed --no-headers | wc -l)\\nif
        [ \\\"$FAILED_COUNT\\\" -gt 0 ]; then\\n  kubectl delete pods --field-selector=status.phase=Failed
        -A\\n  echo \\\" Deleted $FAILED_COUNT failed pods\\\"\\nelse\\n  echo \\\"
        No failed pods found\\\"\\nfi\\n\\n# Step 2: Scale down broken deployments
        (0 ready replicas)\\necho \\\"\\\"\\necho \\\"\U0001F4CA Step 2: Finding broken
        deployments...\\\"\\nBROKEN_DEPS=$(kubectl get deployments -A -o json | \\\\\\n
        \ jq -r '.items[] | select(.spec.replicas \\u003e 0 and (.status.readyReplicas
        == null or .status.readyReplicas == 0)) | \\\"\\\\(.metadata.namespace)/\\\\(.metadata.name)\\\"')\\n\\nif
        [ -n \\\"$BROKEN_DEPS\\\" ]; then\\n  BROKEN_COUNT=$(echo \\\"$BROKEN_DEPS\\\"
        | wc -l | tr -d ' ')\\n  echo \\\"  Found $BROKEN_COUNT broken deployments\\\"\\n\\n
        \ echo \\\"$BROKEN_DEPS\\\" | while read dep; do\\n    NAMESPACE=$(echo $dep
        | cut -d'/' -f1)\\n    NAME=$(echo $dep | cut -d'/' -f2)\\n    kubectl scale
        deployment \\\"$NAME\\\" -n \\\"$NAMESPACE\\\" --replicas=0 || true\\n    echo
        \\\"    $dep\\\"\\n  done\\n\\n  echo \\\" Scaled down $BROKEN_COUNT deployments\\\"\\nelse\\n
        \ echo \\\" No broken deployments found\\\"\\nfi\\n\\n# Step 3: Report final
        status\\necho \\\"\\\"\\necho \\\"\U0001F4CA Final Status:\\\"\\necho \\\"Running
        pods:  $(kubectl get pods -A --field-selector=status.phase=Running --no-headers
        | wc -l)\\\"\\necho \\\"Pending pods:  $(kubectl get pods -A --field-selector=status.phase=Pending
        --no-headers | wc -l)\\\"\\necho \\\"Failed pods:   $(kubectl get pods -A
        --field-selector=status.phase=Failed --no-headers | wc -l)\\\"\\n\\necho \\\"\\\"\\necho
        \\\" K8s Auto-Cleanup completed at $(date)\\\"\\n\"],\"image\":\"bitnami/kubectl:latest\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"cleanup\",\"resources\":{\"limits\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"},\"requests\":{\"cpu\":\"50m\",\"memory\":\"64Mi\"}}}],\"restartPolicy\":\"Never\",\"serviceAccountName\":\"k8s-cleanup-sa\"}}}},\"schedule\":\"0
        3 * * *\",\"successfulJobsHistoryLimit\":3}}\n"
    creationTimestamp: "2025-11-11T17:29:52Z"
    generation: 1
    labels:
      app: k8s-auto-cleanup
      managed-by: buildkit
    name: k8s-auto-cleanup
    namespace: default
    resourceVersion: "7511787"
    uid: 188457b8-a4df-46a8-a4d3-5c3e96e972f6
  spec:
    concurrencyPolicy: Replace
    failedJobsHistoryLimit: 1
    jobTemplate:
      metadata:
        creationTimestamp: null
      spec:
        activeDeadlineSeconds: 600
        backoffLimit: 0
        template:
          metadata:
            creationTimestamp: null
            labels:
              app: k8s-auto-cleanup
          spec:
            containers:
            - command:
              - /bin/bash
              - -c
              - "set -e\n\necho \"\U0001F9F9 Starting K8s Auto-Cleanup at $(date)\"\n\n#
                Step 1: Delete failed pods\necho \"\U0001F4E6 Step 1: Cleaning failed
                pods...\"\nFAILED_COUNT=$(kubectl get pods -A --field-selector=status.phase=Failed
                --no-headers | wc -l)\nif [ \"$FAILED_COUNT\" -gt 0 ]; then\n  kubectl
                delete pods --field-selector=status.phase=Failed -A\n  echo \" Deleted
                $FAILED_COUNT failed pods\"\nelse\n  echo \" No failed pods found\"\nfi\n\n#
                Step 2: Scale down broken deployments (0 ready replicas)\necho \"\"\necho
                \"\U0001F4CA Step 2: Finding broken deployments...\"\nBROKEN_DEPS=$(kubectl
                get deployments -A -o json | \\\n  jq -r '.items[] | select(.spec.replicas
                > 0 and (.status.readyReplicas == null or .status.readyReplicas ==
                0)) | \"\\(.metadata.namespace)/\\(.metadata.name)\"')\n\nif [ -n
                \"$BROKEN_DEPS\" ]; then\n  BROKEN_COUNT=$(echo \"$BROKEN_DEPS\" |
                wc -l | tr -d ' ')\n  echo \"  Found $BROKEN_COUNT broken deployments\"\n\n
                \ echo \"$BROKEN_DEPS\" | while read dep; do\n    NAMESPACE=$(echo
                $dep | cut -d'/' -f1)\n    NAME=$(echo $dep | cut -d'/' -f2)\n    kubectl
                scale deployment \"$NAME\" -n \"$NAMESPACE\" --replicas=0 || true\n
                \   echo \"    $dep\"\n  done\n\n  echo \" Scaled down $BROKEN_COUNT
                deployments\"\nelse\n  echo \" No broken deployments found\"\nfi\n\n#
                Step 3: Report final status\necho \"\"\necho \"\U0001F4CA Final Status:\"\necho
                \"Running pods:  $(kubectl get pods -A --field-selector=status.phase=Running
                --no-headers | wc -l)\"\necho \"Pending pods:  $(kubectl get pods
                -A --field-selector=status.phase=Pending --no-headers | wc -l)\"\necho
                \"Failed pods:   $(kubectl get pods -A --field-selector=status.phase=Failed
                --no-headers | wc -l)\"\n\necho \"\"\necho \" K8s Auto-Cleanup completed
                at $(date)\"\n"
              image: bitnami/kubectl:latest
              imagePullPolicy: IfNotPresent
              name: cleanup
              resources:
                limits:
                  cpu: 100m
                  memory: 128Mi
                requests:
                  cpu: 50m
                  memory: 64Mi
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
            dnsPolicy: ClusterFirst
            restartPolicy: Never
            schedulerName: default-scheduler
            securityContext: {}
            serviceAccount: k8s-cleanup-sa
            serviceAccountName: k8s-cleanup-sa
            terminationGracePeriodSeconds: 30
    schedule: 0 3 * * *
    successfulJobsHistoryLimit: 3
    suspend: false
  status:
    lastScheduleTime: "2025-11-17T08:00:00Z"
    lastSuccessfulTime: "2025-11-17T08:00:38Z"
- apiVersion: batch/v1
  kind: CronJob
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"batch/v1","kind":"CronJob","metadata":{"annotations":{},"name":"resource-optimizer","namespace":"kube-system"},"spec":{"failedJobsHistoryLimit":2,"jobTemplate":{"spec":{"template":{"metadata":{"labels":{"app":"resource-optimizer"}},"spec":{"containers":[{"command":["/bin/sh","-c","echo \"Starting resource optimization...\"\n\n# Get all deployments without resource requests\nkubectl get deployments --all-namespaces -o json | \\\n  jq -r '.items[] | select(.spec.template.spec.containers[0].resources.requests == null) | \"\\(.metadata.namespace) \\(.metadata.name)\"' | \\\n  while read namespace name; do\n    echo \"Adding default resources to deployment: $namespace/$name\"\n    \n    # Patch deployment with default resources\n    kubectl patch deployment \"$name\" -n \"$namespace\" --type='json' -p='[\n      {\n        \"op\": \"add\",\n        \"path\": \"/spec/template/spec/containers/0/resources\",\n        \"value\": {\n          \"requests\": {\n            \"cpu\": \"100m\",\n            \"memory\": \"128Mi\"\n          },\n          \"limits\": {\n            \"cpu\": \"500m\",\n            \"memory\": \"512Mi\"\n          }\n        }\n      }\n    ]' || echo \"Failed to patch $namespace/$name\"\n  done\n\necho \"Resource optimization completed\"\n"],"image":"bitnami/kubectl:latest","name":"optimizer","resources":{"limits":{"cpu":"100m","memory":"128Mi"},"requests":{"cpu":"50m","memory":"64Mi"}}}],"restartPolicy":"OnFailure","serviceAccountName":"resource-optimizer"}}}},"schedule":"0 */6 * * *","successfulJobsHistoryLimit":2}}
    creationTimestamp: "2025-11-10T04:40:40Z"
    generation: 1
    name: resource-optimizer
    namespace: kube-system
    resourceVersion: "7630441"
    uid: e7f28190-a979-489b-b5e3-1b4b577009d4
  spec:
    concurrencyPolicy: Allow
    failedJobsHistoryLimit: 2
    jobTemplate:
      metadata:
        creationTimestamp: null
      spec:
        template:
          metadata:
            creationTimestamp: null
            labels:
              app: resource-optimizer
          spec:
            containers:
            - command:
              - /bin/sh
              - -c
              - "echo \"Starting resource optimization...\"\n\n# Get all deployments
                without resource requests\nkubectl get deployments --all-namespaces
                -o json | \\\n  jq -r '.items[] | select(.spec.template.spec.containers[0].resources.requests
                == null) | \"\\(.metadata.namespace) \\(.metadata.name)\"' | \\\n
                \ while read namespace name; do\n    echo \"Adding default resources
                to deployment: $namespace/$name\"\n    \n    # Patch deployment with
                default resources\n    kubectl patch deployment \"$name\" -n \"$namespace\"
                --type='json' -p='[\n      {\n        \"op\": \"add\",\n        \"path\":
                \"/spec/template/spec/containers/0/resources\",\n        \"value\":
                {\n          \"requests\": {\n            \"cpu\": \"100m\",\n            \"memory\":
                \"128Mi\"\n          },\n          \"limits\": {\n            \"cpu\":
                \"500m\",\n            \"memory\": \"512Mi\"\n          }\n        }\n
                \     }\n    ]' || echo \"Failed to patch $namespace/$name\"\n  done\n\necho
                \"Resource optimization completed\"\n"
              image: bitnami/kubectl:latest
              imagePullPolicy: Always
              name: optimizer
              resources:
                limits:
                  cpu: 100m
                  memory: 128Mi
                requests:
                  cpu: 50m
                  memory: 64Mi
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
            dnsPolicy: ClusterFirst
            restartPolicy: OnFailure
            schedulerName: default-scheduler
            securityContext: {}
            serviceAccount: resource-optimizer
            serviceAccountName: resource-optimizer
            terminationGracePeriodSeconds: 30
    schedule: 0 */6 * * *
    successfulJobsHistoryLimit: 2
    suspend: false
  status:
    lastScheduleTime: "2025-11-17T23:00:00Z"
    lastSuccessfulTime: "2025-11-17T23:45:47Z"
- apiVersion: batch/v1
  kind: CronJob
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: "{\"apiVersion\":\"batch/v1\",\"kind\":\"CronJob\",\"metadata\":{\"annotations\":{},\"name\":\"vpa-recommender\",\"namespace\":\"kube-system\"},\"spec\":{\"failedJobsHistoryLimit\":3,\"jobTemplate\":{\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app\":\"vpa-recommender\"}},\"spec\":{\"containers\":[{\"command\":[\"/bin/sh\",\"-c\",\"echo
        \\\"=== VPA Resource Recommendations ===\\\"\\necho \\\"\\\"\\n\\n# Get all
        running pods with their resource usage\\nkubectl top pods --all-namespaces
        --no-headers 2\\u003e/dev/null | while read namespace name cpu_usage memory_usage
        rest; do\\n  # Get pod resource requests/limits\\n  pod_json=$(kubectl get
        pod \\\"$name\\\" -n \\\"$namespace\\\" -o json 2\\u003e/dev/null || echo
        \\\"{}\\\")\\n  \\n  cpu_request=$(echo \\\"$pod_json\\\" | jq -r '.spec.containers[0].resources.requests.cpu
        // \\\"none\\\"')\\n  memory_request=$(echo \\\"$pod_json\\\" | jq -r '.spec.containers[0].resources.requests.memory
        // \\\"none\\\"')\\n  cpu_limit=$(echo \\\"$pod_json\\\" | jq -r '.spec.containers[0].resources.limits.cpu
        // \\\"none\\\"')\\n  memory_limit=$(echo \\\"$pod_json\\\" | jq -r '.spec.containers[0].resources.limits.memory
        // \\\"none\\\"')\\n  \\n  # Parse CPU usage (remove 'm' suffix)\\n  cpu_num=$(echo
        \\\"$cpu_usage\\\" | sed 's/m//' | awk '{print int($1)}')\\n  memory_num=$(echo
        \\\"$memory_usage\\\" | sed 's/Mi//' | awk '{print int($1)}')\\n  \\n  # Calculate
        recommendations (add 20% buffer)\\n  if [ \\\"$cpu_request\\\" != \\\"none\\\"
        ] \\u0026\\u0026 [ \\\"$cpu_num\\\" -gt 0 ]; then\\n    cpu_req_num=$(echo
        \\\"$cpu_request\\\" | sed 's/m//' | awk '{print int($1)}')\\n    if [ \\\"$cpu_num\\\"
        -lt $((cpu_req_num * 50 / 100)) ]; then\\n      echo \\\"  $namespace/$name:
        CPU underutilized (using ${cpu_usage}, requested ${cpu_request})\\\"\\n    elif
        [ \\\"$cpu_num\\\" -gt $((cpu_req_num * 90 / 100)) ]; then\\n      recommended=$((cpu_num
        * 120 / 100))\\n      echo \\\"\U0001F4C8 $namespace/$name: CPU recommendation:
        ${recommended}m (currently using ${cpu_usage}, requested ${cpu_request})\\\"\\n
        \   fi\\n  fi\\n  \\n  if [ \\\"$memory_request\\\" != \\\"none\\\" ] \\u0026\\u0026
        [ \\\"$memory_num\\\" -gt 0 ]; then\\n    mem_req_num=$(echo \\\"$memory_request\\\"
        | sed 's/Mi//' | awk '{print int($1)}')\\n    if [ \\\"$memory_num\\\" -lt
        $((mem_req_num * 50 / 100)) ]; then\\n      echo \\\"  $namespace/$name:
        Memory underutilized (using ${memory_usage}, requested ${memory_request})\\\"\\n
        \   elif [ \\\"$memory_num\\\" -gt $((mem_req_num * 90 / 100)) ]; then\\n
        \     recommended=$((memory_num * 120 / 100))\\n      echo \\\"\U0001F4C8
        $namespace/$name: Memory recommendation: ${recommended}Mi (currently using
        ${memory_usage}, requested ${memory_request})\\\"\\n    fi\\n  fi\\ndone\\n\\necho
        \\\"\\\"\\necho \\\"=== Pods Without Resource Requests ===\\\"\\nkubectl get
        pods --all-namespaces -o json | \\\\\\n  jq -r '.items[] | select(.status.phase
        == \\\"Running\\\") | select(.spec.containers[0].resources.requests == null)
        | \\\"\\\\(.metadata.namespace)/\\\\(.metadata.name)\\\"' | \\\\\\n  while
        read pod; do\\n    echo \\\" $pod: Missing resource requests\\\"\\n  done\\n\\necho
        \\\"\\\"\\necho \\\"=== Resource Summary ===\\\"\\ntotal_pods=$(kubectl get
        pods --all-namespaces --field-selector=status.phase==Running --no-headers
        | wc -l | tr -d ' ')\\npods_with_resources=$(kubectl get pods --all-namespaces
        -o json | jq -r '[.items[] | select(.status.phase == \\\"Running\\\") | select(.spec.containers[0].resources.requests
        != null)] | length')\\necho \\\"Running pods: $total_pods\\\"\\necho \\\"Pods
        with resource requests: $pods_with_resources\\\"\\necho \\\"Pods without resource
        requests: $((total_pods - pods_with_resources))\\\"\\n\"],\"image\":\"bitnami/kubectl:latest\",\"name\":\"recommender\",\"resources\":{\"limits\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"},\"requests\":{\"cpu\":\"50m\",\"memory\":\"64Mi\"}}}],\"restartPolicy\":\"OnFailure\",\"serviceAccountName\":\"vpa-recommender\"}}}},\"schedule\":\"0
        */4 * * *\",\"successfulJobsHistoryLimit\":3}}\n"
    creationTimestamp: "2025-11-10T04:40:49Z"
    generation: 1
    name: vpa-recommender
    namespace: kube-system
    resourceVersion: "7639618"
    uid: 2a69835c-9dca-498f-a5df-701ee4ec51f3
  spec:
    concurrencyPolicy: Allow
    failedJobsHistoryLimit: 3
    jobTemplate:
      metadata:
        creationTimestamp: null
      spec:
        template:
          metadata:
            creationTimestamp: null
            labels:
              app: vpa-recommender
          spec:
            containers:
            - command:
              - /bin/sh
              - -c
              - "echo \"=== VPA Resource Recommendations ===\"\necho \"\"\n\n# Get
                all running pods with their resource usage\nkubectl top pods --all-namespaces
                --no-headers 2>/dev/null | while read namespace name cpu_usage memory_usage
                rest; do\n  # Get pod resource requests/limits\n  pod_json=$(kubectl
                get pod \"$name\" -n \"$namespace\" -o json 2>/dev/null || echo \"{}\")\n
                \ \n  cpu_request=$(echo \"$pod_json\" | jq -r '.spec.containers[0].resources.requests.cpu
                // \"none\"')\n  memory_request=$(echo \"$pod_json\" | jq -r '.spec.containers[0].resources.requests.memory
                // \"none\"')\n  cpu_limit=$(echo \"$pod_json\" | jq -r '.spec.containers[0].resources.limits.cpu
                // \"none\"')\n  memory_limit=$(echo \"$pod_json\" | jq -r '.spec.containers[0].resources.limits.memory
                // \"none\"')\n  \n  # Parse CPU usage (remove 'm' suffix)\n  cpu_num=$(echo
                \"$cpu_usage\" | sed 's/m//' | awk '{print int($1)}')\n  memory_num=$(echo
                \"$memory_usage\" | sed 's/Mi//' | awk '{print int($1)}')\n  \n  #
                Calculate recommendations (add 20% buffer)\n  if [ \"$cpu_request\"
                != \"none\" ] && [ \"$cpu_num\" -gt 0 ]; then\n    cpu_req_num=$(echo
                \"$cpu_request\" | sed 's/m//' | awk '{print int($1)}')\n    if [
                \"$cpu_num\" -lt $((cpu_req_num * 50 / 100)) ]; then\n      echo \"
                \ $namespace/$name: CPU underutilized (using ${cpu_usage}, requested
                ${cpu_request})\"\n    elif [ \"$cpu_num\" -gt $((cpu_req_num * 90
                / 100)) ]; then\n      recommended=$((cpu_num * 120 / 100))\n      echo
                \"\U0001F4C8 $namespace/$name: CPU recommendation: ${recommended}m
                (currently using ${cpu_usage}, requested ${cpu_request})\"\n    fi\n
                \ fi\n  \n  if [ \"$memory_request\" != \"none\" ] && [ \"$memory_num\"
                -gt 0 ]; then\n    mem_req_num=$(echo \"$memory_request\" | sed 's/Mi//'
                | awk '{print int($1)}')\n    if [ \"$memory_num\" -lt $((mem_req_num
                * 50 / 100)) ]; then\n      echo \"  $namespace/$name: Memory underutilized
                (using ${memory_usage}, requested ${memory_request})\"\n    elif [
                \"$memory_num\" -gt $((mem_req_num * 90 / 100)) ]; then\n      recommended=$((memory_num
                * 120 / 100))\n      echo \"\U0001F4C8 $namespace/$name: Memory recommendation:
                ${recommended}Mi (currently using ${memory_usage}, requested ${memory_request})\"\n
                \   fi\n  fi\ndone\n\necho \"\"\necho \"=== Pods Without Resource
                Requests ===\"\nkubectl get pods --all-namespaces -o json | \\\n  jq
                -r '.items[] | select(.status.phase == \"Running\") | select(.spec.containers[0].resources.requests
                == null) | \"\\(.metadata.namespace)/\\(.metadata.name)\"' | \\\n
                \ while read pod; do\n    echo \" $pod: Missing resource requests\"\n
                \ done\n\necho \"\"\necho \"=== Resource Summary ===\"\ntotal_pods=$(kubectl
                get pods --all-namespaces --field-selector=status.phase==Running --no-headers
                | wc -l | tr -d ' ')\npods_with_resources=$(kubectl get pods --all-namespaces
                -o json | jq -r '[.items[] | select(.status.phase == \"Running\")
                | select(.spec.containers[0].resources.requests != null)] | length')\necho
                \"Running pods: $total_pods\"\necho \"Pods with resource requests:
                $pods_with_resources\"\necho \"Pods without resource requests: $((total_pods
                - pods_with_resources))\"\n"
              image: bitnami/kubectl:latest
              imagePullPolicy: Always
              name: recommender
              resources:
                limits:
                  cpu: 100m
                  memory: 128Mi
                requests:
                  cpu: 50m
                  memory: 64Mi
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
            dnsPolicy: ClusterFirst
            restartPolicy: OnFailure
            schedulerName: default-scheduler
            securityContext: {}
            serviceAccount: vpa-recommender
            serviceAccountName: vpa-recommender
            terminationGracePeriodSeconds: 30
    schedule: 0 */4 * * *
    successfulJobsHistoryLimit: 3
    suspend: false
  status:
    lastScheduleTime: "2025-11-18T01:00:00Z"
    lastSuccessfulTime: "2025-11-18T01:00:08Z"
- apiVersion: batch/v1
  kind: CronJob
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"batch/v1","kind":"CronJob","metadata":{"annotations":{},"labels":{"app":"registry-sync","component":"service-discovery"},"name":"registry-sync","namespace":"llm-platform"},"spec":{"concurrencyPolicy":"Forbid","failedJobsHistoryLimit":1,"jobTemplate":{"metadata":{"labels":{"app":"registry-sync","component":"service-discovery"}},"spec":{"activeDeadlineSeconds":300,"backoffLimit":1,"template":{"metadata":{"labels":{"app":"registry-sync","component":"service-discovery"}},"spec":{"containers":[{"command":["npx","tsx","src/services/registry-sync/cli.ts"],"env":[{"name":"NODE_ENV","value":"production"}],"envFrom":[{"configMapRef":{"name":"registry-sync-config"}}],"image":"registry-sync:latest","imagePullPolicy":"IfNotPresent","name":"registry-sync","resources":{"limits":{"cpu":"500m","memory":"512Mi"},"requests":{"cpu":"100m","memory":"256Mi"}},"volumeMounts":[{"mountPath":"/workspace","name":"workspace","readOnly":true}]}],"restartPolicy":"OnFailure","volumes":[{"hostPath":{"path":"/Users/flux423/Sites/LLM","type":"Directory"},"name":"workspace"}]}}}},"schedule":"*/15 * * * *","successfulJobsHistoryLimit":3}}
    creationTimestamp: "2025-10-09T23:44:46Z"
    generation: 1
    labels:
      app: registry-sync
      component: service-discovery
    name: registry-sync
    namespace: llm-platform
    resourceVersion: "7647642"
    uid: 8ddfc9e8-ae14-4fd8-a568-834b1c76f113
  spec:
    concurrencyPolicy: Forbid
    failedJobsHistoryLimit: 1
    jobTemplate:
      metadata:
        creationTimestamp: null
        labels:
          app: registry-sync
          component: service-discovery
      spec:
        activeDeadlineSeconds: 300
        backoffLimit: 1
        template:
          metadata:
            creationTimestamp: null
            labels:
              app: registry-sync
              component: service-discovery
          spec:
            containers:
            - command:
              - npx
              - tsx
              - src/services/registry-sync/cli.ts
              env:
              - name: NODE_ENV
                value: production
              envFrom:
              - configMapRef:
                  name: registry-sync-config
              image: registry-sync:latest
              imagePullPolicy: IfNotPresent
              name: registry-sync
              resources:
                limits:
                  cpu: 500m
                  memory: 512Mi
                requests:
                  cpu: 100m
                  memory: 256Mi
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              volumeMounts:
              - mountPath: /workspace
                name: workspace
                readOnly: true
            dnsPolicy: ClusterFirst
            restartPolicy: OnFailure
            schedulerName: default-scheduler
            securityContext: {}
            terminationGracePeriodSeconds: 30
            volumes:
            - hostPath:
                path: /Users/flux423/Sites/LLM
                type: Directory
              name: workspace
    schedule: '*/15 * * * *'
    successfulJobsHistoryLimit: 3
    suspend: false
  status:
    lastScheduleTime: "2025-11-18T02:15:00Z"
- apiVersion: batch/v1
  kind: CronJob
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"batch/v1","kind":"CronJob","metadata":{"annotations":{},"name":"pod-cleanup","namespace":"pod-cleanup"},"spec":{"failedJobsHistoryLimit":3,"jobTemplate":{"spec":{"template":{"metadata":{"labels":{"app":"pod-cleanup"}},"spec":{"containers":[{"command":["/bin/sh","-c","echo \"Starting pod cleanup...\"\n\n# Clean up Succeeded pods older than 1 hour\nSUCCEEDED=$(kubectl get pods --all-namespaces --field-selector=status.phase==Succeeded -o json | jq -r '.items[] | select((.status.startTime // .metadata.creationTimestamp) | fromdateiso8601 \u003c (now - 3600)) | \"\\(.metadata.namespace) \\(.metadata.name)\"')\n\nif [ -n \"$SUCCEEDED\" ]; then\n  echo \"$SUCCEEDED\" | while read namespace name; do\n    echo \"Deleting succeeded pod: $namespace/$name\"\n    kubectl delete pod \"$name\" -n \"$namespace\" --grace-period=0 || true\n  done\nfi\n\n# Clean up Failed pods older than 24 hours\nFAILED=$(kubectl get pods --all-namespaces --field-selector=status.phase==Failed -o json | jq -r '.items[] | select((.status.startTime // .metadata.creationTimestamp) | fromdateiso8601 \u003c (now - 86400)) | \"\\(.metadata.namespace) \\(.metadata.name)\"')\n\nif [ -n \"$FAILED\" ]; then\n  echo \"$FAILED\" | while read namespace name; do\n    echo \"Deleting failed pod: $namespace/$name\"\n    kubectl delete pod \"$name\" -n \"$namespace\" --grace-period=0 || true\n  done\nfi\n\n# Clean up Evicted pods immediately\nEVICTED=$(kubectl get pods --all-namespaces --field-selector=status.phase==Failed -o json | jq -r '.items[] | select(.status.reason == \"Evicted\") | \"\\(.metadata.namespace) \\(.metadata.name)\"')\n\nif [ -n \"$EVICTED\" ]; then\n  echo \"$EVICTED\" | while read namespace name; do\n    echo \"Deleting evicted pod: $namespace/$name\"\n    kubectl delete pod \"$name\" -n \"$namespace\" --grace-period=0 || true\n  done\nfi\n\necho \"Pod cleanup completed\"\n"],"image":"bitnami/kubectl:latest","name":"cleanup","resources":{"limits":{"cpu":"100m","memory":"128Mi"},"requests":{"cpu":"50m","memory":"64Mi"}}}],"restartPolicy":"OnFailure","serviceAccountName":"pod-cleanup"}}}},"schedule":"*/30 * * * *","successfulJobsHistoryLimit":3}}
    creationTimestamp: "2025-11-10T04:40:39Z"
    generation: 1
    name: pod-cleanup
    namespace: pod-cleanup
    resourceVersion: "7645964"
    uid: 922f3cf1-b800-42c6-b831-12e7c80be8e7
  spec:
    concurrencyPolicy: Allow
    failedJobsHistoryLimit: 3
    jobTemplate:
      metadata:
        creationTimestamp: null
      spec:
        template:
          metadata:
            creationTimestamp: null
            labels:
              app: pod-cleanup
          spec:
            containers:
            - command:
              - /bin/sh
              - -c
              - |
                echo "Starting pod cleanup..."

                # Clean up Succeeded pods older than 1 hour
                SUCCEEDED=$(kubectl get pods --all-namespaces --field-selector=status.phase==Succeeded -o json | jq -r '.items[] | select((.status.startTime // .metadata.creationTimestamp) | fromdateiso8601 < (now - 3600)) | "\(.metadata.namespace) \(.metadata.name)"')

                if [ -n "$SUCCEEDED" ]; then
                  echo "$SUCCEEDED" | while read namespace name; do
                    echo "Deleting succeeded pod: $namespace/$name"
                    kubectl delete pod "$name" -n "$namespace" --grace-period=0 || true
                  done
                fi

                # Clean up Failed pods older than 24 hours
                FAILED=$(kubectl get pods --all-namespaces --field-selector=status.phase==Failed -o json | jq -r '.items[] | select((.status.startTime // .metadata.creationTimestamp) | fromdateiso8601 < (now - 86400)) | "\(.metadata.namespace) \(.metadata.name)"')

                if [ -n "$FAILED" ]; then
                  echo "$FAILED" | while read namespace name; do
                    echo "Deleting failed pod: $namespace/$name"
                    kubectl delete pod "$name" -n "$namespace" --grace-period=0 || true
                  done
                fi

                # Clean up Evicted pods immediately
                EVICTED=$(kubectl get pods --all-namespaces --field-selector=status.phase==Failed -o json | jq -r '.items[] | select(.status.reason == "Evicted") | "\(.metadata.namespace) \(.metadata.name)"')

                if [ -n "$EVICTED" ]; then
                  echo "$EVICTED" | while read namespace name; do
                    echo "Deleting evicted pod: $namespace/$name"
                    kubectl delete pod "$name" -n "$namespace" --grace-period=0 || true
                  done
                fi

                echo "Pod cleanup completed"
              image: bitnami/kubectl:latest
              imagePullPolicy: Always
              name: cleanup
              resources:
                limits:
                  cpu: 100m
                  memory: 128Mi
                requests:
                  cpu: 50m
                  memory: 64Mi
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
            dnsPolicy: ClusterFirst
            restartPolicy: OnFailure
            schedulerName: default-scheduler
            securityContext: {}
            serviceAccount: pod-cleanup
            serviceAccountName: pod-cleanup
            terminationGracePeriodSeconds: 30
    schedule: '*/30 * * * *'
    successfulJobsHistoryLimit: 3
    suspend: false
  status:
    lastScheduleTime: "2025-11-18T02:00:00Z"
    lastSuccessfulTime: "2025-11-18T02:00:10Z"
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-11-15T08:00:00Z"
    creationTimestamp: "2025-11-15T08:10:45Z"
    generation: 1
    labels:
      app: k8s-auto-cleanup
      batch.kubernetes.io/controller-uid: ada4e611-0325-44a5-9d8d-f4cc6d32bb3b
      batch.kubernetes.io/job-name: k8s-auto-cleanup-29386560
      controller-uid: ada4e611-0325-44a5-9d8d-f4cc6d32bb3b
      job-name: k8s-auto-cleanup-29386560
    name: k8s-auto-cleanup-29386560
    namespace: default
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: k8s-auto-cleanup
      uid: 188457b8-a4df-46a8-a4d3-5c3e96e972f6
    resourceVersion: "7226228"
    uid: ada4e611-0325-44a5-9d8d-f4cc6d32bb3b
  spec:
    activeDeadlineSeconds: 600
    backoffLimit: 0
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: ada4e611-0325-44a5-9d8d-f4cc6d32bb3b
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: k8s-auto-cleanup
          batch.kubernetes.io/controller-uid: ada4e611-0325-44a5-9d8d-f4cc6d32bb3b
          batch.kubernetes.io/job-name: k8s-auto-cleanup-29386560
          controller-uid: ada4e611-0325-44a5-9d8d-f4cc6d32bb3b
          job-name: k8s-auto-cleanup-29386560
      spec:
        containers:
        - command:
          - /bin/bash
          - -c
          - "set -e\n\necho \"\U0001F9F9 Starting K8s Auto-Cleanup at $(date)\"\n\n#
            Step 1: Delete failed pods\necho \"\U0001F4E6 Step 1: Cleaning failed
            pods...\"\nFAILED_COUNT=$(kubectl get pods -A --field-selector=status.phase=Failed
            --no-headers | wc -l)\nif [ \"$FAILED_COUNT\" -gt 0 ]; then\n  kubectl
            delete pods --field-selector=status.phase=Failed -A\n  echo \" Deleted
            $FAILED_COUNT failed pods\"\nelse\n  echo \" No failed pods found\"\nfi\n\n#
            Step 2: Scale down broken deployments (0 ready replicas)\necho \"\"\necho
            \"\U0001F4CA Step 2: Finding broken deployments...\"\nBROKEN_DEPS=$(kubectl
            get deployments -A -o json | \\\n  jq -r '.items[] | select(.spec.replicas
            > 0 and (.status.readyReplicas == null or .status.readyReplicas == 0))
            | \"\\(.metadata.namespace)/\\(.metadata.name)\"')\n\nif [ -n \"$BROKEN_DEPS\"
            ]; then\n  BROKEN_COUNT=$(echo \"$BROKEN_DEPS\" | wc -l | tr -d ' ')\n
            \ echo \"  Found $BROKEN_COUNT broken deployments\"\n\n  echo \"$BROKEN_DEPS\"
            | while read dep; do\n    NAMESPACE=$(echo $dep | cut -d'/' -f1)\n    NAME=$(echo
            $dep | cut -d'/' -f2)\n    kubectl scale deployment \"$NAME\" -n \"$NAMESPACE\"
            --replicas=0 || true\n    echo \"    $dep\"\n  done\n\n  echo \" Scaled
            down $BROKEN_COUNT deployments\"\nelse\n  echo \" No broken deployments
            found\"\nfi\n\n# Step 3: Report final status\necho \"\"\necho \"\U0001F4CA
            Final Status:\"\necho \"Running pods:  $(kubectl get pods -A --field-selector=status.phase=Running
            --no-headers | wc -l)\"\necho \"Pending pods:  $(kubectl get pods -A --field-selector=status.phase=Pending
            --no-headers | wc -l)\"\necho \"Failed pods:   $(kubectl get pods -A --field-selector=status.phase=Failed
            --no-headers | wc -l)\"\n\necho \"\"\necho \" K8s Auto-Cleanup completed
            at $(date)\"\n"
          image: bitnami/kubectl:latest
          imagePullPolicy: IfNotPresent
          name: cleanup
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Never
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: k8s-cleanup-sa
        serviceAccountName: k8s-cleanup-sa
        terminationGracePeriodSeconds: 30
  status:
    completionTime: "2025-11-15T08:11:01Z"
    conditions:
    - lastProbeTime: "2025-11-15T08:11:01Z"
      lastTransitionTime: "2025-11-15T08:11:01Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-11-15T08:11:01Z"
      lastTransitionTime: "2025-11-15T08:11:01Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-11-15T08:10:45Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-11-16T08:00:00Z"
    creationTimestamp: "2025-11-16T08:04:58Z"
    generation: 1
    labels:
      app: k8s-auto-cleanup
      batch.kubernetes.io/controller-uid: 1ef03f5c-10d8-4876-b810-41b28ad4a1b5
      batch.kubernetes.io/job-name: k8s-auto-cleanup-29388000
      controller-uid: 1ef03f5c-10d8-4876-b810-41b28ad4a1b5
      job-name: k8s-auto-cleanup-29388000
    name: k8s-auto-cleanup-29388000
    namespace: default
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: k8s-auto-cleanup
      uid: 188457b8-a4df-46a8-a4d3-5c3e96e972f6
    resourceVersion: "7360371"
    uid: 1ef03f5c-10d8-4876-b810-41b28ad4a1b5
  spec:
    activeDeadlineSeconds: 600
    backoffLimit: 0
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 1ef03f5c-10d8-4876-b810-41b28ad4a1b5
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: k8s-auto-cleanup
          batch.kubernetes.io/controller-uid: 1ef03f5c-10d8-4876-b810-41b28ad4a1b5
          batch.kubernetes.io/job-name: k8s-auto-cleanup-29388000
          controller-uid: 1ef03f5c-10d8-4876-b810-41b28ad4a1b5
          job-name: k8s-auto-cleanup-29388000
      spec:
        containers:
        - command:
          - /bin/bash
          - -c
          - "set -e\n\necho \"\U0001F9F9 Starting K8s Auto-Cleanup at $(date)\"\n\n#
            Step 1: Delete failed pods\necho \"\U0001F4E6 Step 1: Cleaning failed
            pods...\"\nFAILED_COUNT=$(kubectl get pods -A --field-selector=status.phase=Failed
            --no-headers | wc -l)\nif [ \"$FAILED_COUNT\" -gt 0 ]; then\n  kubectl
            delete pods --field-selector=status.phase=Failed -A\n  echo \" Deleted
            $FAILED_COUNT failed pods\"\nelse\n  echo \" No failed pods found\"\nfi\n\n#
            Step 2: Scale down broken deployments (0 ready replicas)\necho \"\"\necho
            \"\U0001F4CA Step 2: Finding broken deployments...\"\nBROKEN_DEPS=$(kubectl
            get deployments -A -o json | \\\n  jq -r '.items[] | select(.spec.replicas
            > 0 and (.status.readyReplicas == null or .status.readyReplicas == 0))
            | \"\\(.metadata.namespace)/\\(.metadata.name)\"')\n\nif [ -n \"$BROKEN_DEPS\"
            ]; then\n  BROKEN_COUNT=$(echo \"$BROKEN_DEPS\" | wc -l | tr -d ' ')\n
            \ echo \"  Found $BROKEN_COUNT broken deployments\"\n\n  echo \"$BROKEN_DEPS\"
            | while read dep; do\n    NAMESPACE=$(echo $dep | cut -d'/' -f1)\n    NAME=$(echo
            $dep | cut -d'/' -f2)\n    kubectl scale deployment \"$NAME\" -n \"$NAMESPACE\"
            --replicas=0 || true\n    echo \"    $dep\"\n  done\n\n  echo \" Scaled
            down $BROKEN_COUNT deployments\"\nelse\n  echo \" No broken deployments
            found\"\nfi\n\n# Step 3: Report final status\necho \"\"\necho \"\U0001F4CA
            Final Status:\"\necho \"Running pods:  $(kubectl get pods -A --field-selector=status.phase=Running
            --no-headers | wc -l)\"\necho \"Pending pods:  $(kubectl get pods -A --field-selector=status.phase=Pending
            --no-headers | wc -l)\"\necho \"Failed pods:   $(kubectl get pods -A --field-selector=status.phase=Failed
            --no-headers | wc -l)\"\n\necho \"\"\necho \" K8s Auto-Cleanup completed
            at $(date)\"\n"
          image: bitnami/kubectl:latest
          imagePullPolicy: IfNotPresent
          name: cleanup
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Never
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: k8s-cleanup-sa
        serviceAccountName: k8s-cleanup-sa
        terminationGracePeriodSeconds: 30
  status:
    completionTime: "2025-11-16T08:05:12Z"
    conditions:
    - lastProbeTime: "2025-11-16T08:05:12Z"
      lastTransitionTime: "2025-11-16T08:05:12Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-11-16T08:05:12Z"
      lastTransitionTime: "2025-11-16T08:05:12Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-11-16T08:04:58Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-11-17T08:00:00Z"
    creationTimestamp: "2025-11-17T08:00:28Z"
    generation: 1
    labels:
      app: k8s-auto-cleanup
      batch.kubernetes.io/controller-uid: dad46498-bd10-4970-beda-40b5680a1df9
      batch.kubernetes.io/job-name: k8s-auto-cleanup-29389440
      controller-uid: dad46498-bd10-4970-beda-40b5680a1df9
      job-name: k8s-auto-cleanup-29389440
    name: k8s-auto-cleanup-29389440
    namespace: default
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: k8s-auto-cleanup
      uid: 188457b8-a4df-46a8-a4d3-5c3e96e972f6
    resourceVersion: "7511783"
    uid: dad46498-bd10-4970-beda-40b5680a1df9
  spec:
    activeDeadlineSeconds: 600
    backoffLimit: 0
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: dad46498-bd10-4970-beda-40b5680a1df9
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: k8s-auto-cleanup
          batch.kubernetes.io/controller-uid: dad46498-bd10-4970-beda-40b5680a1df9
          batch.kubernetes.io/job-name: k8s-auto-cleanup-29389440
          controller-uid: dad46498-bd10-4970-beda-40b5680a1df9
          job-name: k8s-auto-cleanup-29389440
      spec:
        containers:
        - command:
          - /bin/bash
          - -c
          - "set -e\n\necho \"\U0001F9F9 Starting K8s Auto-Cleanup at $(date)\"\n\n#
            Step 1: Delete failed pods\necho \"\U0001F4E6 Step 1: Cleaning failed
            pods...\"\nFAILED_COUNT=$(kubectl get pods -A --field-selector=status.phase=Failed
            --no-headers | wc -l)\nif [ \"$FAILED_COUNT\" -gt 0 ]; then\n  kubectl
            delete pods --field-selector=status.phase=Failed -A\n  echo \" Deleted
            $FAILED_COUNT failed pods\"\nelse\n  echo \" No failed pods found\"\nfi\n\n#
            Step 2: Scale down broken deployments (0 ready replicas)\necho \"\"\necho
            \"\U0001F4CA Step 2: Finding broken deployments...\"\nBROKEN_DEPS=$(kubectl
            get deployments -A -o json | \\\n  jq -r '.items[] | select(.spec.replicas
            > 0 and (.status.readyReplicas == null or .status.readyReplicas == 0))
            | \"\\(.metadata.namespace)/\\(.metadata.name)\"')\n\nif [ -n \"$BROKEN_DEPS\"
            ]; then\n  BROKEN_COUNT=$(echo \"$BROKEN_DEPS\" | wc -l | tr -d ' ')\n
            \ echo \"  Found $BROKEN_COUNT broken deployments\"\n\n  echo \"$BROKEN_DEPS\"
            | while read dep; do\n    NAMESPACE=$(echo $dep | cut -d'/' -f1)\n    NAME=$(echo
            $dep | cut -d'/' -f2)\n    kubectl scale deployment \"$NAME\" -n \"$NAMESPACE\"
            --replicas=0 || true\n    echo \"    $dep\"\n  done\n\n  echo \" Scaled
            down $BROKEN_COUNT deployments\"\nelse\n  echo \" No broken deployments
            found\"\nfi\n\n# Step 3: Report final status\necho \"\"\necho \"\U0001F4CA
            Final Status:\"\necho \"Running pods:  $(kubectl get pods -A --field-selector=status.phase=Running
            --no-headers | wc -l)\"\necho \"Pending pods:  $(kubectl get pods -A --field-selector=status.phase=Pending
            --no-headers | wc -l)\"\necho \"Failed pods:   $(kubectl get pods -A --field-selector=status.phase=Failed
            --no-headers | wc -l)\"\n\necho \"\"\necho \" K8s Auto-Cleanup completed
            at $(date)\"\n"
          image: bitnami/kubectl:latest
          imagePullPolicy: IfNotPresent
          name: cleanup
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Never
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: k8s-cleanup-sa
        serviceAccountName: k8s-cleanup-sa
        terminationGracePeriodSeconds: 30
  status:
    completionTime: "2025-11-17T08:00:38Z"
    conditions:
    - lastProbeTime: "2025-11-17T08:00:38Z"
      lastTransitionTime: "2025-11-17T08:00:38Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-11-17T08:00:38Z"
      lastTransitionTime: "2025-11-17T08:00:38Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-11-17T08:00:28Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-11-17T17:00:00Z"
    creationTimestamp: "2025-11-17T17:00:11Z"
    generation: 1
    labels:
      app: resource-optimizer
      batch.kubernetes.io/controller-uid: 628afebe-e9ff-4f26-bdff-d043f55860f8
      batch.kubernetes.io/job-name: resource-optimizer-29389980
      controller-uid: 628afebe-e9ff-4f26-bdff-d043f55860f8
      job-name: resource-optimizer-29389980
    name: resource-optimizer-29389980
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: resource-optimizer
      uid: e7f28190-a979-489b-b5e3-1b4b577009d4
    resourceVersion: "7629835"
    uid: 628afebe-e9ff-4f26-bdff-d043f55860f8
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 628afebe-e9ff-4f26-bdff-d043f55860f8
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: resource-optimizer
          batch.kubernetes.io/controller-uid: 628afebe-e9ff-4f26-bdff-d043f55860f8
          batch.kubernetes.io/job-name: resource-optimizer-29389980
          controller-uid: 628afebe-e9ff-4f26-bdff-d043f55860f8
          job-name: resource-optimizer-29389980
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - "echo \"Starting resource optimization...\"\n\n# Get all deployments without
            resource requests\nkubectl get deployments --all-namespaces -o json |
            \\\n  jq -r '.items[] | select(.spec.template.spec.containers[0].resources.requests
            == null) | \"\\(.metadata.namespace) \\(.metadata.name)\"' | \\\n  while
            read namespace name; do\n    echo \"Adding default resources to deployment:
            $namespace/$name\"\n    \n    # Patch deployment with default resources\n
            \   kubectl patch deployment \"$name\" -n \"$namespace\" --type='json'
            -p='[\n      {\n        \"op\": \"add\",\n        \"path\": \"/spec/template/spec/containers/0/resources\",\n
            \       \"value\": {\n          \"requests\": {\n            \"cpu\":
            \"100m\",\n            \"memory\": \"128Mi\"\n          },\n          \"limits\":
            {\n            \"cpu\": \"500m\",\n            \"memory\": \"512Mi\"\n
            \         }\n        }\n      }\n    ]' || echo \"Failed to patch $namespace/$name\"\n
            \ done\n\necho \"Resource optimization completed\"\n"
          image: bitnami/kubectl:latest
          imagePullPolicy: Always
          name: optimizer
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: resource-optimizer
        serviceAccountName: resource-optimizer
        terminationGracePeriodSeconds: 30
  status:
    completionTime: "2025-11-17T23:44:34Z"
    conditions:
    - lastProbeTime: "2025-11-17T23:44:34Z"
      lastTransitionTime: "2025-11-17T23:44:34Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-11-17T23:44:34Z"
      lastTransitionTime: "2025-11-17T23:44:34Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-11-17T17:00:11Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-11-17T23:00:00Z"
    creationTimestamp: "2025-11-17T23:00:00Z"
    generation: 1
    labels:
      app: resource-optimizer
      batch.kubernetes.io/controller-uid: ecfd5b46-508d-4041-b23c-a4bcfb827817
      batch.kubernetes.io/job-name: resource-optimizer-29390340
      controller-uid: ecfd5b46-508d-4041-b23c-a4bcfb827817
      job-name: resource-optimizer-29390340
    name: resource-optimizer-29390340
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: resource-optimizer
      uid: e7f28190-a979-489b-b5e3-1b4b577009d4
    resourceVersion: "7630437"
    uid: ecfd5b46-508d-4041-b23c-a4bcfb827817
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: ecfd5b46-508d-4041-b23c-a4bcfb827817
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: resource-optimizer
          batch.kubernetes.io/controller-uid: ecfd5b46-508d-4041-b23c-a4bcfb827817
          batch.kubernetes.io/job-name: resource-optimizer-29390340
          controller-uid: ecfd5b46-508d-4041-b23c-a4bcfb827817
          job-name: resource-optimizer-29390340
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - "echo \"Starting resource optimization...\"\n\n# Get all deployments without
            resource requests\nkubectl get deployments --all-namespaces -o json |
            \\\n  jq -r '.items[] | select(.spec.template.spec.containers[0].resources.requests
            == null) | \"\\(.metadata.namespace) \\(.metadata.name)\"' | \\\n  while
            read namespace name; do\n    echo \"Adding default resources to deployment:
            $namespace/$name\"\n    \n    # Patch deployment with default resources\n
            \   kubectl patch deployment \"$name\" -n \"$namespace\" --type='json'
            -p='[\n      {\n        \"op\": \"add\",\n        \"path\": \"/spec/template/spec/containers/0/resources\",\n
            \       \"value\": {\n          \"requests\": {\n            \"cpu\":
            \"100m\",\n            \"memory\": \"128Mi\"\n          },\n          \"limits\":
            {\n            \"cpu\": \"500m\",\n            \"memory\": \"512Mi\"\n
            \         }\n        }\n      }\n    ]' || echo \"Failed to patch $namespace/$name\"\n
            \ done\n\necho \"Resource optimization completed\"\n"
          image: bitnami/kubectl:latest
          imagePullPolicy: Always
          name: optimizer
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: resource-optimizer
        serviceAccountName: resource-optimizer
        terminationGracePeriodSeconds: 30
  status:
    completionTime: "2025-11-17T23:45:47Z"
    conditions:
    - lastProbeTime: "2025-11-17T23:45:47Z"
      lastTransitionTime: "2025-11-17T23:45:47Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-11-17T23:45:47Z"
      lastTransitionTime: "2025-11-17T23:45:47Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-11-17T23:00:00Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-11-17T17:00:00Z"
    creationTimestamp: "2025-11-17T17:00:00Z"
    generation: 1
    labels:
      app: vpa-recommender
      batch.kubernetes.io/controller-uid: 60d46ac6-55bf-4fab-b12a-a7964ccdde3f
      batch.kubernetes.io/job-name: vpa-recommender-29389980
      controller-uid: 60d46ac6-55bf-4fab-b12a-a7964ccdde3f
      job-name: vpa-recommender-29389980
    name: vpa-recommender-29389980
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: vpa-recommender
      uid: 2a69835c-9dca-498f-a5df-701ee4ec51f3
    resourceVersion: "7629130"
    uid: 60d46ac6-55bf-4fab-b12a-a7964ccdde3f
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 60d46ac6-55bf-4fab-b12a-a7964ccdde3f
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: vpa-recommender
          batch.kubernetes.io/controller-uid: 60d46ac6-55bf-4fab-b12a-a7964ccdde3f
          batch.kubernetes.io/job-name: vpa-recommender-29389980
          controller-uid: 60d46ac6-55bf-4fab-b12a-a7964ccdde3f
          job-name: vpa-recommender-29389980
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - "echo \"=== VPA Resource Recommendations ===\"\necho \"\"\n\n# Get all
            running pods with their resource usage\nkubectl top pods --all-namespaces
            --no-headers 2>/dev/null | while read namespace name cpu_usage memory_usage
            rest; do\n  # Get pod resource requests/limits\n  pod_json=$(kubectl get
            pod \"$name\" -n \"$namespace\" -o json 2>/dev/null || echo \"{}\")\n
            \ \n  cpu_request=$(echo \"$pod_json\" | jq -r '.spec.containers[0].resources.requests.cpu
            // \"none\"')\n  memory_request=$(echo \"$pod_json\" | jq -r '.spec.containers[0].resources.requests.memory
            // \"none\"')\n  cpu_limit=$(echo \"$pod_json\" | jq -r '.spec.containers[0].resources.limits.cpu
            // \"none\"')\n  memory_limit=$(echo \"$pod_json\" | jq -r '.spec.containers[0].resources.limits.memory
            // \"none\"')\n  \n  # Parse CPU usage (remove 'm' suffix)\n  cpu_num=$(echo
            \"$cpu_usage\" | sed 's/m//' | awk '{print int($1)}')\n  memory_num=$(echo
            \"$memory_usage\" | sed 's/Mi//' | awk '{print int($1)}')\n  \n  # Calculate
            recommendations (add 20% buffer)\n  if [ \"$cpu_request\" != \"none\"
            ] && [ \"$cpu_num\" -gt 0 ]; then\n    cpu_req_num=$(echo \"$cpu_request\"
            | sed 's/m//' | awk '{print int($1)}')\n    if [ \"$cpu_num\" -lt $((cpu_req_num
            * 50 / 100)) ]; then\n      echo \"  $namespace/$name: CPU underutilized
            (using ${cpu_usage}, requested ${cpu_request})\"\n    elif [ \"$cpu_num\"
            -gt $((cpu_req_num * 90 / 100)) ]; then\n      recommended=$((cpu_num
            * 120 / 100))\n      echo \"\U0001F4C8 $namespace/$name: CPU recommendation:
            ${recommended}m (currently using ${cpu_usage}, requested ${cpu_request})\"\n
            \   fi\n  fi\n  \n  if [ \"$memory_request\" != \"none\" ] && [ \"$memory_num\"
            -gt 0 ]; then\n    mem_req_num=$(echo \"$memory_request\" | sed 's/Mi//'
            | awk '{print int($1)}')\n    if [ \"$memory_num\" -lt $((mem_req_num
            * 50 / 100)) ]; then\n      echo \"  $namespace/$name: Memory underutilized
            (using ${memory_usage}, requested ${memory_request})\"\n    elif [ \"$memory_num\"
            -gt $((mem_req_num * 90 / 100)) ]; then\n      recommended=$((memory_num
            * 120 / 100))\n      echo \"\U0001F4C8 $namespace/$name: Memory recommendation:
            ${recommended}Mi (currently using ${memory_usage}, requested ${memory_request})\"\n
            \   fi\n  fi\ndone\n\necho \"\"\necho \"=== Pods Without Resource Requests
            ===\"\nkubectl get pods --all-namespaces -o json | \\\n  jq -r '.items[]
            | select(.status.phase == \"Running\") | select(.spec.containers[0].resources.requests
            == null) | \"\\(.metadata.namespace)/\\(.metadata.name)\"' | \\\n  while
            read pod; do\n    echo \" $pod: Missing resource requests\"\n  done\n\necho
            \"\"\necho \"=== Resource Summary ===\"\ntotal_pods=$(kubectl get pods
            --all-namespaces --field-selector=status.phase==Running --no-headers |
            wc -l | tr -d ' ')\npods_with_resources=$(kubectl get pods --all-namespaces
            -o json | jq -r '[.items[] | select(.status.phase == \"Running\") | select(.spec.containers[0].resources.requests
            != null)] | length')\necho \"Running pods: $total_pods\"\necho \"Pods
            with resource requests: $pods_with_resources\"\necho \"Pods without resource
            requests: $((total_pods - pods_with_resources))\"\n"
          image: bitnami/kubectl:latest
          imagePullPolicy: Always
          name: recommender
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: vpa-recommender
        serviceAccountName: vpa-recommender
        terminationGracePeriodSeconds: 30
  status:
    completionTime: "2025-11-17T23:44:10Z"
    conditions:
    - lastProbeTime: "2025-11-17T23:44:10Z"
      lastTransitionTime: "2025-11-17T23:44:10Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-11-17T23:44:10Z"
      lastTransitionTime: "2025-11-17T23:44:10Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-11-17T17:00:00Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-11-17T21:00:00Z"
    creationTimestamp: "2025-11-17T21:00:00Z"
    generation: 1
    labels:
      app: vpa-recommender
      batch.kubernetes.io/controller-uid: 1f497263-d69d-4361-bb5d-856ed8b26f80
      batch.kubernetes.io/job-name: vpa-recommender-29390220
      controller-uid: 1f497263-d69d-4361-bb5d-856ed8b26f80
      job-name: vpa-recommender-29390220
    name: vpa-recommender-29390220
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: vpa-recommender
      uid: 2a69835c-9dca-498f-a5df-701ee4ec51f3
    resourceVersion: "7630264"
    uid: 1f497263-d69d-4361-bb5d-856ed8b26f80
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 1f497263-d69d-4361-bb5d-856ed8b26f80
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: vpa-recommender
          batch.kubernetes.io/controller-uid: 1f497263-d69d-4361-bb5d-856ed8b26f80
          batch.kubernetes.io/job-name: vpa-recommender-29390220
          controller-uid: 1f497263-d69d-4361-bb5d-856ed8b26f80
          job-name: vpa-recommender-29390220
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - "echo \"=== VPA Resource Recommendations ===\"\necho \"\"\n\n# Get all
            running pods with their resource usage\nkubectl top pods --all-namespaces
            --no-headers 2>/dev/null | while read namespace name cpu_usage memory_usage
            rest; do\n  # Get pod resource requests/limits\n  pod_json=$(kubectl get
            pod \"$name\" -n \"$namespace\" -o json 2>/dev/null || echo \"{}\")\n
            \ \n  cpu_request=$(echo \"$pod_json\" | jq -r '.spec.containers[0].resources.requests.cpu
            // \"none\"')\n  memory_request=$(echo \"$pod_json\" | jq -r '.spec.containers[0].resources.requests.memory
            // \"none\"')\n  cpu_limit=$(echo \"$pod_json\" | jq -r '.spec.containers[0].resources.limits.cpu
            // \"none\"')\n  memory_limit=$(echo \"$pod_json\" | jq -r '.spec.containers[0].resources.limits.memory
            // \"none\"')\n  \n  # Parse CPU usage (remove 'm' suffix)\n  cpu_num=$(echo
            \"$cpu_usage\" | sed 's/m//' | awk '{print int($1)}')\n  memory_num=$(echo
            \"$memory_usage\" | sed 's/Mi//' | awk '{print int($1)}')\n  \n  # Calculate
            recommendations (add 20% buffer)\n  if [ \"$cpu_request\" != \"none\"
            ] && [ \"$cpu_num\" -gt 0 ]; then\n    cpu_req_num=$(echo \"$cpu_request\"
            | sed 's/m//' | awk '{print int($1)}')\n    if [ \"$cpu_num\" -lt $((cpu_req_num
            * 50 / 100)) ]; then\n      echo \"  $namespace/$name: CPU underutilized
            (using ${cpu_usage}, requested ${cpu_request})\"\n    elif [ \"$cpu_num\"
            -gt $((cpu_req_num * 90 / 100)) ]; then\n      recommended=$((cpu_num
            * 120 / 100))\n      echo \"\U0001F4C8 $namespace/$name: CPU recommendation:
            ${recommended}m (currently using ${cpu_usage}, requested ${cpu_request})\"\n
            \   fi\n  fi\n  \n  if [ \"$memory_request\" != \"none\" ] && [ \"$memory_num\"
            -gt 0 ]; then\n    mem_req_num=$(echo \"$memory_request\" | sed 's/Mi//'
            | awk '{print int($1)}')\n    if [ \"$memory_num\" -lt $((mem_req_num
            * 50 / 100)) ]; then\n      echo \"  $namespace/$name: Memory underutilized
            (using ${memory_usage}, requested ${memory_request})\"\n    elif [ \"$memory_num\"
            -gt $((mem_req_num * 90 / 100)) ]; then\n      recommended=$((memory_num
            * 120 / 100))\n      echo \"\U0001F4C8 $namespace/$name: Memory recommendation:
            ${recommended}Mi (currently using ${memory_usage}, requested ${memory_request})\"\n
            \   fi\n  fi\ndone\n\necho \"\"\necho \"=== Pods Without Resource Requests
            ===\"\nkubectl get pods --all-namespaces -o json | \\\n  jq -r '.items[]
            | select(.status.phase == \"Running\") | select(.spec.containers[0].resources.requests
            == null) | \"\\(.metadata.namespace)/\\(.metadata.name)\"' | \\\n  while
            read pod; do\n    echo \" $pod: Missing resource requests\"\n  done\n\necho
            \"\"\necho \"=== Resource Summary ===\"\ntotal_pods=$(kubectl get pods
            --all-namespaces --field-selector=status.phase==Running --no-headers |
            wc -l | tr -d ' ')\npods_with_resources=$(kubectl get pods --all-namespaces
            -o json | jq -r '[.items[] | select(.status.phase == \"Running\") | select(.spec.containers[0].resources.requests
            != null)] | length')\necho \"Running pods: $total_pods\"\necho \"Pods
            with resource requests: $pods_with_resources\"\necho \"Pods without resource
            requests: $((total_pods - pods_with_resources))\"\n"
          image: bitnami/kubectl:latest
          imagePullPolicy: Always
          name: recommender
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: vpa-recommender
        serviceAccountName: vpa-recommender
        terminationGracePeriodSeconds: 30
  status:
    completionTime: "2025-11-17T23:45:25Z"
    conditions:
    - lastProbeTime: "2025-11-17T23:45:25Z"
      lastTransitionTime: "2025-11-17T23:45:25Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-11-17T23:45:25Z"
      lastTransitionTime: "2025-11-17T23:45:25Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-11-17T21:00:00Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-11-18T01:00:00Z"
    creationTimestamp: "2025-11-18T01:00:00Z"
    generation: 1
    labels:
      app: vpa-recommender
      batch.kubernetes.io/controller-uid: 570a83eb-e97c-44b6-9b86-c9c07a274094
      batch.kubernetes.io/job-name: vpa-recommender-29390460
      controller-uid: 570a83eb-e97c-44b6-9b86-c9c07a274094
      job-name: vpa-recommender-29390460
    name: vpa-recommender-29390460
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: vpa-recommender
      uid: 2a69835c-9dca-498f-a5df-701ee4ec51f3
    resourceVersion: "7639614"
    uid: 570a83eb-e97c-44b6-9b86-c9c07a274094
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 570a83eb-e97c-44b6-9b86-c9c07a274094
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: vpa-recommender
          batch.kubernetes.io/controller-uid: 570a83eb-e97c-44b6-9b86-c9c07a274094
          batch.kubernetes.io/job-name: vpa-recommender-29390460
          controller-uid: 570a83eb-e97c-44b6-9b86-c9c07a274094
          job-name: vpa-recommender-29390460
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - "echo \"=== VPA Resource Recommendations ===\"\necho \"\"\n\n# Get all
            running pods with their resource usage\nkubectl top pods --all-namespaces
            --no-headers 2>/dev/null | while read namespace name cpu_usage memory_usage
            rest; do\n  # Get pod resource requests/limits\n  pod_json=$(kubectl get
            pod \"$name\" -n \"$namespace\" -o json 2>/dev/null || echo \"{}\")\n
            \ \n  cpu_request=$(echo \"$pod_json\" | jq -r '.spec.containers[0].resources.requests.cpu
            // \"none\"')\n  memory_request=$(echo \"$pod_json\" | jq -r '.spec.containers[0].resources.requests.memory
            // \"none\"')\n  cpu_limit=$(echo \"$pod_json\" | jq -r '.spec.containers[0].resources.limits.cpu
            // \"none\"')\n  memory_limit=$(echo \"$pod_json\" | jq -r '.spec.containers[0].resources.limits.memory
            // \"none\"')\n  \n  # Parse CPU usage (remove 'm' suffix)\n  cpu_num=$(echo
            \"$cpu_usage\" | sed 's/m//' | awk '{print int($1)}')\n  memory_num=$(echo
            \"$memory_usage\" | sed 's/Mi//' | awk '{print int($1)}')\n  \n  # Calculate
            recommendations (add 20% buffer)\n  if [ \"$cpu_request\" != \"none\"
            ] && [ \"$cpu_num\" -gt 0 ]; then\n    cpu_req_num=$(echo \"$cpu_request\"
            | sed 's/m//' | awk '{print int($1)}')\n    if [ \"$cpu_num\" -lt $((cpu_req_num
            * 50 / 100)) ]; then\n      echo \"  $namespace/$name: CPU underutilized
            (using ${cpu_usage}, requested ${cpu_request})\"\n    elif [ \"$cpu_num\"
            -gt $((cpu_req_num * 90 / 100)) ]; then\n      recommended=$((cpu_num
            * 120 / 100))\n      echo \"\U0001F4C8 $namespace/$name: CPU recommendation:
            ${recommended}m (currently using ${cpu_usage}, requested ${cpu_request})\"\n
            \   fi\n  fi\n  \n  if [ \"$memory_request\" != \"none\" ] && [ \"$memory_num\"
            -gt 0 ]; then\n    mem_req_num=$(echo \"$memory_request\" | sed 's/Mi//'
            | awk '{print int($1)}')\n    if [ \"$memory_num\" -lt $((mem_req_num
            * 50 / 100)) ]; then\n      echo \"  $namespace/$name: Memory underutilized
            (using ${memory_usage}, requested ${memory_request})\"\n    elif [ \"$memory_num\"
            -gt $((mem_req_num * 90 / 100)) ]; then\n      recommended=$((memory_num
            * 120 / 100))\n      echo \"\U0001F4C8 $namespace/$name: Memory recommendation:
            ${recommended}Mi (currently using ${memory_usage}, requested ${memory_request})\"\n
            \   fi\n  fi\ndone\n\necho \"\"\necho \"=== Pods Without Resource Requests
            ===\"\nkubectl get pods --all-namespaces -o json | \\\n  jq -r '.items[]
            | select(.status.phase == \"Running\") | select(.spec.containers[0].resources.requests
            == null) | \"\\(.metadata.namespace)/\\(.metadata.name)\"' | \\\n  while
            read pod; do\n    echo \" $pod: Missing resource requests\"\n  done\n\necho
            \"\"\necho \"=== Resource Summary ===\"\ntotal_pods=$(kubectl get pods
            --all-namespaces --field-selector=status.phase==Running --no-headers |
            wc -l | tr -d ' ')\npods_with_resources=$(kubectl get pods --all-namespaces
            -o json | jq -r '[.items[] | select(.status.phase == \"Running\") | select(.spec.containers[0].resources.requests
            != null)] | length')\necho \"Running pods: $total_pods\"\necho \"Pods
            with resource requests: $pods_with_resources\"\necho \"Pods without resource
            requests: $((total_pods - pods_with_resources))\"\n"
          image: bitnami/kubectl:latest
          imagePullPolicy: Always
          name: recommender
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: vpa-recommender
        serviceAccountName: vpa-recommender
        terminationGracePeriodSeconds: 30
  status:
    completionTime: "2025-11-18T01:00:08Z"
    conditions:
    - lastProbeTime: "2025-11-18T01:00:08Z"
      lastTransitionTime: "2025-11-18T01:00:08Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-11-18T01:00:08Z"
      lastTransitionTime: "2025-11-18T01:00:08Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-11-18T01:00:00Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"batch/v1","kind":"Job","metadata":{"annotations":{},"name":"ollama-pull-models","namespace":"llm-inference"},"spec":{"template":{"spec":{"containers":[{"command":["/bin/sh","-c","# Wait for Ollama to be ready\nuntil curl -f http://ollama:11434; do\n  echo \"Waiting for Ollama...\"\n  sleep 5\ndone\n\n# Pull free models for different tasks\ncurl -X POST http://ollama:11434/api/pull -d '{\"name\": \"qwen2.5-coder:7b\"}'\ncurl -X POST http://ollama:11434/api/pull -d '{\"name\": \"llama3.2:3b\"}'\ncurl -X POST http://ollama:11434/api/pull -d '{\"name\": \"deepseek-coder:6.7b\"}'\n\necho \"Models pulled successfully!\"\n"],"image":"curlimages/curl:latest","name":"pull-models"}],"restartPolicy":"OnFailure"}}}}
    creationTimestamp: "2025-11-17T19:58:34Z"
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: 08baaa97-304e-44fb-ad60-829e97e50d2b
      batch.kubernetes.io/job-name: ollama-pull-models
      controller-uid: 08baaa97-304e-44fb-ad60-829e97e50d2b
      job-name: ollama-pull-models
    name: ollama-pull-models
    namespace: llm-inference
    resourceVersion: "7643012"
    uid: 08baaa97-304e-44fb-ad60-829e97e50d2b
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 08baaa97-304e-44fb-ad60-829e97e50d2b
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: 08baaa97-304e-44fb-ad60-829e97e50d2b
          batch.kubernetes.io/job-name: ollama-pull-models
          controller-uid: 08baaa97-304e-44fb-ad60-829e97e50d2b
          job-name: ollama-pull-models
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - |
            # Wait for Ollama to be ready
            until curl -f http://ollama:11434; do
              echo "Waiting for Ollama..."
              sleep 5
            done

            # Pull free models for different tasks
            curl -X POST http://ollama:11434/api/pull -d '{"name": "qwen2.5-coder:7b"}'
            curl -X POST http://ollama:11434/api/pull -d '{"name": "llama3.2:3b"}'
            curl -X POST http://ollama:11434/api/pull -d '{"name": "deepseek-coder:6.7b"}'

            echo "Models pulled successfully!"
          image: curlimages/curl:latest
          imagePullPolicy: Always
          name: pull-models
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    active: 1
    ready: 1
    startTime: "2025-11-17T19:58:34Z"
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-11-18T02:15:00Z"
    creationTimestamp: "2025-11-18T02:15:00Z"
    generation: 1
    labels:
      app: registry-sync
      component: service-discovery
    name: registry-sync-29390535
    namespace: llm-platform
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: registry-sync
      uid: 8ddfc9e8-ae14-4fd8-a568-834b1c76f113
    resourceVersion: "7647638"
    uid: 0ef5920f-023b-465c-b331-71bb204ef57d
  spec:
    activeDeadlineSeconds: 300
    backoffLimit: 1
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 0ef5920f-023b-465c-b331-71bb204ef57d
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: registry-sync
          batch.kubernetes.io/controller-uid: 0ef5920f-023b-465c-b331-71bb204ef57d
          batch.kubernetes.io/job-name: registry-sync-29390535
          component: service-discovery
          controller-uid: 0ef5920f-023b-465c-b331-71bb204ef57d
          job-name: registry-sync-29390535
      spec:
        containers:
        - command:
          - npx
          - tsx
          - src/services/registry-sync/cli.ts
          env:
          - name: NODE_ENV
            value: production
          envFrom:
          - configMapRef:
              name: registry-sync-config
          image: registry-sync:latest
          imagePullPolicy: IfNotPresent
          name: registry-sync
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /workspace
            name: workspace
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /Users/flux423/Sites/LLM
            type: Directory
          name: workspace
  status:
    conditions:
    - lastProbeTime: "2025-11-18T02:20:00Z"
      lastTransitionTime: "2025-11-18T02:20:00Z"
      message: Job was active longer than specified deadline
      reason: DeadlineExceeded
      status: "True"
      type: FailureTarget
    - lastProbeTime: "2025-11-18T02:20:02Z"
      lastTransitionTime: "2025-11-18T02:20:02Z"
      message: Job was active longer than specified deadline
      reason: DeadlineExceeded
      status: "True"
      type: Failed
    failed: 1
    ready: 0
    startTime: "2025-11-18T02:15:00Z"
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-11-18T01:00:00Z"
    creationTimestamp: "2025-11-18T01:00:00Z"
    generation: 1
    labels:
      app: pod-cleanup
      batch.kubernetes.io/controller-uid: ca52d6a7-95bb-4273-89ba-64ff4ce154e7
      batch.kubernetes.io/job-name: pod-cleanup-29390460
      controller-uid: ca52d6a7-95bb-4273-89ba-64ff4ce154e7
      job-name: pod-cleanup-29390460
    name: pod-cleanup-29390460
    namespace: pod-cleanup
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: pod-cleanup
      uid: 922f3cf1-b800-42c6-b831-12e7c80be8e7
    resourceVersion: "7639628"
    uid: ca52d6a7-95bb-4273-89ba-64ff4ce154e7
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: ca52d6a7-95bb-4273-89ba-64ff4ce154e7
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: pod-cleanup
          batch.kubernetes.io/controller-uid: ca52d6a7-95bb-4273-89ba-64ff4ce154e7
          batch.kubernetes.io/job-name: pod-cleanup-29390460
          controller-uid: ca52d6a7-95bb-4273-89ba-64ff4ce154e7
          job-name: pod-cleanup-29390460
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - |
            echo "Starting pod cleanup..."

            # Clean up Succeeded pods older than 1 hour
            SUCCEEDED=$(kubectl get pods --all-namespaces --field-selector=status.phase==Succeeded -o json | jq -r '.items[] | select((.status.startTime // .metadata.creationTimestamp) | fromdateiso8601 < (now - 3600)) | "\(.metadata.namespace) \(.metadata.name)"')

            if [ -n "$SUCCEEDED" ]; then
              echo "$SUCCEEDED" | while read namespace name; do
                echo "Deleting succeeded pod: $namespace/$name"
                kubectl delete pod "$name" -n "$namespace" --grace-period=0 || true
              done
            fi

            # Clean up Failed pods older than 24 hours
            FAILED=$(kubectl get pods --all-namespaces --field-selector=status.phase==Failed -o json | jq -r '.items[] | select((.status.startTime // .metadata.creationTimestamp) | fromdateiso8601 < (now - 86400)) | "\(.metadata.namespace) \(.metadata.name)"')

            if [ -n "$FAILED" ]; then
              echo "$FAILED" | while read namespace name; do
                echo "Deleting failed pod: $namespace/$name"
                kubectl delete pod "$name" -n "$namespace" --grace-period=0 || true
              done
            fi

            # Clean up Evicted pods immediately
            EVICTED=$(kubectl get pods --all-namespaces --field-selector=status.phase==Failed -o json | jq -r '.items[] | select(.status.reason == "Evicted") | "\(.metadata.namespace) \(.metadata.name)"')

            if [ -n "$EVICTED" ]; then
              echo "$EVICTED" | while read namespace name; do
                echo "Deleting evicted pod: $namespace/$name"
                kubectl delete pod "$name" -n "$namespace" --grace-period=0 || true
              done
            fi

            echo "Pod cleanup completed"
          image: bitnami/kubectl:latest
          imagePullPolicy: Always
          name: cleanup
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: pod-cleanup
        serviceAccountName: pod-cleanup
        terminationGracePeriodSeconds: 30
  status:
    completionTime: "2025-11-18T01:00:11Z"
    conditions:
    - lastProbeTime: "2025-11-18T01:00:11Z"
      lastTransitionTime: "2025-11-18T01:00:11Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-11-18T01:00:11Z"
      lastTransitionTime: "2025-11-18T01:00:11Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-11-18T01:00:00Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-11-18T01:30:00Z"
    creationTimestamp: "2025-11-18T01:34:05Z"
    generation: 1
    labels:
      app: pod-cleanup
      batch.kubernetes.io/controller-uid: 24f0ae88-28c0-4700-af79-57fe58317190
      batch.kubernetes.io/job-name: pod-cleanup-29390490
      controller-uid: 24f0ae88-28c0-4700-af79-57fe58317190
      job-name: pod-cleanup-29390490
    name: pod-cleanup-29390490
    namespace: pod-cleanup
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: pod-cleanup
      uid: 922f3cf1-b800-42c6-b831-12e7c80be8e7
    resourceVersion: "7641698"
    uid: 24f0ae88-28c0-4700-af79-57fe58317190
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 24f0ae88-28c0-4700-af79-57fe58317190
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: pod-cleanup
          batch.kubernetes.io/controller-uid: 24f0ae88-28c0-4700-af79-57fe58317190
          batch.kubernetes.io/job-name: pod-cleanup-29390490
          controller-uid: 24f0ae88-28c0-4700-af79-57fe58317190
          job-name: pod-cleanup-29390490
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - |
            echo "Starting pod cleanup..."

            # Clean up Succeeded pods older than 1 hour
            SUCCEEDED=$(kubectl get pods --all-namespaces --field-selector=status.phase==Succeeded -o json | jq -r '.items[] | select((.status.startTime // .metadata.creationTimestamp) | fromdateiso8601 < (now - 3600)) | "\(.metadata.namespace) \(.metadata.name)"')

            if [ -n "$SUCCEEDED" ]; then
              echo "$SUCCEEDED" | while read namespace name; do
                echo "Deleting succeeded pod: $namespace/$name"
                kubectl delete pod "$name" -n "$namespace" --grace-period=0 || true
              done
            fi

            # Clean up Failed pods older than 24 hours
            FAILED=$(kubectl get pods --all-namespaces --field-selector=status.phase==Failed -o json | jq -r '.items[] | select((.status.startTime // .metadata.creationTimestamp) | fromdateiso8601 < (now - 86400)) | "\(.metadata.namespace) \(.metadata.name)"')

            if [ -n "$FAILED" ]; then
              echo "$FAILED" | while read namespace name; do
                echo "Deleting failed pod: $namespace/$name"
                kubectl delete pod "$name" -n "$namespace" --grace-period=0 || true
              done
            fi

            # Clean up Evicted pods immediately
            EVICTED=$(kubectl get pods --all-namespaces --field-selector=status.phase==Failed -o json | jq -r '.items[] | select(.status.reason == "Evicted") | "\(.metadata.namespace) \(.metadata.name)"')

            if [ -n "$EVICTED" ]; then
              echo "$EVICTED" | while read namespace name; do
                echo "Deleting evicted pod: $namespace/$name"
                kubectl delete pod "$name" -n "$namespace" --grace-period=0 || true
              done
            fi

            echo "Pod cleanup completed"
          image: bitnami/kubectl:latest
          imagePullPolicy: Always
          name: cleanup
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: pod-cleanup
        serviceAccountName: pod-cleanup
        terminationGracePeriodSeconds: 30
  status:
    completionTime: "2025-11-18T01:34:23Z"
    conditions:
    - lastProbeTime: "2025-11-18T01:34:23Z"
      lastTransitionTime: "2025-11-18T01:34:23Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-11-18T01:34:23Z"
      lastTransitionTime: "2025-11-18T01:34:23Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-11-18T01:34:05Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-11-18T02:00:00Z"
    creationTimestamp: "2025-11-18T02:00:00Z"
    generation: 1
    labels:
      app: pod-cleanup
      batch.kubernetes.io/controller-uid: 591ceaf6-43c9-4180-bcdf-4eee763d0c22
      batch.kubernetes.io/job-name: pod-cleanup-29390520
      controller-uid: 591ceaf6-43c9-4180-bcdf-4eee763d0c22
      job-name: pod-cleanup-29390520
    name: pod-cleanup-29390520
    namespace: pod-cleanup
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: pod-cleanup
      uid: 922f3cf1-b800-42c6-b831-12e7c80be8e7
    resourceVersion: "7645960"
    uid: 591ceaf6-43c9-4180-bcdf-4eee763d0c22
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 591ceaf6-43c9-4180-bcdf-4eee763d0c22
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: pod-cleanup
          batch.kubernetes.io/controller-uid: 591ceaf6-43c9-4180-bcdf-4eee763d0c22
          batch.kubernetes.io/job-name: pod-cleanup-29390520
          controller-uid: 591ceaf6-43c9-4180-bcdf-4eee763d0c22
          job-name: pod-cleanup-29390520
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - |
            echo "Starting pod cleanup..."

            # Clean up Succeeded pods older than 1 hour
            SUCCEEDED=$(kubectl get pods --all-namespaces --field-selector=status.phase==Succeeded -o json | jq -r '.items[] | select((.status.startTime // .metadata.creationTimestamp) | fromdateiso8601 < (now - 3600)) | "\(.metadata.namespace) \(.metadata.name)"')

            if [ -n "$SUCCEEDED" ]; then
              echo "$SUCCEEDED" | while read namespace name; do
                echo "Deleting succeeded pod: $namespace/$name"
                kubectl delete pod "$name" -n "$namespace" --grace-period=0 || true
              done
            fi

            # Clean up Failed pods older than 24 hours
            FAILED=$(kubectl get pods --all-namespaces --field-selector=status.phase==Failed -o json | jq -r '.items[] | select((.status.startTime // .metadata.creationTimestamp) | fromdateiso8601 < (now - 86400)) | "\(.metadata.namespace) \(.metadata.name)"')

            if [ -n "$FAILED" ]; then
              echo "$FAILED" | while read namespace name; do
                echo "Deleting failed pod: $namespace/$name"
                kubectl delete pod "$name" -n "$namespace" --grace-period=0 || true
              done
            fi

            # Clean up Evicted pods immediately
            EVICTED=$(kubectl get pods --all-namespaces --field-selector=status.phase==Failed -o json | jq -r '.items[] | select(.status.reason == "Evicted") | "\(.metadata.namespace) \(.metadata.name)"')

            if [ -n "$EVICTED" ]; then
              echo "$EVICTED" | while read namespace name; do
                echo "Deleting evicted pod: $namespace/$name"
                kubectl delete pod "$name" -n "$namespace" --grace-period=0 || true
              done
            fi

            echo "Pod cleanup completed"
          image: bitnami/kubectl:latest
          imagePullPolicy: Always
          name: cleanup
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: pod-cleanup
        serviceAccountName: pod-cleanup
        terminationGracePeriodSeconds: 30
  status:
    completionTime: "2025-11-18T02:00:10Z"
    conditions:
    - lastProbeTime: "2025-11-18T02:00:10Z"
      lastTransitionTime: "2025-11-18T02:00:10Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-11-18T02:00:10Z"
      lastTransitionTime: "2025-11-18T02:00:10Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-11-18T02:00:00Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
kind: List
metadata:
  resourceVersion: ""
